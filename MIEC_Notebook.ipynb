{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f44185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import *\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from keras.optimizers import Adam, SGD\n",
    "from keras.layers import *\n",
    "from keras.metrics import *\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from hyperopt import hp, STATUS_OK\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, \\\n",
    "                                    GlobalAveragePooling1D, AveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "## sequence.pad_sequence(train_x, maxlen=max_words)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import sem\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42d5aeb2",
   "metadata": {},
   "source": [
    "# Define Path to MIEC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e960c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/stg3/data1/chad/MIEC_data/'\n",
    "path3 = '/stg3/data1/chad/MIEC_data/generalizePairDist_15A_noH_10A.list'\n",
    "\n",
    "file2 = 'MIEC_10A_T_Full_AVER.list'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c9f26",
   "metadata": {},
   "source": [
    "# Function for MIEC DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ac1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Preprocessing ##############\n",
    "\n",
    "def df1(path, file):\n",
    "    df = pd.read_csv(path1+file2, delimiter= ' ', header= None)\n",
    "    df.iloc[0,2:]\n",
    "    return df\n",
    "\n",
    "\n",
    "#df1(path1,file2)\n",
    "######################\n",
    "\n",
    "######################\n",
    "\n",
    "def four_inter(data):\n",
    "    df = []\n",
    "#    print(data)\n",
    "    df = data\n",
    "    c_ln = int(df.shape[1])\n",
    "    n_dim = c_ln/4\n",
    "    \n",
    "    lt = [\"VDW\", \"ELE\", \"GB\", \"SA\"]\n",
    "    op = []\n",
    "    \n",
    "    for k, j in enumerate(lt):\n",
    "        if lt[k] == j:\n",
    "            for i in range(int(n_dim)):\n",
    "                f = j + \"_\" + str(i)\n",
    "                op.append(f)\n",
    "            \n",
    "            \n",
    "    res = [list(i) for j, i in groupby(op, lambda a: a.split('_')[0])]\n",
    "    df1 = pd.DataFrame(res[0])\n",
    "    df2 = pd.DataFrame(res[1])\n",
    "    df3 = pd.DataFrame(res[2])\n",
    "    df4 = pd.DataFrame(res[3])\n",
    "    \n",
    "    column_names = pd.concat([df1,df2,df3,df4], axis=1).to_numpy().flatten().tolist()\n",
    "    nam = [\"names\"]\n",
    "    bind = [\"bind\"]\n",
    "\n",
    "    lt_col = nam + bind + column_names\n",
    "    \n",
    "    return lt_col\n",
    "\n",
    "def df_intCol():\n",
    "    df = pd.read_csv(path1+file2, delimiter=' ',names=four_inter(df1(path1,file2)))\n",
    "    return df\n",
    "    \n",
    "def bind():\n",
    "    df = df_intCol()\n",
    "    df_bind = df['bind']\n",
    "    return df_bind\n",
    "\n",
    "def ary_bind():\n",
    "    df_bind = bind()\n",
    "    ary_bindd = np.asarray(df_bind)\n",
    "    return ary_bindd\n",
    "    \n",
    "\n",
    "def df_SclWmax():\n",
    "    df = df_intCol()\n",
    "    df_max = pd.DataFrame({'max': abs(df.iloc[:,2:]).max(axis=1)})\n",
    "    df2 = pd.concat([df, df_max], axis = 1)\n",
    "    df2_drop=df2.drop(columns=['names','bind','max'])\n",
    "    max_values = df2['max']\n",
    "    df2_div = df2_drop.divide(max_values, axis=0)\n",
    "    df2_scaled = pd.concat([df2[['names','bind']],df2_div], axis = 1)\n",
    "    return df2_scaled\n",
    "\n",
    "def df2_ary():\n",
    "    df2_scaled = df_SclWmax()\n",
    "    df2_ary = np.asarray(df2_scaled.iloc[:,2:])\n",
    "    return df2_ary\n",
    "\n",
    "def data_vec():\n",
    "    num_rows = df_SclWmax().shape[0]\n",
    "    ary1 = []\n",
    "    for i in range(num_rows):\n",
    "        ary1.append(df2_scaled.iloc[:,2:].iloc[i].to_numpy().tolist())\n",
    "    return ary1\n",
    "\n",
    "def n_x_4():\n",
    "    ary1 = data_vec()\n",
    "    ary2 = []\n",
    "    for k in range(len(ary1)):\n",
    "        hj = [ary1[k][i:i + 4] for i in range(0, len(ary1[k]), 4)]\n",
    "        ary2.append(hj)\n",
    "    ary3 = np.array(ary2)\n",
    "    return ary3\n",
    "\n",
    "############## Training & Test Data ###############\n",
    "\n",
    "def train_test1():\n",
    "    training_data, testing_data = train_test_split(n_x_4(), test_size=0.1, random_state=10)\n",
    "    x_train3, x_test3, y_train3, y_test3 = train_test_split(n_x_4(), ary_bind(), test_size=0.1, random_state=10)\n",
    "    \n",
    "    return x_train3, x_test3, y_train3, y_test3\n",
    "\n",
    "def train_test2():\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(df2_ary(), ary_bind(), test_size=0.1, random_state=10)\n",
    "    return x_train2, x_test2, y_train2, y_test2\n",
    "\n",
    "def tr():\n",
    "    x_train2 = train_test2()[0]\n",
    "    tr = np.expand_dims(x_train2, axis=2)\n",
    "    return tr\n",
    "\n",
    "\n",
    "def inp_t2exp():\n",
    "    tr = np.expand_dims(x_train2, axis=2)\n",
    "    return tr\n",
    "\n",
    "################ Contextual Regrssion Model ##################\n",
    "def define_model():\n",
    "    layer0 = Input(shape=tr().shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(12,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "  #  layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer16)\n",
    "    #model.layers[16].trainable = False\n",
    "\n",
    "#    model.summary()\n",
    "    model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "    #model.fit(x_train2, y_train2, batch_size=32, epochs=50, validation_split=0.20)\n",
    "    \n",
    "    return model\n",
    "##############################################################################\n",
    "\n",
    "def gen_pair():\n",
    "\n",
    "    df5 = pd.read_csv(path3, sep='\\\\t', header=None)\n",
    "    df5.columns = ['res1','res2','prot_res','lig_res','arb1','dist','arb2']\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #pd.set_option('display.max_columns', None)\n",
    "    return df5\n",
    "   \n",
    "################# This is to Index weights to rank them #####################\n",
    "def four_inter2(data):\n",
    "    df = []\n",
    "#    print(data)\n",
    "    df = data\n",
    "    c_ln = int(df.shape[1])\n",
    "    n_dim = c_ln/4\n",
    "    \n",
    "    lt = [\"VDW\", \"ELE\", \"GB\", \"SA\"]\n",
    "    op = []\n",
    "    \n",
    "    for k, j in enumerate(lt):\n",
    "        if lt[k] == j:\n",
    "            for i in range(int(n_dim)):\n",
    "                f = j + \"_\" + str(i)\n",
    "                op.append(f)\n",
    "            \n",
    "            \n",
    "    res = [list(i) for j, i in groupby(op, lambda a: a.split('_')[0])]\n",
    "    ddf1 = pd.DataFrame(res[0])\n",
    "    ddf2 = pd.DataFrame(res[1])\n",
    "    ddf3 = pd.DataFrame(res[2])\n",
    "    ddf4 = pd.DataFrame(res[3])\n",
    "    \n",
    "    column_names = pd.concat([ddf1,ddf2,ddf3,ddf4], axis=1).to_numpy().flatten().tolist()\n",
    "\n",
    "    lt_col = column_names\n",
    "    \n",
    "    return lt_col\n",
    "\n",
    "############################ get predictions ################\n",
    "def pred_ary(x_test2):\n",
    "    x_test2 = test_train2()[1]\n",
    "    pre = abs(np.asarray(model.predict(x_test2).round()))\n",
    "    y_test2\n",
    "    ld = [x[0] for x in pre]\n",
    "    ints = [int(item) for item in ld]\n",
    "    pre_ary = np.array(ints)\n",
    "    return pre_ary\n",
    "\n",
    "########## Evaluate Model with Cross-Validation Procedure #######################\n",
    "### Note if you want to loop over kfold you need to change this so it is looopable\n",
    "\n",
    "def evaluate_model(dataX, dataY, n_folds):\n",
    "    scores, histories = list(), list()\n",
    "    # prepare cross validation\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in kfold.split(dataX):\n",
    "    # define model\n",
    "        model = define_model()\n",
    "        # select rows for train and test\n",
    "        trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "        # fit model\n",
    "        earlystop_cb = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "        check_cb = ModelCheckpoint('bestparams2.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "        history = model.fit(x_train2, y_train2, batch_size=32, epochs=50, validation_split=0.20, callbacks=[check_cb],verbose=0) \n",
    "        # evaluate model\n",
    "        _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "        print('> %.3f' % (acc * 100.0))\n",
    "        # append scores\n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "    return scores, histories\n",
    "\n",
    "#def evaluate_model2(X, Y, repeats):\n",
    "# prepare the cross-validation procedure\n",
    "#cv = RepeatedKFold(n_splits=5, n_repeats=repeats, random_state=1)\n",
    "## create model\n",
    "#model = define_model()\n",
    "## evaluate model\n",
    "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "#return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a4cfee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stg3/data1/chad/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 77.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stg3/data1/chad/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 78.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stg3/data1/chad/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 78.60%\n",
      "78.14% (+/- 0.43%)\n"
     ]
    }
   ],
   "source": [
    "def define_model():\n",
    "    layer0 = Input(shape=tr().shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(12,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "  #  layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer16)\n",
    "    #model.layers[16].trainable = False\n",
    "\n",
    "#    model.summary()\n",
    "    model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "    #model.fit(x_train2, y_train2, batch_size=32, epochs=50, validation_split=0.20)\n",
    "\n",
    "    return model\n",
    "\n",
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "# split into input (X) and output (Y) variables\n",
    "X, Y = train_test2()[0], train_test2()[2]\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(train_test2()[0], train_test2()[2]):\n",
    "  # create model\n",
    "    model = define_model()\n",
    "    # Compile model\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], Y[train], epochs=50, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7880b9",
   "metadata": {},
   "source": [
    "# General Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f293a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1635924/2331773265.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df5 = pd.read_csv(path3, sep='\\\\t', header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res1</th>\n",
       "      <th>res2</th>\n",
       "      <th>prot_res</th>\n",
       "      <th>lig_res</th>\n",
       "      <th>arb1</th>\n",
       "      <th>dist</th>\n",
       "      <th>arb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>42560</td>\n",
       "      <td>3.801</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>42560</td>\n",
       "      <td>4.324</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>42560</td>\n",
       "      <td>4.372</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>42560</td>\n",
       "      <td>5.515</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>42560</td>\n",
       "      <td>7.953</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>-2</td>\n",
       "      <td>27360</td>\n",
       "      <td>5.409</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>-5</td>\n",
       "      <td>27360</td>\n",
       "      <td>7.493</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>27360</td>\n",
       "      <td>8.035</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>27360</td>\n",
       "      <td>9.782</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>-6</td>\n",
       "      <td>27360</td>\n",
       "      <td>9.018</td>\n",
       "      <td>39520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     res1  res2  prot_res  lig_res   arb1   dist   arb2\n",
       "0       1     1         1       -3  42560  3.801  39520\n",
       "1       1     1         1       -2  42560  4.324  39520\n",
       "2       1     1         1       -4  42560  4.372  39520\n",
       "3       1     1         1       -1  42560  5.515  39520\n",
       "4       1     1         1       -6  42560  7.953  39520\n",
       "..    ...   ...       ...      ...    ...    ...    ...\n",
       "153     1     1        40       -2  27360  5.409  39520\n",
       "154     1     1        40       -5  27360  7.493  39520\n",
       "155     1     1        40       -1  27360  8.035  39520\n",
       "156     1     1        40        0  27360  9.782  39520\n",
       "157     1     1        40       -6  27360  9.018  39520\n",
       "\n",
       "[158 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_pair():\n",
    "\n",
    "    df5 = pd.read_csv(path3, sep='\\\\t', header=None)\n",
    "    df5.columns = ['res1','res2','prot_res','lig_res','arb1','dist','arb2']\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #pd.set_option('display.max_columns', None)\n",
    "    return df5\n",
    "\n",
    "gen_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14f022",
   "metadata": {},
   "source": [
    "# Other Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "461aaf98",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (812677503.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [71]\u001b[0;36m\u001b[0m\n\u001b[0;31m    svm = SVC(kernel='poly', C, coef0, degree=3)\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def svc_poly(kernel, C, degree, coef0):\n",
    "    svm = SVC(kernel='poly', C, coef0, degree=3)\n",
    "    svm.fit(train_test2()[0], train_test2()[2]) \n",
    "    return svm\n",
    "\n",
    "def RCF(n_estimators=75, max_features=4,max_samples=0.6, random_state=5):\n",
    "    rfc = RandomForestClassifier(n_estimators, max_features,max_samples, random_state)\n",
    "    rfc = rfc.fit(train_test2()[0], train_test2()[2])\n",
    "    return rfc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da58ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">10 0.799 (0.011)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m results, names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# store the results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(scores)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m cv \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# evaluate the model and collect the results\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "        X, y = train_test2()[0], train_test2()[2]\n",
    "        return X, y\n",
    "\n",
    "# explore bagging ensemble number of trees effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "        models = dict()\n",
    "        # define number of trees to consider\n",
    "        n_trees = [10, 50, 100, 500, 500, 1000, 5000]\n",
    "        for n in n_trees:\n",
    "                models[str(n)] = BaggingClassifier(n_estimators=n)\n",
    "        return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "        # define the evaluation procedure\n",
    "        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "        # evaluate the model and collect the results\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        scores = evaluate_model(model, X, y)\n",
    "        # store the results\n",
    "        results.append(scores)\n",
    "        names.append(name)\n",
    "        # summarize the performance along the way\n",
    "        print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c965fd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">4 0.785 (0.005)\n",
      ">5 0.777 (0.009)\n",
      ">6 0.788 (0.007)\n",
      ">7 0.784 (0.005)\n",
      ">8 0.790 (0.004)\n",
      ">9 0.789 (0.011)\n",
      ">10 0.794 (0.004)\n",
      ">11 0.791 (0.006)\n",
      ">12 0.791 (0.006)\n",
      ">13 0.791 (0.007)\n",
      ">14 0.795 (0.005)\n",
      ">15 0.796 (0.007)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f63a955b9d0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a955bca0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a94890c0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9489390>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948a740>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948aa10>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948bdc0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939c0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939d480>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939d750>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939eb00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939edd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bc1c0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bc490>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bd840>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bdb10>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93beec0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bf190>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e8580>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e8850>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e9c00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e9db0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eb160>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eb430>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f63a955bf70>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9488280>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9489660>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9489930>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948ace0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948afb0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939c3a0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939c670>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939da20>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939dcf0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939f0a0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939f370>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bc760>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bca30>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bdde0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93be0b0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bf460>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bf730>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e8b20>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e8df0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93ea080>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93ea350>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eb700>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eb9d0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f63a955b700>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9488df0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948a470>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948baf0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939d1b0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939e830>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939feb0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bd570>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bebf0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e82b0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e9930>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eae90>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f63a9488550>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9489c00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948b280>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939c940>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939dfc0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939f640>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bcd00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93be380>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bfa00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e90c0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93ea620>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93ebca0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f63a9488af0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948a1a0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948b820>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939cee0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939e560>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939fbe0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bd2a0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93be920>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bffa0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e9660>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93eabc0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a94103d0>],\n",
       " 'means': [<matplotlib.lines.Line2D at 0x7f63a9488820>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9489ed0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a948b550>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939cc10>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939e290>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a939f910>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bcfd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93be650>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93bfcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93e9390>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a93ea8f0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f63a9410100>]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# explore the number of selected features for RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "        X, y = train_test2()[0], train_test2()[2]\n",
    "        return X, y\n",
    "    \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "        models = dict()\n",
    "        for i in range(4,664,4):\n",
    "            models[str(i)] = RandomForestClassifier(n_estimators=i)\n",
    "                #rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "                #models = RandomForestClassifier(max_features=i)\n",
    "                #models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "        return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "        cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=1)\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "        return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "        scores = evaluate_model(model, X, y)\n",
    "        results.append(scores)\n",
    "        names.append(name)\n",
    "        print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7676ec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, coef0=1, degree=5, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, coef0=1, degree=5, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, coef0=1, degree=5, kernel='poly')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=75, max_features=4,max_samples=0.6, random_state=5)\n",
    "rfc.fit(train_test2()[0], train_test2()[2])\n",
    "#clf = SVC(C = 1, kernel = 'linear')\n",
    "#clf.fit(x_train2, y_train2) \n",
    "clf2 = SVC(kernel='poly', C=1, coef0=1, degree=5)\n",
    "clf2.fit(train_test2()[0], train_test2()[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fb52f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Get row max values\n",
    "df_max = pd.DataFrame({'max': abs(df.iloc[:,2:]).max(axis=1)})\n",
    "\n",
    "## Add a column to contain max values from each row\n",
    "df2 = pd.concat([df1(path1,file2), df_max], axis = 1)\n",
    "df_bind = df['bind']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5956d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a383d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_drop=df2.drop(columns=['names','bind','max'])\n",
    "max_values = df2['max']\n",
    "\n",
    "df2_div = df2_drop.divide(max_values, axis=0)\n",
    "df2_scaled = pd.concat([df2[['names','bind']],df2_div], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "726e9291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>bind</th>\n",
       "      <th>VDW_0</th>\n",
       "      <th>ELE_0</th>\n",
       "      <th>GB_0</th>\n",
       "      <th>SA_0</th>\n",
       "      <th>VDW_1</th>\n",
       "      <th>ELE_1</th>\n",
       "      <th>GB_1</th>\n",
       "      <th>SA_1</th>\n",
       "      <th>...</th>\n",
       "      <th>GB_163</th>\n",
       "      <th>SA_163</th>\n",
       "      <th>VDW_164</th>\n",
       "      <th>ELE_164</th>\n",
       "      <th>GB_164</th>\n",
       "      <th>SA_164</th>\n",
       "      <th>VDW_165</th>\n",
       "      <th>ELE_165</th>\n",
       "      <th>GB_165</th>\n",
       "      <th>SA_165</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBX1-1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.023687</td>\n",
       "      <td>-0.073302</td>\n",
       "      <td>0.041080</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.025577</td>\n",
       "      <td>-0.064578</td>\n",
       "      <td>0.063390</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056962</td>\n",
       "      <td>-0.008562</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>-0.349251</td>\n",
       "      <td>-0.009480</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.652586</td>\n",
       "      <td>-0.011047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBX1-1_10</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.024464</td>\n",
       "      <td>-0.080057</td>\n",
       "      <td>0.042976</td>\n",
       "      <td>-0.003977</td>\n",
       "      <td>-0.031903</td>\n",
       "      <td>-0.092275</td>\n",
       "      <td>0.086724</td>\n",
       "      <td>-0.007783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064521</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>-0.009642</td>\n",
       "      <td>-0.189185</td>\n",
       "      <td>-0.379714</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.036624</td>\n",
       "      <td>-0.841144</td>\n",
       "      <td>0.677396</td>\n",
       "      <td>-0.013391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBX1-1_100</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.032261</td>\n",
       "      <td>-0.032713</td>\n",
       "      <td>-0.004589</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>-0.031775</td>\n",
       "      <td>-0.085660</td>\n",
       "      <td>0.069842</td>\n",
       "      <td>-0.007787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085451</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>-0.001912</td>\n",
       "      <td>-0.675891</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>-0.011333</td>\n",
       "      <td>-0.036642</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.831809</td>\n",
       "      <td>-0.016165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBX1-1_101</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026858</td>\n",
       "      <td>-0.093437</td>\n",
       "      <td>0.047046</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>-0.033081</td>\n",
       "      <td>-0.014620</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>-0.007950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031533</td>\n",
       "      <td>-0.009886</td>\n",
       "      <td>-0.002233</td>\n",
       "      <td>-0.468973</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>-0.058718</td>\n",
       "      <td>-0.697505</td>\n",
       "      <td>0.680860</td>\n",
       "      <td>-0.017776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBX1-1_102</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.064613</td>\n",
       "      <td>-0.226965</td>\n",
       "      <td>0.114883</td>\n",
       "      <td>-0.009562</td>\n",
       "      <td>-0.091865</td>\n",
       "      <td>-0.260911</td>\n",
       "      <td>0.205587</td>\n",
       "      <td>-0.019193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083328</td>\n",
       "      <td>-0.027799</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>-0.023018</td>\n",
       "      <td>-0.109009</td>\n",
       "      <td>-0.232225</td>\n",
       "      <td>-0.085103</td>\n",
       "      <td>-0.038795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 666 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        names  bind     VDW_0     ELE_0      GB_0      SA_0     VDW_1  \\\n",
       "0    CBX1-1_1     1 -0.023687 -0.073302  0.041080 -0.003970 -0.025577   \n",
       "1   CBX1-1_10     1 -0.024464 -0.080057  0.042976 -0.003977 -0.031903   \n",
       "2  CBX1-1_100     1 -0.032261 -0.032713 -0.004589 -0.005076 -0.031775   \n",
       "3  CBX1-1_101     0 -0.026858 -0.093437  0.047046 -0.004139 -0.033081   \n",
       "4  CBX1-1_102     0 -0.064613 -0.226965  0.114883 -0.009562 -0.091865   \n",
       "\n",
       "      ELE_1      GB_1      SA_1  ...    GB_163    SA_163   VDW_164   ELE_164  \\\n",
       "0 -0.064578  0.063390 -0.006860  ... -0.056962 -0.008562  0.003808  0.064902   \n",
       "1 -0.092275  0.086724 -0.007783  ... -0.064521 -0.009185 -0.009642 -0.189185   \n",
       "2 -0.085660  0.069842 -0.007787  ... -0.085451 -0.010742 -0.001912 -0.675891   \n",
       "3 -0.014620  0.008248 -0.007950  ... -0.031533 -0.009886 -0.002233 -0.468973   \n",
       "4 -0.260911  0.205587 -0.019193  ... -0.083328 -0.027799 -0.020490 -1.000000   \n",
       "\n",
       "     GB_164    SA_164   VDW_165   ELE_165    GB_165    SA_165  \n",
       "0 -0.349251 -0.009480 -0.019392 -1.000000  0.652586 -0.011047  \n",
       "1 -0.379714 -0.009557 -0.036624 -0.841144  0.677396 -0.013391  \n",
       "2  0.049296 -0.011333 -0.036642 -1.000000  0.831809 -0.016165  \n",
       "3  0.017181 -0.009379 -0.058718 -0.697505  0.680860 -0.017776  \n",
       "4  0.114200 -0.023018 -0.109009 -0.232225 -0.085103 -0.038795  \n",
       "\n",
       "[5 rows x 666 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ccb2175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5484, 664)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_ary = np.asarray(df2_scaled.iloc[:,2:])\n",
    "df2_ary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "320bc000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VDW_0     -0.023687\n",
       "ELE_0     -0.073302\n",
       "GB_0       0.041080\n",
       "SA_0      -0.003970\n",
       "VDW_1     -0.025577\n",
       "             ...   \n",
       "SA_164    -0.009480\n",
       "VDW_165   -0.019392\n",
       "ELE_165   -1.000000\n",
       "GB_165     0.652586\n",
       "SA_165    -0.011047\n",
       "Name: 0, Length: 664, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_list = df2_scaled.iloc[:,2:].iloc[0].to_numpy().tolist()\n",
    "df2_scaled.iloc[:,2:].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e2eb1995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5484"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows = df2_scaled.shape[0]\n",
    "num_cols1 = df2_scaled.shape[1]\n",
    "num_cols2 = df2_scaled.iloc[:,2:].shape[1]\n",
    "df2_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d57b8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ary1 = []\n",
    "for i in range(num_rows):\n",
    "    ary1.append(df2_scaled.iloc[:,2:].iloc[i].to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3669b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(ary1).shape\n",
    "ary_bindd = np.asarray(df_bind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c82f36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ary2 = []\n",
    "for k in range(len(ary1)):\n",
    "    hj = [ary1[k][i:i + 4] for i in range(0, len(ary1[k]), 4)]\n",
    "    ary2.append(hj)\n",
    "ary3 = np.array(ary2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c091d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(ary3, test_size=0.1, random_state=10)\n",
    "\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(ary3, ary_bindd, test_size=0.1, random_state=10)\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df2_ary, ary_bindd, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e32714f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4935, 664, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = np.expand_dims(x_train2, axis=2)\n",
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45c3777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import *\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from keras.optimizers import Adam, SGD\n",
    "from keras.layers import *\n",
    "from keras.metrics import *\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from hyperopt import hp, STATUS_OK\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, GlobalAveragePooling1D,AveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "## sequence.pad_sequence(train_x, maxlen=max_words)\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce7699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 664, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " Conv1D_1 (Conv1D)              (None, 661, 16)      80          ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 661, 16)     64          ['Conv1D_1[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 330, 16)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " Conv1D_2 (Conv1D)              (None, 327, 16)      1040        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 163, 16)     0           ['Conv1D_2[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 163, 16)      0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " Conv1D_3 (Conv1D)              (None, 160, 10)      650         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling1d (AveragePool  (None, 80, 10)      0           ['Conv1D_3[0][0]']               \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 80, 10)       0           ['average_pooling1d[0][0]']      \n",
      "                                                                                                  \n",
      " Conv1D_4 (Conv1D)              (None, 77, 8)        328         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling1d_1 (AveragePo  (None, 38, 8)       0           ['Conv1D_4[0][0]']               \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " Conv1D_5 (Conv1D)              (None, 38, 4)        36          ['average_pooling1d_1[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['Conv1D_5[0][0]']               \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " Flatten_1 (Flatten)            (None, 4)            0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)                (None, 20)           100         ['Flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 20)           0           ['Dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " Dense_3 (Dense)                (None, 1)            21          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1)            0           ['Dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " Flatten_2 (Flatten)            (None, 664)          0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " Contextual_Weight (Dense)      (None, 664)          1328        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 664)          0           ['Flatten_2[0][0]',              \n",
      "                                                                  'Contextual_Weight[0][0]']      \n",
      "                                                                                                  \n",
      " Sum (Dense)                    (None, 1)            664         ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            2           ['Sum[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,313\n",
      "Trainable params: 4,279\n",
      "Non-trainable params: 34\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:50:06.227626: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "\n",
    "###  Convolutional Layer\n",
    "layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "#model.add(LeakyReLU())\n",
    "layer3 = MaxPool1D(pool_size=2, strides=2)(layer2)\n",
    "\n",
    "layer4 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_2')(layer3)\n",
    "layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "layer6 = Dropout(0.5, name='dropout_1')(layer5)\n",
    "\n",
    "layer7 = Conv1D(filters=10, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_3')(layer6)\n",
    "layer8 = AveragePooling1D(pool_size=2, strides=2)(layer7)\n",
    "layer9 = Dropout(0.5, name='dropout_2')(layer8)\n",
    "\n",
    "layer10 = Conv1D(filters=8, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_4')(layer9)\n",
    "layer11 = AveragePooling1D(pool_size=2, strides=2)(layer10)\n",
    "\n",
    "layer12 = Conv1D(filters=4, kernel_size=1, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_5')(layer11)\n",
    "layer13 = GlobalAveragePooling1D()(layer12)\n",
    "\n",
    "layer14 = Flatten(name='Flatten_1')(layer13)\n",
    "layer15 = Dense(20,activation='relu',name='Dense_1')(layer14)\n",
    "layer16 = Dropout(0.00099,name='dropout_3')(layer15)\n",
    "layer17 = Dense(1,activation='relu',name='Dense_3')(layer16)\n",
    "\n",
    "layer18 = Dropout(0.00099,name='dropout_4')(layer17)\n",
    "\n",
    "### Contextual regression\n",
    "layer19 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer18) ## make first layer\n",
    "layer20 = Dropout(0.00099,name='dropout_5')(layer19)\n",
    "\n",
    "#### Dot Product\n",
    "layer21 = Flatten(name='Flatten_2')(layer0)\n",
    "layer22 = Multiply()([layer21,layer19]) ## same number of nodes as input feaures\n",
    "\n",
    "layer23 = Dense(1,kernel_initializer='ones',use_bias=False,name='Sum')(layer22)\n",
    "layer24 = Dense(1,name='Output')(layer23) \n",
    "model = Model(inputs=layer0, outputs=layer24)\n",
    "model.layers[23].trainable = False\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, \"model_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ea2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reduced Model for Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e7cc1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 664, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " Conv1D_1 (Conv1D)              (None, 661, 16)      80          ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 661, 16)     64          ['Conv1D_1[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_70 (MaxPooling1D  (None, 661, 16)     0           ['batch_normalization_35[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv1D_2 (Conv1D)              (None, 658, 12)      780         ['max_pooling1d_70[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_71 (MaxPooling1D  (None, 329, 12)     0           ['Conv1D_2[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_35 (G  (None, 12)          0           ['max_pooling1d_71[0][0]']       \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " Flatten_1 (Flatten)            (None, 12)           0           ['global_average_pooling1d_35[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)                (None, 12)           156         ['Flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 12)           0           ['Dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " Dense_3 (Dense)                (None, 1)            13          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1)            0           ['Dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " Contextual_Weight (Dense)      (None, 664)          1328        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 664)          0           ['Contextual_Weight[0][0]']      \n",
      "                                                                                                  \n",
      " Flatten_2 (Flatten)            (None, 664)          0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_35 (Multiply)         (None, 664)          0           ['dropout_5[0][0]',              \n",
      "                                                                  'Flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " Sum (Dense)                    (None, 1)            664         ['multiply_35[0][0]']            \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            2           ['Sum[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,087\n",
      "Trainable params: 3,055\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(120)\n",
    "\n",
    "layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "model = Model(inputs=layer0, outputs=layer17)\n",
    "#model.layers[16].trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c5de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f3e6da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded (None, 5216)\n",
      "shape of decoded (None, 320, 32)\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 664, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_97 (Conv1D)          (None, 661, 64)           320       \n",
      "                                                                 \n",
      " max_pooling1d_52 (MaxPoolin  (None, 330, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_98 (Conv1D)          (None, 327, 32)           8224      \n",
      "                                                                 \n",
      " max_pooling1d_53 (MaxPoolin  (None, 163, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_99 (Conv1D)          (None, 160, 32)           4128      \n",
      "                                                                 \n",
      " up_sampling1d_45 (UpSamplin  (None, 320, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_100 (Conv1D)         (None, 317, 64)           8256      \n",
      "                                                                 \n",
      " up_sampling1d_46 (UpSamplin  (None, 634, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_33 (Flatten)        (None, 40576)             0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 664)               26943128  \n",
      "                                                                 \n",
      " reshape_25 (Reshape)        (None, 664, 1)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,964,056\n",
      "Trainable params: 26,964,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "input_sig =Input(shape=tr.shape[1:], name='input')\n",
    "x = Conv1D(64,4, activation='relu', padding='valid')(input_sig)\n",
    "x1 = MaxPooling1D(2)(x)\n",
    "x2 = Conv1D(32,4, activation='relu', padding='valid')(x1)\n",
    "x3 = MaxPooling1D(2)(x2)\n",
    "flat = Flatten()(x3)\n",
    "encoded = Dense(32,activation = 'relu')(flat)\n",
    "\n",
    "print(\"shape of encoded {}\".format(K.int_shape(flat)))\n",
    "\n",
    "x2_ = Conv1D(32, 4, activation='relu', padding='valid')(x3)\n",
    "x1_ = UpSampling1D(2)(x2_)\n",
    "x_ = Conv1D(64, 4, activation='relu', padding='valid')(x1_)\n",
    "upsamp = UpSampling1D(2)(x_)\n",
    "flat = Flatten()(upsamp)\n",
    "decoded = Dense(664,activation = 'relu')(flat)\n",
    "decoded = Reshape((664,1))(decoded)\n",
    "\n",
    "print(\"shape of decoded {}\".format(K.int_shape(x1_)))\n",
    "\n",
    "model = Model(input_sig, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36adb52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stg3/data1/chad/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1901 - accuracy: 0.7579\n",
      "Epoch 1: val_loss improved from inf to 0.18833, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.1897 - accuracy: 0.7586 - val_loss: 0.1883 - val_accuracy: 0.7568\n",
      "Epoch 2/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1859 - accuracy: 0.7600\n",
      "Epoch 2: val_loss improved from 0.18833 to 0.18641, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1866 - accuracy: 0.7586 - val_loss: 0.1864 - val_accuracy: 0.7568\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.7584\n",
      "Epoch 3: val_loss improved from 0.18641 to 0.18375, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1858 - accuracy: 0.7586 - val_loss: 0.1838 - val_accuracy: 0.7568\n",
      "Epoch 4/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1811 - accuracy: 0.7589\n",
      "Epoch 4: val_loss improved from 0.18375 to 0.18121, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1814 - accuracy: 0.7586 - val_loss: 0.1812 - val_accuracy: 0.7568\n",
      "Epoch 5/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.7599\n",
      "Epoch 5: val_loss improved from 0.18121 to 0.16839, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1742 - accuracy: 0.7596 - val_loss: 0.1684 - val_accuracy: 0.7599\n",
      "Epoch 6/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1690 - accuracy: 0.7671\n",
      "Epoch 6: val_loss improved from 0.16839 to 0.16446, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1688 - accuracy: 0.7670 - val_loss: 0.1645 - val_accuracy: 0.7680\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.7696\n",
      "Epoch 7: val_loss improved from 0.16446 to 0.16271, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1660 - accuracy: 0.7695 - val_loss: 0.1627 - val_accuracy: 0.7710\n",
      "Epoch 8/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.7675\n",
      "Epoch 8: val_loss improved from 0.16271 to 0.16261, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1637 - accuracy: 0.7687 - val_loss: 0.1626 - val_accuracy: 0.7670\n",
      "Epoch 9/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1630 - accuracy: 0.7710\n",
      "Epoch 9: val_loss did not improve from 0.16261\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1632 - accuracy: 0.7708 - val_loss: 0.1642 - val_accuracy: 0.7680\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.7762\n",
      "Epoch 10: val_loss did not improve from 0.16261\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1621 - accuracy: 0.7751 - val_loss: 0.1657 - val_accuracy: 0.7589\n",
      "Epoch 11/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.7667\n",
      "Epoch 11: val_loss improved from 0.16261 to 0.16157, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1621 - accuracy: 0.7680 - val_loss: 0.1616 - val_accuracy: 0.7700\n",
      "Epoch 12/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1611 - accuracy: 0.7700\n",
      "Epoch 12: val_loss improved from 0.16157 to 0.16020, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1610 - accuracy: 0.7695 - val_loss: 0.1602 - val_accuracy: 0.7629\n",
      "Epoch 13/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.7686\n",
      "Epoch 13: val_loss did not improve from 0.16020\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1605 - accuracy: 0.7708 - val_loss: 0.1612 - val_accuracy: 0.7700\n",
      "Epoch 14/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1583 - accuracy: 0.7759\n",
      "Epoch 14: val_loss did not improve from 0.16020\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1591 - accuracy: 0.7741 - val_loss: 0.1610 - val_accuracy: 0.7741\n",
      "Epoch 15/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1606 - accuracy: 0.7693\n",
      "Epoch 15: val_loss improved from 0.16020 to 0.15825, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1592 - accuracy: 0.7725 - val_loss: 0.1582 - val_accuracy: 0.7639\n",
      "Epoch 16/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.7754\n",
      "Epoch 16: val_loss did not improve from 0.15825\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1584 - accuracy: 0.7756 - val_loss: 0.1588 - val_accuracy: 0.7781\n",
      "Epoch 17/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.7756\n",
      "Epoch 17: val_loss did not improve from 0.15825\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1579 - accuracy: 0.7771 - val_loss: 0.1583 - val_accuracy: 0.7710\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.7743\n",
      "Epoch 18: val_loss did not improve from 0.15825\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1578 - accuracy: 0.7730 - val_loss: 0.1606 - val_accuracy: 0.7730\n",
      "Epoch 19/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.7780\n",
      "Epoch 19: val_loss improved from 0.15825 to 0.15767, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1576 - accuracy: 0.7779 - val_loss: 0.1577 - val_accuracy: 0.7710\n",
      "Epoch 20/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1586 - accuracy: 0.7734\n",
      "Epoch 20: val_loss did not improve from 0.15767\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1581 - accuracy: 0.7751 - val_loss: 0.1607 - val_accuracy: 0.7761\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.7780\n",
      "Epoch 21: val_loss improved from 0.15767 to 0.15614, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1571 - accuracy: 0.7779 - val_loss: 0.1561 - val_accuracy: 0.7822\n",
      "Epoch 22/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1562 - accuracy: 0.7749\n",
      "Epoch 22: val_loss did not improve from 0.15614\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1560 - accuracy: 0.7771 - val_loss: 0.1627 - val_accuracy: 0.7710\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.7788\n",
      "Epoch 23: val_loss improved from 0.15614 to 0.15587, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1555 - accuracy: 0.7791 - val_loss: 0.1559 - val_accuracy: 0.7751\n",
      "Epoch 24/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.7780\n",
      "Epoch 24: val_loss improved from 0.15587 to 0.15506, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1549 - accuracy: 0.7781 - val_loss: 0.1551 - val_accuracy: 0.7852\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.7816\n",
      "Epoch 25: val_loss did not improve from 0.15506\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1550 - accuracy: 0.7814 - val_loss: 0.1581 - val_accuracy: 0.7822\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.7845\n",
      "Epoch 26: val_loss did not improve from 0.15506\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1548 - accuracy: 0.7842 - val_loss: 0.1582 - val_accuracy: 0.7751\n",
      "Epoch 27/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.7760\n",
      "Epoch 27: val_loss did not improve from 0.15506\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1568 - accuracy: 0.7789 - val_loss: 0.1564 - val_accuracy: 0.7812\n",
      "Epoch 28/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.7816\n",
      "Epoch 28: val_loss improved from 0.15506 to 0.15463, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1553 - accuracy: 0.7801 - val_loss: 0.1546 - val_accuracy: 0.7791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1543 - accuracy: 0.7806\n",
      "Epoch 29: val_loss improved from 0.15463 to 0.15331, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1543 - accuracy: 0.7806 - val_loss: 0.1533 - val_accuracy: 0.7842\n",
      "Epoch 30/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1542 - accuracy: 0.7810\n",
      "Epoch 30: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1542 - accuracy: 0.7809 - val_loss: 0.1548 - val_accuracy: 0.7842\n",
      "Epoch 31/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.7827\n",
      "Epoch 31: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1539 - accuracy: 0.7829 - val_loss: 0.1694 - val_accuracy: 0.7670\n",
      "Epoch 32/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1539 - accuracy: 0.7823\n",
      "Epoch 32: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1551 - accuracy: 0.7806 - val_loss: 0.1541 - val_accuracy: 0.7893\n",
      "Epoch 33/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1548 - accuracy: 0.7824\n",
      "Epoch 33: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1539 - accuracy: 0.7842 - val_loss: 0.1539 - val_accuracy: 0.7832\n",
      "Epoch 34/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.7811\n",
      "Epoch 34: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1527 - accuracy: 0.7827 - val_loss: 0.1533 - val_accuracy: 0.7812\n",
      "Epoch 35/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.7845\n",
      "Epoch 35: val_loss did not improve from 0.15331\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1521 - accuracy: 0.7839 - val_loss: 0.1543 - val_accuracy: 0.7852\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.7845\n",
      "Epoch 36: val_loss improved from 0.15331 to 0.15273, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1529 - accuracy: 0.7834 - val_loss: 0.1527 - val_accuracy: 0.7801\n",
      "Epoch 37/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.7866\n",
      "Epoch 37: val_loss did not improve from 0.15273\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1519 - accuracy: 0.7855 - val_loss: 0.1539 - val_accuracy: 0.7862\n",
      "Epoch 38/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1531 - accuracy: 0.7865\n",
      "Epoch 38: val_loss did not improve from 0.15273\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1532 - accuracy: 0.7855 - val_loss: 0.1620 - val_accuracy: 0.7781\n",
      "Epoch 39/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.7886\n",
      "Epoch 39: val_loss did not improve from 0.15273\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1526 - accuracy: 0.7865 - val_loss: 0.1561 - val_accuracy: 0.7953\n",
      "Epoch 40/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1531 - accuracy: 0.7873\n",
      "Epoch 40: val_loss did not improve from 0.15273\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1521 - accuracy: 0.7888 - val_loss: 0.1556 - val_accuracy: 0.7822\n",
      "Epoch 41/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.7892\n",
      "Epoch 41: val_loss improved from 0.15273 to 0.15243, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1507 - accuracy: 0.7905 - val_loss: 0.1524 - val_accuracy: 0.7882\n",
      "Epoch 42/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1513 - accuracy: 0.7869\n",
      "Epoch 42: val_loss did not improve from 0.15243\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1517 - accuracy: 0.7867 - val_loss: 0.1551 - val_accuracy: 0.7791\n",
      "Epoch 43/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.7900\n",
      "Epoch 43: val_loss did not improve from 0.15243\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.7893 - val_loss: 0.1563 - val_accuracy: 0.7812\n",
      "Epoch 44/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1517 - accuracy: 0.7872\n",
      "Epoch 44: val_loss did not improve from 0.15243\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1516 - accuracy: 0.7867 - val_loss: 0.1549 - val_accuracy: 0.7842\n",
      "Epoch 45/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.7895\n",
      "Epoch 45: val_loss improved from 0.15243 to 0.15086, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1508 - accuracy: 0.7895 - val_loss: 0.1509 - val_accuracy: 0.7893\n",
      "Epoch 46/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.7863\n",
      "Epoch 46: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1501 - accuracy: 0.7867 - val_loss: 0.1526 - val_accuracy: 0.7872\n",
      "Epoch 47/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.7903\n",
      "Epoch 47: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1499 - accuracy: 0.7915 - val_loss: 0.1576 - val_accuracy: 0.7801\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.7929\n",
      "Epoch 48: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.7926 - val_loss: 0.1544 - val_accuracy: 0.7852\n",
      "Epoch 49/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.7921\n",
      "Epoch 49: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1494 - accuracy: 0.7915 - val_loss: 0.1521 - val_accuracy: 0.7872\n",
      "Epoch 50/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1490 - accuracy: 0.7891\n",
      "Epoch 50: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.7880 - val_loss: 0.1514 - val_accuracy: 0.7842\n",
      "Epoch 51/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.7902\n",
      "Epoch 51: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.7908 - val_loss: 0.1520 - val_accuracy: 0.7852\n",
      "Epoch 52/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1489 - accuracy: 0.7961\n",
      "Epoch 52: val_loss did not improve from 0.15086\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1489 - accuracy: 0.7953 - val_loss: 0.1520 - val_accuracy: 0.7913\n",
      "Epoch 53/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.7942\n",
      "Epoch 53: val_loss improved from 0.15086 to 0.15072, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1491 - accuracy: 0.7933 - val_loss: 0.1507 - val_accuracy: 0.7882\n",
      "Epoch 54/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.7926\n",
      "Epoch 54: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1491 - accuracy: 0.7933 - val_loss: 0.1559 - val_accuracy: 0.7812\n",
      "Epoch 55/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1483 - accuracy: 0.7915\n",
      "Epoch 55: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1486 - accuracy: 0.7903 - val_loss: 0.1581 - val_accuracy: 0.7862\n",
      "Epoch 56/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.7931\n",
      "Epoch 56: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1479 - accuracy: 0.7951 - val_loss: 0.1524 - val_accuracy: 0.7913\n",
      "Epoch 57/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.7947\n",
      "Epoch 57: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1473 - accuracy: 0.7938 - val_loss: 0.1528 - val_accuracy: 0.7842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.7958\n",
      "Epoch 58: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1484 - accuracy: 0.7938 - val_loss: 0.1602 - val_accuracy: 0.7781\n",
      "Epoch 59/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.7939\n",
      "Epoch 59: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1487 - accuracy: 0.7966 - val_loss: 0.1528 - val_accuracy: 0.7822\n",
      "Epoch 60/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1475 - accuracy: 0.7991\n",
      "Epoch 60: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1478 - accuracy: 0.7989 - val_loss: 0.1542 - val_accuracy: 0.7842\n",
      "Epoch 61/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.7938\n",
      "Epoch 61: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1474 - accuracy: 0.7951 - val_loss: 0.1514 - val_accuracy: 0.7943\n",
      "Epoch 62/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.7937\n",
      "Epoch 62: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1472 - accuracy: 0.7943 - val_loss: 0.1568 - val_accuracy: 0.7812\n",
      "Epoch 63/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1493 - accuracy: 0.7910\n",
      "Epoch 63: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1487 - accuracy: 0.7920 - val_loss: 0.1520 - val_accuracy: 0.7882\n",
      "Epoch 64/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1451 - accuracy: 0.7978\n",
      "Epoch 64: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1452 - accuracy: 0.7979 - val_loss: 0.1561 - val_accuracy: 0.7832\n",
      "Epoch 65/150\n",
      "241/247 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.7990\n",
      "Epoch 65: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.1470 - accuracy: 0.7981 - val_loss: 0.1564 - val_accuracy: 0.7791\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.7947\n",
      "Epoch 66: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1484 - accuracy: 0.7936 - val_loss: 0.1535 - val_accuracy: 0.7882\n",
      "Epoch 67/150\n",
      "241/247 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.7975\n",
      "Epoch 67: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.7981 - val_loss: 0.1519 - val_accuracy: 0.7882\n",
      "Epoch 68/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.7921\n",
      "Epoch 68: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.7918 - val_loss: 0.1656 - val_accuracy: 0.7812\n",
      "Epoch 69/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1469 - accuracy: 0.7912\n",
      "Epoch 69: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1457 - accuracy: 0.7933 - val_loss: 0.1564 - val_accuracy: 0.7842\n",
      "Epoch 70/150\n",
      "245/247 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.7990\n",
      "Epoch 70: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.7991 - val_loss: 0.1512 - val_accuracy: 0.7903\n",
      "Epoch 71/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.8015\n",
      "Epoch 71: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7999 - val_loss: 0.1556 - val_accuracy: 0.7872\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.7952\n",
      "Epoch 72: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7979 - val_loss: 0.1524 - val_accuracy: 0.7852\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.7973\n",
      "Epoch 73: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1469 - accuracy: 0.7956 - val_loss: 0.1548 - val_accuracy: 0.7842\n",
      "Epoch 74/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.7952\n",
      "Epoch 74: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.7943 - val_loss: 0.1577 - val_accuracy: 0.7852\n",
      "Epoch 75/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1459 - accuracy: 0.7988\n",
      "Epoch 75: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.7984 - val_loss: 0.1541 - val_accuracy: 0.7893\n",
      "Epoch 76/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1445 - accuracy: 0.8025\n",
      "Epoch 76: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1452 - accuracy: 0.8007 - val_loss: 0.1523 - val_accuracy: 0.7913\n",
      "Epoch 77/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.7960\n",
      "Epoch 77: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.7958 - val_loss: 0.1539 - val_accuracy: 0.7781\n",
      "Epoch 78/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1453 - accuracy: 0.8004\n",
      "Epoch 78: val_loss did not improve from 0.15072\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.8019 - val_loss: 0.1530 - val_accuracy: 0.7933\n",
      "Epoch 79/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.8017\n",
      "Epoch 79: val_loss improved from 0.15072 to 0.15019, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.8004 - val_loss: 0.1502 - val_accuracy: 0.7913\n",
      "Epoch 80/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1468 - accuracy: 0.7946\n",
      "Epoch 80: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7969 - val_loss: 0.1543 - val_accuracy: 0.7893\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.7968\n",
      "Epoch 81: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1450 - accuracy: 0.7961 - val_loss: 0.1515 - val_accuracy: 0.7822\n",
      "Epoch 82/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1449 - accuracy: 0.7939\n",
      "Epoch 82: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.7943 - val_loss: 0.1518 - val_accuracy: 0.7893\n",
      "Epoch 83/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.8007\n",
      "Epoch 83: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1443 - accuracy: 0.8007 - val_loss: 0.1517 - val_accuracy: 0.7862\n",
      "Epoch 84/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.7984\n",
      "Epoch 84: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1449 - accuracy: 0.7986 - val_loss: 0.1544 - val_accuracy: 0.7842\n",
      "Epoch 85/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.7942\n",
      "Epoch 85: val_loss improved from 0.15019 to 0.14914, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1452 - accuracy: 0.7956 - val_loss: 0.1491 - val_accuracy: 0.7933\n",
      "Epoch 86/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1459 - accuracy: 0.7952\n",
      "Epoch 86: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7956 - val_loss: 0.1505 - val_accuracy: 0.7903\n",
      "Epoch 87/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.8054\n",
      "Epoch 87: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1417 - accuracy: 0.8047 - val_loss: 0.1539 - val_accuracy: 0.7852\n",
      "Epoch 88/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1456 - accuracy: 0.7979\n",
      "Epoch 88: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.8004 - val_loss: 0.1514 - val_accuracy: 0.7842\n",
      "Epoch 89/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.7992\n",
      "Epoch 89: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7986 - val_loss: 0.1542 - val_accuracy: 0.7913\n",
      "Epoch 90/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1440 - accuracy: 0.7967\n",
      "Epoch 90: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7979 - val_loss: 0.1552 - val_accuracy: 0.7933\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.7994\n",
      "Epoch 91: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1423 - accuracy: 0.8002 - val_loss: 0.1556 - val_accuracy: 0.7872\n",
      "Epoch 92/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8007\n",
      "Epoch 92: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.7986 - val_loss: 0.1496 - val_accuracy: 0.8004\n",
      "Epoch 93/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.7961\n",
      "Epoch 93: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.7986 - val_loss: 0.1544 - val_accuracy: 0.7832\n",
      "Epoch 94/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.7955\n",
      "Epoch 94: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.7958 - val_loss: 0.1516 - val_accuracy: 0.7923\n",
      "Epoch 95/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.7937\n",
      "Epoch 95: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1443 - accuracy: 0.7961 - val_loss: 0.1515 - val_accuracy: 0.7943\n",
      "Epoch 96/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.8036\n",
      "Epoch 96: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.8037 - val_loss: 0.1532 - val_accuracy: 0.7872\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.8007\n",
      "Epoch 97: val_loss did not improve from 0.14914\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.8002 - val_loss: 0.1536 - val_accuracy: 0.7893\n",
      "Epoch 98/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8010\n",
      "Epoch 98: val_loss improved from 0.14914 to 0.14892, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1421 - accuracy: 0.7989 - val_loss: 0.1489 - val_accuracy: 0.7923\n",
      "Epoch 99/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.7999\n",
      "Epoch 99: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.8004 - val_loss: 0.1527 - val_accuracy: 0.7903\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.8052\n",
      "Epoch 100: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.8037 - val_loss: 0.1511 - val_accuracy: 0.7893\n",
      "Epoch 101/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1432 - accuracy: 0.8028\n",
      "Epoch 101: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.8029 - val_loss: 0.1504 - val_accuracy: 0.7872\n",
      "Epoch 102/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.7924\n",
      "Epoch 102: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.7918 - val_loss: 0.1506 - val_accuracy: 0.7943\n",
      "Epoch 103/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1433 - accuracy: 0.7991\n",
      "Epoch 103: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1436 - accuracy: 0.7999 - val_loss: 0.1549 - val_accuracy: 0.7862\n",
      "Epoch 104/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1408 - accuracy: 0.8004\n",
      "Epoch 104: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7986 - val_loss: 0.1510 - val_accuracy: 0.7832\n",
      "Epoch 105/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.8026\n",
      "Epoch 105: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1421 - accuracy: 0.8029 - val_loss: 0.1518 - val_accuracy: 0.7903\n",
      "Epoch 106/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.7989\n",
      "Epoch 106: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.7991 - val_loss: 0.1527 - val_accuracy: 0.7893\n",
      "Epoch 107/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.7976\n",
      "Epoch 107: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.7971 - val_loss: 0.1522 - val_accuracy: 0.7913\n",
      "Epoch 108/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.8028\n",
      "Epoch 108: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.8027 - val_loss: 0.1534 - val_accuracy: 0.7933\n",
      "Epoch 109/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.8026\n",
      "Epoch 109: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.8019 - val_loss: 0.1505 - val_accuracy: 0.7893\n",
      "Epoch 110/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1421 - accuracy: 0.7969\n",
      "Epoch 110: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.7979 - val_loss: 0.1611 - val_accuracy: 0.7893\n",
      "Epoch 111/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8015\n",
      "Epoch 111: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.7994 - val_loss: 0.1528 - val_accuracy: 0.7913\n",
      "Epoch 112/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.8002\n",
      "Epoch 112: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.7994 - val_loss: 0.1500 - val_accuracy: 0.7893\n",
      "Epoch 113/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.8039\n",
      "Epoch 113: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.8019 - val_loss: 0.1504 - val_accuracy: 0.7882\n",
      "Epoch 114/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.8036\n",
      "Epoch 114: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.8029 - val_loss: 0.1611 - val_accuracy: 0.7923\n",
      "Epoch 115/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1401 - accuracy: 0.8043\n",
      "Epoch 115: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.8027 - val_loss: 0.1536 - val_accuracy: 0.7943\n",
      "Epoch 116/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7992\n",
      "Epoch 116: val_loss did not improve from 0.14892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.7989 - val_loss: 0.1500 - val_accuracy: 0.7872\n",
      "Epoch 117/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.8018\n",
      "Epoch 117: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.8017 - val_loss: 0.1494 - val_accuracy: 0.7872\n",
      "Epoch 118/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8044\n",
      "Epoch 118: val_loss did not improve from 0.14892\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1398 - accuracy: 0.8052 - val_loss: 0.1649 - val_accuracy: 0.7903\n",
      "Epoch 119/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.7997\n",
      "Epoch 119: val_loss improved from 0.14892 to 0.14858, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.8002 - val_loss: 0.1486 - val_accuracy: 0.7933\n",
      "Epoch 120/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.7986\n",
      "Epoch 120: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.7981 - val_loss: 0.1525 - val_accuracy: 0.7903\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.8044\n",
      "Epoch 121: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.8045 - val_loss: 0.1559 - val_accuracy: 0.7893\n",
      "Epoch 122/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.7986\n",
      "Epoch 122: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.7991 - val_loss: 0.1543 - val_accuracy: 0.7842\n",
      "Epoch 123/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8018\n",
      "Epoch 123: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.8017 - val_loss: 0.1526 - val_accuracy: 0.7812\n",
      "Epoch 124/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.7989\n",
      "Epoch 124: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8007 - val_loss: 0.1544 - val_accuracy: 0.7882\n",
      "Epoch 125/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7997\n",
      "Epoch 125: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.7999 - val_loss: 0.1523 - val_accuracy: 0.7852\n",
      "Epoch 126/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.8023\n",
      "Epoch 126: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.8017 - val_loss: 0.1489 - val_accuracy: 0.7893\n",
      "Epoch 127/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8023\n",
      "Epoch 127: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8007 - val_loss: 0.1572 - val_accuracy: 0.7720\n",
      "Epoch 128/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.8067\n",
      "Epoch 128: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.8062 - val_loss: 0.1497 - val_accuracy: 0.7913\n",
      "Epoch 129/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.8015\n",
      "Epoch 129: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.8009 - val_loss: 0.1536 - val_accuracy: 0.7801\n",
      "Epoch 130/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1401 - accuracy: 0.8012\n",
      "Epoch 130: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.7989 - val_loss: 0.1523 - val_accuracy: 0.7822\n",
      "Epoch 131/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.7989\n",
      "Epoch 131: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.7991 - val_loss: 0.1516 - val_accuracy: 0.7832\n",
      "Epoch 132/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8039\n",
      "Epoch 132: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1406 - accuracy: 0.8022 - val_loss: 0.1493 - val_accuracy: 0.7933\n",
      "Epoch 133/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.8005\n",
      "Epoch 133: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.7999 - val_loss: 0.1494 - val_accuracy: 0.7953\n",
      "Epoch 134/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.8049\n",
      "Epoch 134: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8050 - val_loss: 0.1549 - val_accuracy: 0.7872\n",
      "Epoch 135/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.8007\n",
      "Epoch 135: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8004 - val_loss: 0.1501 - val_accuracy: 0.7933\n",
      "Epoch 136/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8044\n",
      "Epoch 136: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8050 - val_loss: 0.1565 - val_accuracy: 0.7791\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.8010\n",
      "Epoch 137: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8012 - val_loss: 0.1545 - val_accuracy: 0.7903\n",
      "Epoch 138/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.8065\n",
      "Epoch 138: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8085 - val_loss: 0.1531 - val_accuracy: 0.7801\n",
      "Epoch 139/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8065\n",
      "Epoch 139: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.8050 - val_loss: 0.1578 - val_accuracy: 0.7923\n",
      "Epoch 140/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.8013\n",
      "Epoch 140: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1417 - accuracy: 0.8014 - val_loss: 0.1506 - val_accuracy: 0.7842\n",
      "Epoch 141/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.8027\n",
      "Epoch 141: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.8014 - val_loss: 0.1502 - val_accuracy: 0.7933\n",
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.8036\n",
      "Epoch 142: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8050 - val_loss: 0.1505 - val_accuracy: 0.7923\n",
      "Epoch 143/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8031\n",
      "Epoch 143: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.8009 - val_loss: 0.1570 - val_accuracy: 0.7882\n",
      "Epoch 144/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8041\n",
      "Epoch 144: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8032 - val_loss: 0.1505 - val_accuracy: 0.7862\n",
      "Epoch 145/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1403 - accuracy: 0.8038\n",
      "Epoch 145: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.8050 - val_loss: 0.1526 - val_accuracy: 0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1420 - accuracy: 0.8046\n",
      "Epoch 146: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.8057 - val_loss: 0.1560 - val_accuracy: 0.7893\n",
      "Epoch 147/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8054\n",
      "Epoch 147: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8067 - val_loss: 0.1533 - val_accuracy: 0.7781\n",
      "Epoch 148/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.8002\n",
      "Epoch 148: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.8009 - val_loss: 0.1509 - val_accuracy: 0.7893\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8060\n",
      "Epoch 149: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8045 - val_loss: 0.1508 - val_accuracy: 0.7872\n",
      "Epoch 150/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8023\n",
      "Epoch 150: val_loss did not improve from 0.14858\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.8017 - val_loss: 0.1501 - val_accuracy: 0.7903\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.metrics import *\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "#model.compile(optimizer='Adam',loss='mse',metrics =['accuracy'])\n",
    "model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n",
    "check_cb = ModelCheckpoint('bestparams2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(x_train2, y_train2, batch_size=16, epochs=150, validation_split=0.20, callbacks=[check_cb]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "11f2b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.7500\n",
      "Epoch 1: val_loss improved from inf to 0.17957, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.1986 - accuracy: 0.7518 - val_loss: 0.1796 - val_accuracy: 0.7568\n",
      "Epoch 2/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.7586\n",
      "Epoch 2: val_loss improved from 0.17957 to 0.17235, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1769 - accuracy: 0.7586 - val_loss: 0.1723 - val_accuracy: 0.7568\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.7599\n",
      "Epoch 3: val_loss improved from 0.17235 to 0.16688, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1701 - accuracy: 0.7611 - val_loss: 0.1669 - val_accuracy: 0.7609\n",
      "Epoch 4/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.7680\n",
      "Epoch 4: val_loss improved from 0.16688 to 0.16435, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1674 - accuracy: 0.7682 - val_loss: 0.1643 - val_accuracy: 0.7710\n",
      "Epoch 5/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.7707\n",
      "Epoch 5: val_loss improved from 0.16435 to 0.16407, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1649 - accuracy: 0.7687 - val_loss: 0.1641 - val_accuracy: 0.7589\n",
      "Epoch 6/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.7725\n",
      "Epoch 6: val_loss improved from 0.16407 to 0.16259, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1655 - accuracy: 0.7723 - val_loss: 0.1626 - val_accuracy: 0.7649\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1627 - accuracy: 0.7688\n",
      "Epoch 7: val_loss did not improve from 0.16259\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.7677 - val_loss: 0.1638 - val_accuracy: 0.7568\n",
      "Epoch 8/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.7678\n",
      "Epoch 8: val_loss did not improve from 0.16259\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1622 - accuracy: 0.7677 - val_loss: 0.1629 - val_accuracy: 0.7710\n",
      "Epoch 9/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.7709\n",
      "Epoch 9: val_loss improved from 0.16259 to 0.16099, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1614 - accuracy: 0.7693 - val_loss: 0.1610 - val_accuracy: 0.7670\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.7696\n",
      "Epoch 10: val_loss did not improve from 0.16099\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1606 - accuracy: 0.7700 - val_loss: 0.1611 - val_accuracy: 0.7579\n",
      "Epoch 11/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.7714\n",
      "Epoch 11: val_loss improved from 0.16099 to 0.16031, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1609 - accuracy: 0.7703 - val_loss: 0.1603 - val_accuracy: 0.7599\n",
      "Epoch 12/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.7699\n",
      "Epoch 12: val_loss did not improve from 0.16031\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1593 - accuracy: 0.7682 - val_loss: 0.1620 - val_accuracy: 0.7680\n",
      "Epoch 13/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1591 - accuracy: 0.7714\n",
      "Epoch 13: val_loss improved from 0.16031 to 0.16027, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1591 - accuracy: 0.7715 - val_loss: 0.1603 - val_accuracy: 0.7700\n",
      "Epoch 14/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1606 - accuracy: 0.7699\n",
      "Epoch 14: val_loss did not improve from 0.16027\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1604 - accuracy: 0.7703 - val_loss: 0.1621 - val_accuracy: 0.7599\n",
      "Epoch 15/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1590 - accuracy: 0.7700\n",
      "Epoch 15: val_loss did not improve from 0.16027\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1578 - accuracy: 0.7713 - val_loss: 0.1622 - val_accuracy: 0.7710\n",
      "Epoch 16/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.7720\n",
      "Epoch 16: val_loss improved from 0.16027 to 0.15856, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1589 - accuracy: 0.7708 - val_loss: 0.1586 - val_accuracy: 0.7639\n",
      "Epoch 17/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.7709\n",
      "Epoch 17: val_loss improved from 0.15856 to 0.15791, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1590 - accuracy: 0.7703 - val_loss: 0.1579 - val_accuracy: 0.7741\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.7738\n",
      "Epoch 18: val_loss did not improve from 0.15791\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1571 - accuracy: 0.7725 - val_loss: 0.1584 - val_accuracy: 0.7730\n",
      "Epoch 19/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1586 - accuracy: 0.7691\n",
      "Epoch 19: val_loss improved from 0.15791 to 0.15732, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1579 - accuracy: 0.7705 - val_loss: 0.1573 - val_accuracy: 0.7670\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.7759\n",
      "Epoch 20: val_loss did not improve from 0.15732\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1565 - accuracy: 0.7768 - val_loss: 0.1610 - val_accuracy: 0.7690\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.7714\n",
      "Epoch 21: val_loss improved from 0.15732 to 0.15645, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1570 - accuracy: 0.7693 - val_loss: 0.1564 - val_accuracy: 0.7649\n",
      "Epoch 22/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.7738\n",
      "Epoch 22: val_loss did not improve from 0.15645\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1560 - accuracy: 0.7728 - val_loss: 0.1588 - val_accuracy: 0.7791\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.7759\n",
      "Epoch 23: val_loss did not improve from 0.15645\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1560 - accuracy: 0.7761 - val_loss: 0.1643 - val_accuracy: 0.7730\n",
      "Epoch 24/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.7712\n",
      "Epoch 24: val_loss did not improve from 0.15645\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1561 - accuracy: 0.7725 - val_loss: 0.1569 - val_accuracy: 0.7690\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.7741\n",
      "Epoch 25: val_loss did not improve from 0.15645\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1552 - accuracy: 0.7730 - val_loss: 0.1570 - val_accuracy: 0.7832\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.7714\n",
      "Epoch 26: val_loss improved from 0.15645 to 0.15547, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1554 - accuracy: 0.7730 - val_loss: 0.1555 - val_accuracy: 0.7842\n",
      "Epoch 27/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.7769\n",
      "Epoch 27: val_loss did not improve from 0.15547\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1548 - accuracy: 0.7774 - val_loss: 0.1561 - val_accuracy: 0.7801\n",
      "Epoch 28/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.7811\n",
      "Epoch 28: val_loss improved from 0.15547 to 0.15535, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1544 - accuracy: 0.7801 - val_loss: 0.1553 - val_accuracy: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.7769\n",
      "Epoch 29: val_loss did not improve from 0.15535\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1545 - accuracy: 0.7774 - val_loss: 0.1555 - val_accuracy: 0.7720\n",
      "Epoch 30/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1550 - accuracy: 0.7775\n",
      "Epoch 30: val_loss did not improve from 0.15535\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1547 - accuracy: 0.7768 - val_loss: 0.1566 - val_accuracy: 0.7720\n",
      "Epoch 31/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.7767\n",
      "Epoch 31: val_loss did not improve from 0.15535\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1546 - accuracy: 0.7761 - val_loss: 0.1557 - val_accuracy: 0.7741\n",
      "Epoch 32/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.7806\n",
      "Epoch 32: val_loss did not improve from 0.15535\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1544 - accuracy: 0.7812 - val_loss: 0.1559 - val_accuracy: 0.7801\n",
      "Epoch 33/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.7769\n",
      "Epoch 33: val_loss did not improve from 0.15535\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1541 - accuracy: 0.7784 - val_loss: 0.1557 - val_accuracy: 0.7832\n",
      "Epoch 34/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.7743\n",
      "Epoch 34: val_loss improved from 0.15535 to 0.15463, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1535 - accuracy: 0.7736 - val_loss: 0.1546 - val_accuracy: 0.7781\n",
      "Epoch 35/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.7754\n",
      "Epoch 35: val_loss did not improve from 0.15463\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1541 - accuracy: 0.7768 - val_loss: 0.1551 - val_accuracy: 0.7680\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.7811\n",
      "Epoch 36: val_loss did not improve from 0.15463\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1532 - accuracy: 0.7829 - val_loss: 0.1616 - val_accuracy: 0.7741\n",
      "Epoch 37/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.7837\n",
      "Epoch 37: val_loss did not improve from 0.15463\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1543 - accuracy: 0.7824 - val_loss: 0.1551 - val_accuracy: 0.7710\n",
      "Epoch 38/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.7756\n",
      "Epoch 38: val_loss improved from 0.15463 to 0.15459, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1550 - accuracy: 0.7766 - val_loss: 0.1546 - val_accuracy: 0.7690\n",
      "Epoch 39/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.7809\n",
      "Epoch 39: val_loss did not improve from 0.15459\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1523 - accuracy: 0.7812 - val_loss: 0.1554 - val_accuracy: 0.7710\n",
      "Epoch 40/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.7767\n",
      "Epoch 40: val_loss did not improve from 0.15459\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1530 - accuracy: 0.7771 - val_loss: 0.1556 - val_accuracy: 0.7720\n",
      "Epoch 41/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.7819\n",
      "Epoch 41: val_loss improved from 0.15459 to 0.15420, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1523 - accuracy: 0.7814 - val_loss: 0.1542 - val_accuracy: 0.7832\n",
      "Epoch 42/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1518 - accuracy: 0.7831\n",
      "Epoch 42: val_loss improved from 0.15420 to 0.15389, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1518 - accuracy: 0.7829 - val_loss: 0.1539 - val_accuracy: 0.7781\n",
      "Epoch 43/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1535 - accuracy: 0.7786\n",
      "Epoch 43: val_loss improved from 0.15389 to 0.15360, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1534 - accuracy: 0.7796 - val_loss: 0.1536 - val_accuracy: 0.7832\n",
      "Epoch 44/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.7803\n",
      "Epoch 44: val_loss did not improve from 0.15360\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1531 - accuracy: 0.7784 - val_loss: 0.1600 - val_accuracy: 0.7842\n",
      "Epoch 45/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.7829\n",
      "Epoch 45: val_loss did not improve from 0.15360\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1535 - accuracy: 0.7819 - val_loss: 0.1552 - val_accuracy: 0.7822\n",
      "Epoch 46/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.7811\n",
      "Epoch 46: val_loss improved from 0.15360 to 0.15311, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.7806 - val_loss: 0.1531 - val_accuracy: 0.7893\n",
      "Epoch 47/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 47: val_loss improved from 0.15311 to 0.15295, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1530 - accuracy: 0.7822 - val_loss: 0.1529 - val_accuracy: 0.7832\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.7782\n",
      "Epoch 48: val_loss did not improve from 0.15295\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1531 - accuracy: 0.7791 - val_loss: 0.1548 - val_accuracy: 0.7842\n",
      "Epoch 49/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.7816\n",
      "Epoch 49: val_loss did not improve from 0.15295\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.7809 - val_loss: 0.1561 - val_accuracy: 0.7761\n",
      "Epoch 50/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.7835\n",
      "Epoch 50: val_loss improved from 0.15295 to 0.15228, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1514 - accuracy: 0.7839 - val_loss: 0.1523 - val_accuracy: 0.7812\n",
      "Epoch 51/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.7850\n",
      "Epoch 51: val_loss did not improve from 0.15228\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1522 - accuracy: 0.7855 - val_loss: 0.1530 - val_accuracy: 0.7852\n",
      "Epoch 52/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.7849\n",
      "Epoch 52: val_loss did not improve from 0.15228\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1512 - accuracy: 0.7855 - val_loss: 0.1555 - val_accuracy: 0.7882\n",
      "Epoch 53/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.7811\n",
      "Epoch 53: val_loss improved from 0.15228 to 0.15210, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.7812 - val_loss: 0.1521 - val_accuracy: 0.7842\n",
      "Epoch 54/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.7871\n",
      "Epoch 54: val_loss did not improve from 0.15210\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.7860 - val_loss: 0.1530 - val_accuracy: 0.7801\n",
      "Epoch 55/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.7866\n",
      "Epoch 55: val_loss improved from 0.15210 to 0.15183, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1505 - accuracy: 0.7872 - val_loss: 0.1518 - val_accuracy: 0.7852\n",
      "Epoch 56/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1481 - accuracy: 0.7911\n",
      "Epoch 56: val_loss improved from 0.15183 to 0.15148, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1494 - accuracy: 0.7888 - val_loss: 0.1515 - val_accuracy: 0.7933\n",
      "Epoch 57/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.7790\n",
      "Epoch 57: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1496 - accuracy: 0.7822 - val_loss: 0.1623 - val_accuracy: 0.7710\n",
      "Epoch 58/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.7877\n",
      "Epoch 58: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1493 - accuracy: 0.7872 - val_loss: 0.1538 - val_accuracy: 0.7923\n",
      "Epoch 59/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1481 - accuracy: 0.7874\n",
      "Epoch 59: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.7875 - val_loss: 0.1525 - val_accuracy: 0.7943\n",
      "Epoch 60/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1485 - accuracy: 0.7921\n",
      "Epoch 60: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1491 - accuracy: 0.7903 - val_loss: 0.1522 - val_accuracy: 0.7872\n",
      "Epoch 61/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.7934\n",
      "Epoch 61: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.7915 - val_loss: 0.1519 - val_accuracy: 0.7882\n",
      "Epoch 62/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.7921\n",
      "Epoch 62: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1479 - accuracy: 0.7920 - val_loss: 0.1520 - val_accuracy: 0.7872\n",
      "Epoch 63/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.7945\n",
      "Epoch 63: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.7936 - val_loss: 0.1584 - val_accuracy: 0.7822\n",
      "Epoch 64/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.7895\n",
      "Epoch 64: val_loss did not improve from 0.15148\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1478 - accuracy: 0.7885 - val_loss: 0.1594 - val_accuracy: 0.7852\n",
      "Epoch 65/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.7926\n",
      "Epoch 65: val_loss improved from 0.15148 to 0.15039, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.7926 - val_loss: 0.1504 - val_accuracy: 0.7801\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.7892\n",
      "Epoch 66: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1483 - accuracy: 0.7910 - val_loss: 0.1601 - val_accuracy: 0.7730\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.7905\n",
      "Epoch 67: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1471 - accuracy: 0.7913 - val_loss: 0.1507 - val_accuracy: 0.7872\n",
      "Epoch 68/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.7952\n",
      "Epoch 68: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.7936 - val_loss: 0.1516 - val_accuracy: 0.7872\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.7950\n",
      "Epoch 69: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1465 - accuracy: 0.7956 - val_loss: 0.1507 - val_accuracy: 0.7903\n",
      "Epoch 70/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.7921\n",
      "Epoch 70: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1467 - accuracy: 0.7915 - val_loss: 0.1614 - val_accuracy: 0.7741\n",
      "Epoch 71/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.7942\n",
      "Epoch 71: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1459 - accuracy: 0.7943 - val_loss: 0.1590 - val_accuracy: 0.7801\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.7856\n",
      "Epoch 72: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.7877 - val_loss: 0.1691 - val_accuracy: 0.7832\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.7997\n",
      "Epoch 73: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1454 - accuracy: 0.7986 - val_loss: 0.1517 - val_accuracy: 0.7913\n",
      "Epoch 74/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.7911\n",
      "Epoch 74: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1450 - accuracy: 0.7936 - val_loss: 0.1567 - val_accuracy: 0.7801\n",
      "Epoch 75/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.7939\n",
      "Epoch 75: val_loss did not improve from 0.15039\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.7931 - val_loss: 0.1512 - val_accuracy: 0.7771\n",
      "Epoch 76/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.7960\n",
      "Epoch 76: val_loss improved from 0.15039 to 0.15018, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1448 - accuracy: 0.7958 - val_loss: 0.1502 - val_accuracy: 0.7994\n",
      "Epoch 77/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1442 - accuracy: 0.7968\n",
      "Epoch 77: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.7966 - val_loss: 0.1538 - val_accuracy: 0.7923\n",
      "Epoch 78/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.7918\n",
      "Epoch 78: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7908 - val_loss: 0.1543 - val_accuracy: 0.7943\n",
      "Epoch 79/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.7976\n",
      "Epoch 79: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.7966 - val_loss: 0.1509 - val_accuracy: 0.7822\n",
      "Epoch 80/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.7960\n",
      "Epoch 80: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.7956 - val_loss: 0.1515 - val_accuracy: 0.7882\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.7981\n",
      "Epoch 81: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7969 - val_loss: 0.1504 - val_accuracy: 0.7953\n",
      "Epoch 82/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.7971\n",
      "Epoch 82: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1438 - accuracy: 0.7981 - val_loss: 0.1529 - val_accuracy: 0.7933\n",
      "Epoch 83/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.7937\n",
      "Epoch 83: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1436 - accuracy: 0.7948 - val_loss: 0.1633 - val_accuracy: 0.7852\n",
      "Epoch 84/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1439 - accuracy: 0.7954\n",
      "Epoch 84: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.7961 - val_loss: 0.1503 - val_accuracy: 0.7923\n",
      "Epoch 85/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.7971\n",
      "Epoch 85: val_loss did not improve from 0.15018\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1454 - accuracy: 0.7953 - val_loss: 0.1515 - val_accuracy: 0.7832\n",
      "Epoch 86/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1438 - accuracy: 0.7941\n",
      "Epoch 86: val_loss did not improve from 0.15018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.7946 - val_loss: 0.1540 - val_accuracy: 0.7913\n",
      "Epoch 87/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.7931\n",
      "Epoch 87: val_loss improved from 0.15018 to 0.14839, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1435 - accuracy: 0.7936 - val_loss: 0.1484 - val_accuracy: 0.7862\n",
      "Epoch 88/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.7947\n",
      "Epoch 88: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7938 - val_loss: 0.1561 - val_accuracy: 0.7974\n",
      "Epoch 89/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.7950\n",
      "Epoch 89: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.7964 - val_loss: 0.1495 - val_accuracy: 0.7842\n",
      "Epoch 90/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.7937\n",
      "Epoch 90: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.7941 - val_loss: 0.1507 - val_accuracy: 0.7801\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.7965\n",
      "Epoch 91: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.7976 - val_loss: 0.1484 - val_accuracy: 0.7862\n",
      "Epoch 92/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.7965\n",
      "Epoch 92: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1417 - accuracy: 0.7984 - val_loss: 0.1490 - val_accuracy: 0.7903\n",
      "Epoch 93/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.7971\n",
      "Epoch 93: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1438 - accuracy: 0.7989 - val_loss: 0.1560 - val_accuracy: 0.7751\n",
      "Epoch 94/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.8005\n",
      "Epoch 94: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.8009 - val_loss: 0.1542 - val_accuracy: 0.7852\n",
      "Epoch 95/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.8026\n",
      "Epoch 95: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.8004 - val_loss: 0.1529 - val_accuracy: 0.7913\n",
      "Epoch 96/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.7973\n",
      "Epoch 96: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1435 - accuracy: 0.7974 - val_loss: 0.1486 - val_accuracy: 0.7872\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.7979\n",
      "Epoch 97: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.7971 - val_loss: 0.1553 - val_accuracy: 0.7893\n",
      "Epoch 98/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.7960\n",
      "Epoch 98: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1427 - accuracy: 0.7956 - val_loss: 0.1553 - val_accuracy: 0.7852\n",
      "Epoch 99/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.7976\n",
      "Epoch 99: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.7966 - val_loss: 0.1498 - val_accuracy: 0.7943\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.7976\n",
      "Epoch 100: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1431 - accuracy: 0.7979 - val_loss: 0.1495 - val_accuracy: 0.7893\n",
      "Epoch 101/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.8033\n",
      "Epoch 101: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.8014 - val_loss: 0.1531 - val_accuracy: 0.7923\n",
      "Epoch 102/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.7931\n",
      "Epoch 102: val_loss did not improve from 0.14839\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1427 - accuracy: 0.7943 - val_loss: 0.1531 - val_accuracy: 0.7832\n",
      "Epoch 103/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.8007\n",
      "Epoch 103: val_loss improved from 0.14839 to 0.14748, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7999 - val_loss: 0.1475 - val_accuracy: 0.7893\n",
      "Epoch 104/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8018\n",
      "Epoch 104: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7999 - val_loss: 0.1485 - val_accuracy: 0.7913\n",
      "Epoch 105/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.7997\n",
      "Epoch 105: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.8004 - val_loss: 0.1479 - val_accuracy: 0.7842\n",
      "Epoch 106/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.7999\n",
      "Epoch 106: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.7996 - val_loss: 0.1487 - val_accuracy: 0.7842\n",
      "Epoch 107/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.7999\n",
      "Epoch 107: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.7976 - val_loss: 0.1524 - val_accuracy: 0.7984\n",
      "Epoch 108/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.8028\n",
      "Epoch 108: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1434 - accuracy: 0.8014 - val_loss: 0.1484 - val_accuracy: 0.7903\n",
      "Epoch 109/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.7984\n",
      "Epoch 109: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.7984 - val_loss: 0.1476 - val_accuracy: 0.7882\n",
      "Epoch 110/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.7997\n",
      "Epoch 110: val_loss did not improve from 0.14748\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.8002 - val_loss: 0.1613 - val_accuracy: 0.7852\n",
      "Epoch 111/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.7931\n",
      "Epoch 111: val_loss improved from 0.14748 to 0.14739, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.7931 - val_loss: 0.1474 - val_accuracy: 0.7842\n",
      "Epoch 112/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1411 - accuracy: 0.8038\n",
      "Epoch 112: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.8022 - val_loss: 0.1489 - val_accuracy: 0.7893\n",
      "Epoch 113/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8002\n",
      "Epoch 113: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.7996 - val_loss: 0.1588 - val_accuracy: 0.7882\n",
      "Epoch 114/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.7984\n",
      "Epoch 114: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.7989 - val_loss: 0.1599 - val_accuracy: 0.7872\n",
      "Epoch 115/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.8020\n",
      "Epoch 115: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1423 - accuracy: 0.8012 - val_loss: 0.1524 - val_accuracy: 0.7913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.8010\n",
      "Epoch 116: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.7994 - val_loss: 0.1499 - val_accuracy: 0.7822\n",
      "Epoch 117/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7994\n",
      "Epoch 117: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7986 - val_loss: 0.1480 - val_accuracy: 0.7913\n",
      "Epoch 118/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.7981\n",
      "Epoch 118: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.7971 - val_loss: 0.1480 - val_accuracy: 0.7832\n",
      "Epoch 119/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1402 - accuracy: 0.7978\n",
      "Epoch 119: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.7981 - val_loss: 0.1519 - val_accuracy: 0.7832\n",
      "Epoch 120/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7986\n",
      "Epoch 120: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.7958 - val_loss: 0.1524 - val_accuracy: 0.7771\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.7952\n",
      "Epoch 121: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.7961 - val_loss: 0.1478 - val_accuracy: 0.7882\n",
      "Epoch 122/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.7996\n",
      "Epoch 122: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.7991 - val_loss: 0.1533 - val_accuracy: 0.7822\n",
      "Epoch 123/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.8047\n",
      "Epoch 123: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.8065 - val_loss: 0.1495 - val_accuracy: 0.7842\n",
      "Epoch 124/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.8054\n",
      "Epoch 124: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.8032 - val_loss: 0.1566 - val_accuracy: 0.7903\n",
      "Epoch 125/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8044\n",
      "Epoch 125: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8029 - val_loss: 0.1476 - val_accuracy: 0.7882\n",
      "Epoch 126/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.7994\n",
      "Epoch 126: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.7991 - val_loss: 0.1475 - val_accuracy: 0.7872\n",
      "Epoch 127/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.7981\n",
      "Epoch 127: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1409 - accuracy: 0.7986 - val_loss: 0.1484 - val_accuracy: 0.7822\n",
      "Epoch 128/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1414 - accuracy: 0.8012\n",
      "Epoch 128: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8029 - val_loss: 0.1487 - val_accuracy: 0.7882\n",
      "Epoch 129/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.8091\n",
      "Epoch 129: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8062 - val_loss: 0.1511 - val_accuracy: 0.7953\n",
      "Epoch 130/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.7981\n",
      "Epoch 130: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.7971 - val_loss: 0.1566 - val_accuracy: 0.7893\n",
      "Epoch 131/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7989\n",
      "Epoch 131: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.7986 - val_loss: 0.1495 - val_accuracy: 0.7994\n",
      "Epoch 132/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8044\n",
      "Epoch 132: val_loss did not improve from 0.14739\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8034 - val_loss: 0.1480 - val_accuracy: 0.7913\n",
      "Epoch 133/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8020\n",
      "Epoch 133: val_loss improved from 0.14739 to 0.14728, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.7999 - val_loss: 0.1473 - val_accuracy: 0.7842\n",
      "Epoch 134/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8031\n",
      "Epoch 134: val_loss did not improve from 0.14728\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8022 - val_loss: 0.1520 - val_accuracy: 0.7832\n",
      "Epoch 135/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.8018\n",
      "Epoch 135: val_loss improved from 0.14728 to 0.14726, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8007 - val_loss: 0.1473 - val_accuracy: 0.7801\n",
      "Epoch 136/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8005\n",
      "Epoch 136: val_loss did not improve from 0.14726\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8007 - val_loss: 0.1483 - val_accuracy: 0.7923\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8018\n",
      "Epoch 137: val_loss did not improve from 0.14726\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.8019 - val_loss: 0.1502 - val_accuracy: 0.7923\n",
      "Epoch 138/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.8026\n",
      "Epoch 138: val_loss did not improve from 0.14726\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8027 - val_loss: 0.1538 - val_accuracy: 0.7893\n",
      "Epoch 139/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.7986\n",
      "Epoch 139: val_loss did not improve from 0.14726\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.7996 - val_loss: 0.1514 - val_accuracy: 0.7842\n",
      "Epoch 140/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.7994\n",
      "Epoch 140: val_loss improved from 0.14726 to 0.14720, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.7999 - val_loss: 0.1472 - val_accuracy: 0.7913\n",
      "Epoch 141/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8015\n",
      "Epoch 141: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.7996 - val_loss: 0.1496 - val_accuracy: 0.7872\n",
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8060\n",
      "Epoch 142: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.8045 - val_loss: 0.1477 - val_accuracy: 0.7893\n",
      "Epoch 143/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.8002\n",
      "Epoch 143: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.8007 - val_loss: 0.1489 - val_accuracy: 0.7801\n",
      "Epoch 144/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.7984\n",
      "Epoch 144: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.7996 - val_loss: 0.1490 - val_accuracy: 0.7882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8018\n",
      "Epoch 145: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8017 - val_loss: 0.1558 - val_accuracy: 0.7822\n",
      "Epoch 146/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8002\n",
      "Epoch 146: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.7999 - val_loss: 0.1496 - val_accuracy: 0.7812\n",
      "Epoch 147/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8023\n",
      "Epoch 147: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8019 - val_loss: 0.1481 - val_accuracy: 0.7943\n",
      "Epoch 148/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.7986\n",
      "Epoch 148: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1393 - accuracy: 0.7986 - val_loss: 0.1480 - val_accuracy: 0.7953\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8010\n",
      "Epoch 149: val_loss improved from 0.14720 to 0.14592, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8017 - val_loss: 0.1459 - val_accuracy: 0.7923\n",
      "Epoch 150/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.7994\n",
      "Epoch 150: val_loss did not improve from 0.14592\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1406 - accuracy: 0.8004 - val_loss: 0.1495 - val_accuracy: 0.7832\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.metrics import *\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "#model.compile(optimizer='Adam',loss='mse',metrics =['accuracy'])\n",
    "model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n",
    "check_cb = ModelCheckpoint('bestparams2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(x_train2, y_train2, batch_size=16, epochs=150, validation_split=0.20, callbacks=[check_cb]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b156822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c7970951",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ef9f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count mismatch: 97    out of: 549   % mismatch: 17.67   accuracy 82.33\n"
     ]
    }
   ],
   "source": [
    "pre = abs(np.asarray(model.predict(x_test2).round()))\n",
    "y_test2\n",
    "ld = [x[0] for x in pre]\n",
    "ints = [int(item) for item in ld]\n",
    "pre_ary = np.array(ints)\n",
    "\n",
    "\n",
    "\n",
    "sub_ary = y_test2 - pre_ary\n",
    "\n",
    "cnt = np.count_nonzero(sub_ary)\n",
    "\n",
    "num = ((cnt)/y_test2.shape[0] *100)\n",
    "perc_acc =  ((y_test2.shape[0] - cnt)/y_test2.shape[0] *100)\n",
    "round(num,2)\n",
    "\n",
    "print(\"count mismatch:\",cnt,\"   out of:\",y_test3.shape[0],\"  % mismatch:\",round(num,2), \"  accuracy\",round(perc_acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56c93d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>-0.095282</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>-0.008533</td>\n",
       "      <td>0.153238</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171184</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.869785</td>\n",
       "      <td>-0.033196</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>-0.463337</td>\n",
       "      <td>-0.994885</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3        4         5         6    \\\n",
       "0  0.000394  0.002912 -0.095282  0.000022  0.00031 -0.008533  0.153238   \n",
       "\n",
       "        7         8         9    ...       654       655       656       657  \\\n",
       "0  0.000012  0.000779  0.152432  ... -0.171184  0.000012  0.000043 -0.869785   \n",
       "\n",
       "        658       659       660       661       662       663  \n",
       "0 -0.033196 -0.000038  0.001141 -0.463337 -0.994885  0.000072  \n",
       "\n",
       "[1 rows x 664 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh = model.get_layer(name='Contextual_Weight').get_weights()[0][0]\n",
    "gh2 = model.get_layer(name='Contextual_Weight').get_weights()[1]\n",
    "df2 = pd.DataFrame(gh).T\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4f557f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def four_inter2(data):\n",
    "    df = []\n",
    "#    print(data)\n",
    "    df = data\n",
    "    c_ln = int(df.shape[1])\n",
    "    n_dim = c_ln/4\n",
    "    \n",
    "    lt = [\"VDW\", \"ELE\", \"GB\", \"SA\"]\n",
    "    op = []\n",
    "    \n",
    "    for k, j in enumerate(lt):\n",
    "        if lt[k] == j:\n",
    "            for i in range(int(n_dim)):\n",
    "                f = j + \"_\" + str(i)\n",
    "                op.append(f)\n",
    "            \n",
    "            \n",
    "    res = [list(i) for j, i in groupby(op, lambda a: a.split('_')[0])]\n",
    "    ddf1 = pd.DataFrame(res[0])\n",
    "    ddf2 = pd.DataFrame(res[1])\n",
    "    ddf3 = pd.DataFrame(res[2])\n",
    "    ddf4 = pd.DataFrame(res[3])\n",
    "    \n",
    "    column_names = pd.concat([ddf1,ddf2,ddf3,ddf4], axis=1).to_numpy().flatten().tolist()\n",
    "\n",
    "    lt_col = column_names\n",
    "    \n",
    "    return lt_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eed09be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GB_165</th>\n",
       "      <th>ELE_164</th>\n",
       "      <th>GB_67</th>\n",
       "      <th>ELE_67</th>\n",
       "      <th>GB_83</th>\n",
       "      <th>GB_49</th>\n",
       "      <th>ELE_83</th>\n",
       "      <th>ELE_49</th>\n",
       "      <th>VDW_53</th>\n",
       "      <th>ELE_80</th>\n",
       "      <th>...</th>\n",
       "      <th>VDW_75</th>\n",
       "      <th>SA_42</th>\n",
       "      <th>VDW_25</th>\n",
       "      <th>SA_64</th>\n",
       "      <th>SA_41</th>\n",
       "      <th>VDW_86</th>\n",
       "      <th>VDW_92</th>\n",
       "      <th>SA_90</th>\n",
       "      <th>SA_85</th>\n",
       "      <th>SA_107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994885</td>\n",
       "      <td>0.869785</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>0.544226</td>\n",
       "      <td>0.526459</td>\n",
       "      <td>0.523596</td>\n",
       "      <td>0.511166</td>\n",
       "      <td>0.492182</td>\n",
       "      <td>0.487036</td>\n",
       "      <td>...</td>\n",
       "      <td>1.474301e-07</td>\n",
       "      <td>1.161969e-07</td>\n",
       "      <td>1.154306e-07</td>\n",
       "      <td>9.814556e-08</td>\n",
       "      <td>9.268399e-08</td>\n",
       "      <td>5.395361e-08</td>\n",
       "      <td>4.053800e-08</td>\n",
       "      <td>3.607749e-08</td>\n",
       "      <td>2.188256e-08</td>\n",
       "      <td>8.000313e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GB_165   ELE_164     GB_67    ELE_67     GB_83     GB_49    ELE_83  \\\n",
       "0  0.994885  0.869785  0.776263  0.687331  0.544226  0.526459  0.523596   \n",
       "\n",
       "     ELE_49    VDW_53    ELE_80  ...        VDW_75         SA_42  \\\n",
       "0  0.511166  0.492182  0.487036  ...  1.474301e-07  1.161969e-07   \n",
       "\n",
       "         VDW_25         SA_64         SA_41        VDW_86        VDW_92  \\\n",
       "0  1.154306e-07  9.814556e-08  9.268399e-08  5.395361e-08  4.053800e-08   \n",
       "\n",
       "          SA_90         SA_85        SA_107  \n",
       "0  3.607749e-08  2.188256e-08  8.000313e-09  \n",
       "\n",
       "[1 rows x 664 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note df2 is renamed for weights...\n",
    "df2.columns = four_inter2(df2)\n",
    "df_abs = abs(df2)\n",
    "df_abs.sort_values(by=0, ascending=False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7641b7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.869785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.776263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.687331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.544226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.526459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.523596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.511166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.492182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.487036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.476410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.463337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.429386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.390088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.388268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.381966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.343233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.308266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.302559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.301706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.300016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.292960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.287829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.277676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.274215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.264255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.261002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.251566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.250351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     weights\n",
       "0   0.994885\n",
       "1   0.869785\n",
       "2   0.776263\n",
       "3   0.687331\n",
       "4   0.544226\n",
       "5   0.526459\n",
       "6   0.523596\n",
       "7   0.511166\n",
       "8   0.492182\n",
       "9   0.487036\n",
       "10  0.476410\n",
       "11  0.463337\n",
       "12  0.429386\n",
       "13  0.411765\n",
       "14  0.390088\n",
       "15  0.388268\n",
       "16  0.381966\n",
       "17  0.343233\n",
       "18  0.308266\n",
       "19  0.302559\n",
       "20  0.301706\n",
       "21  0.300016\n",
       "22  0.292960\n",
       "23  0.287829\n",
       "24  0.277676\n",
       "25  0.274215\n",
       "26  0.264255\n",
       "27  0.261002\n",
       "28  0.251566\n",
       "29  0.250351"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.read_csv(path1+file2, delimiter=' ',names=four_inter(df))\n",
    "#pd.DataFrame(gh).T\n",
    "df2.columns = four_inter2(df2)\n",
    "df_abs = abs(df2)\n",
    "\n",
    "df_rk = df_abs.sort_values(by=0, ascending=False,axis=1)\n",
    "df_rk = round(df_rk.iloc[:,0:30],15)\n",
    "df_rkw = pd.DataFrame(df_rk.iloc[0]).reset_index()[0]\n",
    "df_rkw = pd.DataFrame(df_rkw)\n",
    "df_rkw.columns = ['weights']\n",
    "df_rkw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f70ea3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_rk.T.plot(kind= 'bar',rot=90, legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de130621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e899398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23258ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1532965/1802899870.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df5 = pd.read_csv(path1+file3, sep='\\\\t', header=None)\n"
     ]
    }
   ],
   "source": [
    "file3 = 'generalizePairDist_15A_noH_10A.list'\n",
    "df5 = pd.read_csv(path1+file3, sep='\\\\t', header=None)\n",
    "df5.columns = ['res1','res2','prot_res','lig_res','arb1','dist','arb2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c0c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ab75475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GB_165</th>\n",
       "      <th>ELE_164</th>\n",
       "      <th>GB_67</th>\n",
       "      <th>ELE_67</th>\n",
       "      <th>GB_83</th>\n",
       "      <th>GB_49</th>\n",
       "      <th>ELE_83</th>\n",
       "      <th>ELE_49</th>\n",
       "      <th>VDW_53</th>\n",
       "      <th>ELE_80</th>\n",
       "      <th>...</th>\n",
       "      <th>ELE_161</th>\n",
       "      <th>ELE_82</th>\n",
       "      <th>ELE_120</th>\n",
       "      <th>ELE_111</th>\n",
       "      <th>GB_161</th>\n",
       "      <th>VDW_100</th>\n",
       "      <th>GB_2</th>\n",
       "      <th>ELE_81</th>\n",
       "      <th>GB_68</th>\n",
       "      <th>ELE_115</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.038422</td>\n",
       "      <td>2.090094</td>\n",
       "      <td>0.492927</td>\n",
       "      <td>-0.441954</td>\n",
       "      <td>10.152541</td>\n",
       "      <td>-5.085066</td>\n",
       "      <td>-9.888102</td>\n",
       "      <td>5.000738</td>\n",
       "      <td>-0.265778</td>\n",
       "      <td>11.289004</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.828035</td>\n",
       "      <td>-0.236113</td>\n",
       "      <td>-0.431237</td>\n",
       "      <td>-1.751152</td>\n",
       "      <td>0.199649</td>\n",
       "      <td>-0.230889</td>\n",
       "      <td>-0.067385</td>\n",
       "      <td>-3.948177</td>\n",
       "      <td>2.267362</td>\n",
       "      <td>-0.150962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.553913</td>\n",
       "      <td>-5.751021</td>\n",
       "      <td>0.417630</td>\n",
       "      <td>-0.374596</td>\n",
       "      <td>10.037710</td>\n",
       "      <td>-4.804463</td>\n",
       "      <td>-9.775529</td>\n",
       "      <td>4.726242</td>\n",
       "      <td>-0.240677</td>\n",
       "      <td>10.835573</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.824414</td>\n",
       "      <td>-0.227712</td>\n",
       "      <td>-0.520882</td>\n",
       "      <td>-1.691283</td>\n",
       "      <td>0.196595</td>\n",
       "      <td>-0.229244</td>\n",
       "      <td>-0.061836</td>\n",
       "      <td>-3.695005</td>\n",
       "      <td>2.145101</td>\n",
       "      <td>-0.159474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.804624</td>\n",
       "      <td>-16.910368</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>-0.362911</td>\n",
       "      <td>0.440279</td>\n",
       "      <td>-4.731285</td>\n",
       "      <td>-0.427254</td>\n",
       "      <td>4.653656</td>\n",
       "      <td>-0.510393</td>\n",
       "      <td>10.441561</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.427980</td>\n",
       "      <td>-7.518700</td>\n",
       "      <td>-0.053026</td>\n",
       "      <td>-0.464844</td>\n",
       "      <td>0.310997</td>\n",
       "      <td>-0.436551</td>\n",
       "      <td>-0.105173</td>\n",
       "      <td>-3.682477</td>\n",
       "      <td>2.120700</td>\n",
       "      <td>-3.555238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.749051</td>\n",
       "      <td>-13.699120</td>\n",
       "      <td>0.427721</td>\n",
       "      <td>-0.384218</td>\n",
       "      <td>0.918654</td>\n",
       "      <td>-4.787616</td>\n",
       "      <td>-0.890112</td>\n",
       "      <td>4.709373</td>\n",
       "      <td>-0.462159</td>\n",
       "      <td>10.377760</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192944</td>\n",
       "      <td>-0.254414</td>\n",
       "      <td>-0.312002</td>\n",
       "      <td>6.592149</td>\n",
       "      <td>-4.780475</td>\n",
       "      <td>-0.411049</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>-3.762343</td>\n",
       "      <td>2.138561</td>\n",
       "      <td>3.309894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.239627</td>\n",
       "      <td>-12.734528</td>\n",
       "      <td>0.440918</td>\n",
       "      <td>-0.395216</td>\n",
       "      <td>0.259596</td>\n",
       "      <td>-4.793407</td>\n",
       "      <td>-0.252373</td>\n",
       "      <td>4.715507</td>\n",
       "      <td>-0.488244</td>\n",
       "      <td>1.391461</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.794545</td>\n",
       "      <td>-0.189010</td>\n",
       "      <td>-0.207708</td>\n",
       "      <td>-0.307689</td>\n",
       "      <td>0.018049</td>\n",
       "      <td>-0.314251</td>\n",
       "      <td>-0.096453</td>\n",
       "      <td>-3.764170</td>\n",
       "      <td>2.157177</td>\n",
       "      <td>-0.133938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>19.545519</td>\n",
       "      <td>-15.887501</td>\n",
       "      <td>0.759186</td>\n",
       "      <td>-0.687331</td>\n",
       "      <td>-1.171719</td>\n",
       "      <td>-7.396220</td>\n",
       "      <td>1.307418</td>\n",
       "      <td>7.311209</td>\n",
       "      <td>-0.417370</td>\n",
       "      <td>11.290952</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.503406</td>\n",
       "      <td>-6.435942</td>\n",
       "      <td>-0.326064</td>\n",
       "      <td>-0.748068</td>\n",
       "      <td>0.181323</td>\n",
       "      <td>-0.205387</td>\n",
       "      <td>-0.025104</td>\n",
       "      <td>-6.657638</td>\n",
       "      <td>3.294254</td>\n",
       "      <td>2.969667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>20.924431</td>\n",
       "      <td>-15.802262</td>\n",
       "      <td>0.758409</td>\n",
       "      <td>-0.687331</td>\n",
       "      <td>-1.625060</td>\n",
       "      <td>-7.401484</td>\n",
       "      <td>1.669746</td>\n",
       "      <td>7.319387</td>\n",
       "      <td>-0.416386</td>\n",
       "      <td>13.043794</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.375483</td>\n",
       "      <td>-6.068123</td>\n",
       "      <td>-0.520003</td>\n",
       "      <td>-0.609046</td>\n",
       "      <td>0.216032</td>\n",
       "      <td>-0.201000</td>\n",
       "      <td>-0.011363</td>\n",
       "      <td>-6.140854</td>\n",
       "      <td>3.278908</td>\n",
       "      <td>-0.035550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5481</th>\n",
       "      <td>19.526617</td>\n",
       "      <td>-14.530635</td>\n",
       "      <td>0.787907</td>\n",
       "      <td>-0.714825</td>\n",
       "      <td>-1.502065</td>\n",
       "      <td>-7.516779</td>\n",
       "      <td>1.622099</td>\n",
       "      <td>7.435422</td>\n",
       "      <td>-0.428198</td>\n",
       "      <td>11.377157</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.443977</td>\n",
       "      <td>-0.330618</td>\n",
       "      <td>-0.443541</td>\n",
       "      <td>-0.959910</td>\n",
       "      <td>-0.036653</td>\n",
       "      <td>-0.230889</td>\n",
       "      <td>-0.016912</td>\n",
       "      <td>-6.633887</td>\n",
       "      <td>3.337272</td>\n",
       "      <td>-3.305388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>19.227156</td>\n",
       "      <td>-16.071025</td>\n",
       "      <td>0.800327</td>\n",
       "      <td>-0.726509</td>\n",
       "      <td>-1.549956</td>\n",
       "      <td>-7.539416</td>\n",
       "      <td>1.773942</td>\n",
       "      <td>7.457913</td>\n",
       "      <td>-0.340590</td>\n",
       "      <td>11.228611</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.137592</td>\n",
       "      <td>-0.290115</td>\n",
       "      <td>-0.357411</td>\n",
       "      <td>-1.162254</td>\n",
       "      <td>-0.080804</td>\n",
       "      <td>-0.103379</td>\n",
       "      <td>0.672264</td>\n",
       "      <td>-6.784485</td>\n",
       "      <td>3.361925</td>\n",
       "      <td>3.065551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>20.004162</td>\n",
       "      <td>-15.812699</td>\n",
       "      <td>0.718820</td>\n",
       "      <td>-0.650216</td>\n",
       "      <td>-1.271313</td>\n",
       "      <td>-7.232491</td>\n",
       "      <td>1.384387</td>\n",
       "      <td>7.147124</td>\n",
       "      <td>-0.664445</td>\n",
       "      <td>11.325531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.992464</td>\n",
       "      <td>-8.503052</td>\n",
       "      <td>-0.374109</td>\n",
       "      <td>-0.673808</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>-0.292862</td>\n",
       "      <td>-0.131863</td>\n",
       "      <td>-6.465541</td>\n",
       "      <td>3.222809</td>\n",
       "      <td>-0.128931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5484 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         GB_165    ELE_164     GB_67    ELE_67      GB_83     GB_49    ELE_83  \\\n",
       "0     24.038422   2.090094  0.492927 -0.441954  10.152541 -5.085066 -9.888102   \n",
       "1     23.553913  -5.751021  0.417630 -0.374596  10.037710 -4.804463 -9.775529   \n",
       "2     23.804624 -16.910368  0.404433 -0.362911   0.440279 -4.731285 -0.427254   \n",
       "3     22.749051 -13.699120  0.427721 -0.384218   0.918654 -4.787616 -0.890112   \n",
       "4     -1.239627 -12.734528  0.440918 -0.395216   0.259596 -4.793407 -0.252373   \n",
       "...         ...        ...       ...       ...        ...       ...       ...   \n",
       "5479  19.545519 -15.887501  0.759186 -0.687331  -1.171719 -7.396220  1.307418   \n",
       "5480  20.924431 -15.802262  0.758409 -0.687331  -1.625060 -7.401484  1.669746   \n",
       "5481  19.526617 -14.530635  0.787907 -0.714825  -1.502065 -7.516779  1.622099   \n",
       "5482  19.227156 -16.071025  0.800327 -0.726509  -1.549956 -7.539416  1.773942   \n",
       "5483  20.004162 -15.812699  0.718820 -0.650216  -1.271313 -7.232491  1.384387   \n",
       "\n",
       "        ELE_49    VDW_53     ELE_80  ...   ELE_161    ELE_82   ELE_120  \\\n",
       "0     5.000738 -0.265778  11.289004  ... -1.828035 -0.236113 -0.431237   \n",
       "1     4.726242 -0.240677  10.835573  ... -1.824414 -0.227712 -0.520882   \n",
       "2     4.653656 -0.510393  10.441561  ... -3.427980 -7.518700 -0.053026   \n",
       "3     4.709373 -0.462159  10.377760  ...  1.192944 -0.254414 -0.312002   \n",
       "4     4.715507 -0.488244   1.391461  ... -1.794545 -0.189010 -0.207708   \n",
       "...        ...       ...        ...  ...       ...       ...       ...   \n",
       "5479  7.311209 -0.417370  11.290952  ... -3.503406 -6.435942 -0.326064   \n",
       "5480  7.319387 -0.416386  13.043794  ... -3.375483 -6.068123 -0.520003   \n",
       "5481  7.435422 -0.428198  11.377157  ... -5.443977 -0.330618 -0.443541   \n",
       "5482  7.457913 -0.340590  11.228611  ... -4.137592 -0.290115 -0.357411   \n",
       "5483  7.147124 -0.664445  11.325531  ... -1.992464 -8.503052 -0.374109   \n",
       "\n",
       "       ELE_111    GB_161   VDW_100      GB_2    ELE_81     GB_68   ELE_115  \n",
       "0    -1.751152  0.199649 -0.230889 -0.067385 -3.948177  2.267362 -0.150962  \n",
       "1    -1.691283  0.196595 -0.229244 -0.061836 -3.695005  2.145101 -0.159474  \n",
       "2    -0.464844  0.310997 -0.436551 -0.105173 -3.682477  2.120700 -3.555238  \n",
       "3     6.592149 -4.780475 -0.411049  0.028540 -3.762343  2.138561  3.309894  \n",
       "4    -0.307689  0.018049 -0.314251 -0.096453 -3.764170  2.157177 -0.133938  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5479 -0.748068  0.181323 -0.205387 -0.025104 -6.657638  3.294254  2.969667  \n",
       "5480 -0.609046  0.216032 -0.201000 -0.011363 -6.140854  3.278908 -0.035550  \n",
       "5481 -0.959910 -0.036653 -0.230889 -0.016912 -6.633887  3.337272 -3.305388  \n",
       "5482 -1.162254 -0.080804 -0.103379  0.672264 -6.784485  3.361925  3.065551  \n",
       "5483 -0.673808  0.063866 -0.292862 -0.131863 -6.465541  3.222809 -0.128931  \n",
       "\n",
       "[5484 rows x 30 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rk.columns\n",
    "df_2lt = list(df_rk.columns)\n",
    "\n",
    "df_2lt\n",
    "int_term = []\n",
    "int_pair = []\n",
    "for i in range(len(df_2lt)):\n",
    "#    print(df_2lt[i].split('_'))\n",
    "    int_term.append(df_2lt[i].split('_')[0])\n",
    "    int_pair.append(df_2lt[i].split('_')[1])\n",
    "    \n",
    "#np.sort(int_pair)\n",
    "#sorted(int_pair, key=int, reverse=False)\n",
    "\n",
    "## Put them into DFs in case needed....\n",
    "INT_TERM = pd.DataFrame(int_term, columns=['terms'])\n",
    "INT_PAIR = pd.DataFrame(int_pair, columns=['pair'])\n",
    "\n",
    "pd.concat([INT_TERM,INT_PAIR], axis=1)\n",
    "\n",
    "\n",
    "## We need to even out vectors...so lets Filter data base on the limit of DF5\n",
    "lt_pr = []\n",
    "ind = []\n",
    "\n",
    "for j in int_pair:\n",
    "    j = int(j)\n",
    "    if j < df5.shape[0]:\n",
    "        lt_pr.append(df5.iloc[j])\n",
    " #       ind.append(df_rk.iloc[j]) \n",
    "        \n",
    "\n",
    "lt_pr[0]        \n",
    "#ind[0]\n",
    "int_pair\n",
    "f = pd.DataFrame(lt_pr)\n",
    "g = pd.DataFrame(df_rk.columns,columns=['terms'])\n",
    "#h = pd.DataFrame()\n",
    "#pd.concat([g,f])\n",
    "                 \n",
    "##split data in into different columns\n",
    "\n",
    "h = pd.DataFrame(int_term, columns=['int_term'])\n",
    "gh = pd.concat([g,h,df_rkw],axis=1)\n",
    "gh\n",
    "\n",
    "\n",
    "df_en = df[gh['terms']]\n",
    "\n",
    "df_en_sc = df_en.multiply(np.array(df_rkw.T), axis='columns')\n",
    "df_en.multiply(np.array(df_rkw.T), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc2908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "66b1fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_sc= pd.concat([df[['names','bind']],df_en_sc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bcc5a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1532965/3369454623.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_en_sc['Sum'] = df_en_sc.sum(axis=1)\n",
      "/tmp/ipykernel_1532965/3369454623.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en['Sum'] = df_en.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_en_sc['Sum'] = df_en_sc.sum(axis=1)\n",
    "df_en_sc[['bind','Sum']]\n",
    "\n",
    "df_en['Sum'] = df_en.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d27c7",
   "metadata": {},
   "source": [
    "# Free energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "261816a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum\n",
      "binding       4477\n",
      "nonbinding    1007\n",
      "Name: Sum, dtype: int64 Sum\n",
      "binding       5252\n",
      "nonbinding     232\n",
      "Name: Sum, dtype: int64 bind\n",
      "binding       4179\n",
      "nonbinding    1305\n",
      "Name: bind, dtype: int64\n",
      "% Error_wt: -7.1 % Error_Nowt 25.676\n"
     ]
    }
   ],
   "source": [
    "bar1 = df_en_sc.groupby(df_en_sc['Sum'].apply(lambda x: 'binding' if x < 0 else 'nonbinding' ))['Sum'].count()\n",
    "bar2 = df_en.groupby(df_en['Sum'].apply(lambda x: 'binding' if x < 0 else 'nonbinding' ))['Sum'].count()\n",
    "bar3 = df_en_sc.groupby(df_en_sc['bind'].apply(lambda x: 'binding' if x <= 0 else 'nonbinding'  ))['bind'].count()\n",
    "\n",
    "print(bar1, bar2, bar3)\n",
    "\n",
    "print(\"% Error_wt:\", ((bar3.values[0]-bar1.values[0])/bar3.values[0]).round(3)*100,\n",
    "      \"% Error_Nowt\",((abs(bar3.values[0]-bar2.values[0])/bar3.values[0])*100).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "85884406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sum\n",
       "binding       4477\n",
       "nonbinding    1007\n",
       "Name: Sum, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24f9c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bar1['hue']=\"weight\"\n",
    "bar2['hue']=\"NonWt\"\n",
    "bar3['hue']=\"true\"\n",
    "res=pd.concat([bar1, bar2, bar3])\n",
    "#(x='binding',y='nonbinding',data=res,hue='hue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807846ea",
   "metadata": {},
   "source": [
    "#  MIEC with Some Selected MIEC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b033af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>int_term</th>\n",
       "      <th>pos</th>\n",
       "      <th>prot_res</th>\n",
       "      <th>lig_res</th>\n",
       "      <th>dist</th>\n",
       "      <th>weights</th>\n",
       "      <th>energy_nowt</th>\n",
       "      <th>energy_wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_67</td>\n",
       "      <td>GB</td>\n",
       "      <td>67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.932</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.492927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ELE_67</td>\n",
       "      <td>ELE</td>\n",
       "      <td>67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.932</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.441954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB_83</td>\n",
       "      <td>GB</td>\n",
       "      <td>83</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.410</td>\n",
       "      <td>0.544226</td>\n",
       "      <td>18.444</td>\n",
       "      <td>10.152541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GB_49</td>\n",
       "      <td>GB</td>\n",
       "      <td>49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.567</td>\n",
       "      <td>0.526459</td>\n",
       "      <td>-9.126</td>\n",
       "      <td>-5.085066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELE_83</td>\n",
       "      <td>ELE</td>\n",
       "      <td>83</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.410</td>\n",
       "      <td>0.523596</td>\n",
       "      <td>-18.670</td>\n",
       "      <td>-9.888102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ELE_49</td>\n",
       "      <td>ELE</td>\n",
       "      <td>49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.567</td>\n",
       "      <td>0.511166</td>\n",
       "      <td>9.246</td>\n",
       "      <td>5.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VDW_53</td>\n",
       "      <td>VDW</td>\n",
       "      <td>53</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.583</td>\n",
       "      <td>0.492182</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.265778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ELE_80</td>\n",
       "      <td>ELE</td>\n",
       "      <td>80</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.422</td>\n",
       "      <td>0.487036</td>\n",
       "      <td>22.248</td>\n",
       "      <td>11.289004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ELE_21</td>\n",
       "      <td>ELE</td>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.068</td>\n",
       "      <td>0.429386</td>\n",
       "      <td>-5.782</td>\n",
       "      <td>-2.485716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ELE_136</td>\n",
       "      <td>ELE</td>\n",
       "      <td>136</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.671</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>-11.586</td>\n",
       "      <td>-5.037529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ELE_102</td>\n",
       "      <td>ELE</td>\n",
       "      <td>102</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.299</td>\n",
       "      <td>0.390088</td>\n",
       "      <td>-1.960</td>\n",
       "      <td>-0.645595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ELE_68</td>\n",
       "      <td>ELE</td>\n",
       "      <td>68</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.577</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>-8.637</td>\n",
       "      <td>-3.544891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ELE_28</td>\n",
       "      <td>ELE</td>\n",
       "      <td>28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>5.229</td>\n",
       "      <td>0.381966</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.019098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GB_136</td>\n",
       "      <td>GB</td>\n",
       "      <td>136</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.671</td>\n",
       "      <td>0.343233</td>\n",
       "      <td>11.445</td>\n",
       "      <td>4.145568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GB_98</td>\n",
       "      <td>GB</td>\n",
       "      <td>98</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.198</td>\n",
       "      <td>0.302559</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.376988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ELE_82</td>\n",
       "      <td>ELE</td>\n",
       "      <td>82</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>5.316</td>\n",
       "      <td>0.300016</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.236113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ELE_120</td>\n",
       "      <td>ELE</td>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>4.858</td>\n",
       "      <td>0.292960</td>\n",
       "      <td>-1.778</td>\n",
       "      <td>-0.431237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ELE_111</td>\n",
       "      <td>ELE</td>\n",
       "      <td>111</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.397</td>\n",
       "      <td>0.287829</td>\n",
       "      <td>-5.876</td>\n",
       "      <td>-1.751152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VDW_100</td>\n",
       "      <td>VDW</td>\n",
       "      <td>100</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.388</td>\n",
       "      <td>0.274215</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.230889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GB_2</td>\n",
       "      <td>GB</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.372</td>\n",
       "      <td>0.264255</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.067385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ELE_81</td>\n",
       "      <td>ELE</td>\n",
       "      <td>81</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.212</td>\n",
       "      <td>0.261002</td>\n",
       "      <td>-14.157</td>\n",
       "      <td>-3.948177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GB_68</td>\n",
       "      <td>GB</td>\n",
       "      <td>68</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.577</td>\n",
       "      <td>0.251566</td>\n",
       "      <td>8.527</td>\n",
       "      <td>2.267362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ELE_115</td>\n",
       "      <td>ELE</td>\n",
       "      <td>115</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.250351</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.150962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      terms int_term  pos  prot_res  lig_res   dist   weights  energy_nowt  \\\n",
       "0     GB_67       GB   67      28.0     -1.0  7.932  0.776263        0.538   \n",
       "1    ELE_67      ELE   67      28.0     -1.0  7.932  0.687331       -0.545   \n",
       "2     GB_83       GB   83      35.0     -2.0  5.410  0.544226       18.444   \n",
       "3     GB_49       GB   49      23.0     -1.0  7.567  0.526459       -9.126   \n",
       "4    ELE_83      ELE   83      35.0     -2.0  5.410  0.523596      -18.670   \n",
       "5    ELE_49      ELE   49      23.0     -1.0  7.567  0.511166        9.246   \n",
       "6    VDW_53      VDW   53      24.0     -3.0  3.583  0.492182       -0.489   \n",
       "7    ELE_80      ELE   80      35.0      0.0  5.422  0.487036       22.248   \n",
       "8    ELE_21      ELE   21       4.0     -5.0  3.068  0.429386       -5.782   \n",
       "9   ELE_136      ELE  136      45.0     -6.0  5.671  0.411765      -11.586   \n",
       "10  ELE_102      ELE  102      39.0      0.0  5.299  0.390088       -1.960   \n",
       "11   ELE_68      ELE   68      29.0     -1.0  9.577  0.388268       -8.637   \n",
       "12   ELE_28      ELE   28       5.0     -5.0  5.229  0.381966        0.125   \n",
       "13   GB_136       GB  136      45.0     -6.0  5.671  0.343233       11.445   \n",
       "14    GB_98       GB   98      39.0     -2.0  3.198  0.302559        0.987   \n",
       "15   ELE_82      ELE   82      35.0     -3.0  5.316  0.300016       -0.759   \n",
       "16  ELE_120      ELE  120      43.0     -6.0  4.858  0.292960       -1.778   \n",
       "17  ELE_111      ELE  111      42.0     -4.0  3.397  0.287829       -5.876   \n",
       "18  VDW_100      VDW  100      39.0     -3.0  3.388  0.274215       -0.836   \n",
       "19     GB_2       GB    2       1.0     -4.0  4.372  0.264255       -0.234   \n",
       "20   ELE_81      ELE   81      35.0     -1.0  4.212  0.261002      -14.157   \n",
       "21    GB_68       GB   68      29.0     -1.0  9.577  0.251566        8.527   \n",
       "22  ELE_115      ELE  115      42.0     -5.0  5.555  0.250351       -0.637   \n",
       "\n",
       "    energy_wt  \n",
       "0    0.492927  \n",
       "1   -0.441954  \n",
       "2   10.152541  \n",
       "3   -5.085066  \n",
       "4   -9.888102  \n",
       "5    5.000738  \n",
       "6   -0.265778  \n",
       "7   11.289004  \n",
       "8   -2.485716  \n",
       "9   -5.037529  \n",
       "10  -0.645595  \n",
       "11  -3.544891  \n",
       "12   0.019098  \n",
       "13   4.145568  \n",
       "14   0.376988  \n",
       "15  -0.236113  \n",
       "16  -0.431237  \n",
       "17  -1.751152  \n",
       "18  -0.230889  \n",
       "19  -0.067385  \n",
       "20  -3.948177  \n",
       "21   2.267362  \n",
       "22  -0.150962  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh['pos'] = gh['terms'].str.split('_', expand=True).iloc[:,1].astype(int)\n",
    "#g.reset_index()\n",
    "gh['energy_nowt'] = pd.DataFrame(df_en.drop(columns=['Sum']).iloc[1]).values\n",
    "gh['energy_wt'] = df_en_sc.iloc[:,2:-1].iloc[0].values\n",
    "gh = gh[gh['pos'] < int(df5.shape[0])].reset_index()\n",
    "\n",
    "f = f.reset_index()\n",
    "gg=pd.concat([gh,f], axis = 1)\n",
    "gg = gg[['terms','int_term','pos','prot_res','lig_res','dist','weights','energy_nowt','energy_wt']]\n",
    "\n",
    "gg\n",
    "#pd.concat([gg, df_en.drop(columns=['Sum']).iloc[0].reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50a51875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>int_term</th>\n",
       "      <th>pos</th>\n",
       "      <th>prot_res</th>\n",
       "      <th>lig_res</th>\n",
       "      <th>dist</th>\n",
       "      <th>weights</th>\n",
       "      <th>energy_nowt</th>\n",
       "      <th>energy_wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_67</td>\n",
       "      <td>GB</td>\n",
       "      <td>67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.932</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.492927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ELE_67</td>\n",
       "      <td>ELE</td>\n",
       "      <td>67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.932</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.441954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB_83</td>\n",
       "      <td>GB</td>\n",
       "      <td>83</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.410</td>\n",
       "      <td>0.544226</td>\n",
       "      <td>18.444</td>\n",
       "      <td>10.152541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GB_49</td>\n",
       "      <td>GB</td>\n",
       "      <td>49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.567</td>\n",
       "      <td>0.526459</td>\n",
       "      <td>-9.126</td>\n",
       "      <td>-5.085066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELE_83</td>\n",
       "      <td>ELE</td>\n",
       "      <td>83</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.410</td>\n",
       "      <td>0.523596</td>\n",
       "      <td>-18.670</td>\n",
       "      <td>-9.888102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ELE_49</td>\n",
       "      <td>ELE</td>\n",
       "      <td>49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.567</td>\n",
       "      <td>0.511166</td>\n",
       "      <td>9.246</td>\n",
       "      <td>5.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VDW_53</td>\n",
       "      <td>VDW</td>\n",
       "      <td>53</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.583</td>\n",
       "      <td>0.492182</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.265778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ELE_80</td>\n",
       "      <td>ELE</td>\n",
       "      <td>80</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.422</td>\n",
       "      <td>0.487036</td>\n",
       "      <td>22.248</td>\n",
       "      <td>11.289004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ELE_21</td>\n",
       "      <td>ELE</td>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.068</td>\n",
       "      <td>0.429386</td>\n",
       "      <td>-5.782</td>\n",
       "      <td>-2.485716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ELE_136</td>\n",
       "      <td>ELE</td>\n",
       "      <td>136</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.671</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>-11.586</td>\n",
       "      <td>-5.037529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ELE_102</td>\n",
       "      <td>ELE</td>\n",
       "      <td>102</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.299</td>\n",
       "      <td>0.390088</td>\n",
       "      <td>-1.960</td>\n",
       "      <td>-0.645595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ELE_68</td>\n",
       "      <td>ELE</td>\n",
       "      <td>68</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.577</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>-8.637</td>\n",
       "      <td>-3.544891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ELE_28</td>\n",
       "      <td>ELE</td>\n",
       "      <td>28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>5.229</td>\n",
       "      <td>0.381966</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.019098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GB_136</td>\n",
       "      <td>GB</td>\n",
       "      <td>136</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.671</td>\n",
       "      <td>0.343233</td>\n",
       "      <td>11.445</td>\n",
       "      <td>4.145568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GB_98</td>\n",
       "      <td>GB</td>\n",
       "      <td>98</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.198</td>\n",
       "      <td>0.302559</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.376988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ELE_82</td>\n",
       "      <td>ELE</td>\n",
       "      <td>82</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>5.316</td>\n",
       "      <td>0.300016</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.236113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ELE_120</td>\n",
       "      <td>ELE</td>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>4.858</td>\n",
       "      <td>0.292960</td>\n",
       "      <td>-1.778</td>\n",
       "      <td>-0.431237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ELE_111</td>\n",
       "      <td>ELE</td>\n",
       "      <td>111</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.397</td>\n",
       "      <td>0.287829</td>\n",
       "      <td>-5.876</td>\n",
       "      <td>-1.751152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VDW_100</td>\n",
       "      <td>VDW</td>\n",
       "      <td>100</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.388</td>\n",
       "      <td>0.274215</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.230889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GB_2</td>\n",
       "      <td>GB</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.372</td>\n",
       "      <td>0.264255</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.067385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ELE_81</td>\n",
       "      <td>ELE</td>\n",
       "      <td>81</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.212</td>\n",
       "      <td>0.261002</td>\n",
       "      <td>-14.157</td>\n",
       "      <td>-3.948177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GB_68</td>\n",
       "      <td>GB</td>\n",
       "      <td>68</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.577</td>\n",
       "      <td>0.251566</td>\n",
       "      <td>8.527</td>\n",
       "      <td>2.267362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ELE_115</td>\n",
       "      <td>ELE</td>\n",
       "      <td>115</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.250351</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.150962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      terms int_term  pos  prot_res  lig_res   dist   weights  energy_nowt  \\\n",
       "0     GB_67       GB   67      28.0     -1.0  7.932  0.776263        0.538   \n",
       "1    ELE_67      ELE   67      28.0     -1.0  7.932  0.687331       -0.545   \n",
       "2     GB_83       GB   83      35.0     -2.0  5.410  0.544226       18.444   \n",
       "3     GB_49       GB   49      23.0     -1.0  7.567  0.526459       -9.126   \n",
       "4    ELE_83      ELE   83      35.0     -2.0  5.410  0.523596      -18.670   \n",
       "5    ELE_49      ELE   49      23.0     -1.0  7.567  0.511166        9.246   \n",
       "6    VDW_53      VDW   53      24.0     -3.0  3.583  0.492182       -0.489   \n",
       "7    ELE_80      ELE   80      35.0      0.0  5.422  0.487036       22.248   \n",
       "8    ELE_21      ELE   21       4.0     -5.0  3.068  0.429386       -5.782   \n",
       "9   ELE_136      ELE  136      45.0     -6.0  5.671  0.411765      -11.586   \n",
       "10  ELE_102      ELE  102      39.0      0.0  5.299  0.390088       -1.960   \n",
       "11   ELE_68      ELE   68      29.0     -1.0  9.577  0.388268       -8.637   \n",
       "12   ELE_28      ELE   28       5.0     -5.0  5.229  0.381966        0.125   \n",
       "13   GB_136       GB  136      45.0     -6.0  5.671  0.343233       11.445   \n",
       "14    GB_98       GB   98      39.0     -2.0  3.198  0.302559        0.987   \n",
       "15   ELE_82      ELE   82      35.0     -3.0  5.316  0.300016       -0.759   \n",
       "16  ELE_120      ELE  120      43.0     -6.0  4.858  0.292960       -1.778   \n",
       "17  ELE_111      ELE  111      42.0     -4.0  3.397  0.287829       -5.876   \n",
       "18  VDW_100      VDW  100      39.0     -3.0  3.388  0.274215       -0.836   \n",
       "19     GB_2       GB    2       1.0     -4.0  4.372  0.264255       -0.234   \n",
       "20   ELE_81      ELE   81      35.0     -1.0  4.212  0.261002      -14.157   \n",
       "21    GB_68       GB   68      29.0     -1.0  9.577  0.251566        8.527   \n",
       "22  ELE_115      ELE  115      42.0     -5.0  5.555  0.250351       -0.637   \n",
       "\n",
       "    energy_wt  \n",
       "0    0.492927  \n",
       "1   -0.441954  \n",
       "2   10.152541  \n",
       "3   -5.085066  \n",
       "4   -9.888102  \n",
       "5    5.000738  \n",
       "6   -0.265778  \n",
       "7   11.289004  \n",
       "8   -2.485716  \n",
       "9   -5.037529  \n",
       "10  -0.645595  \n",
       "11  -3.544891  \n",
       "12   0.019098  \n",
       "13   4.145568  \n",
       "14   0.376988  \n",
       "15  -0.236113  \n",
       "16  -0.431237  \n",
       "17  -1.751152  \n",
       "18  -0.230889  \n",
       "19  -0.067385  \n",
       "20  -3.948177  \n",
       "21   2.267362  \n",
       "22  -0.150962  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ce2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4d347aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38979f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, coef0=1, degree=5, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, coef0=1, degree=5, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, coef0=1, degree=5, kernel='poly')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=75, max_features=4,max_samples=0.6, random_state=5)\n",
    "rfc.fit(x_train2, y_train2)\n",
    "#clf = SVC(C = 1, kernel = 'linear')\n",
    "#clf.fit(x_train2, y_train2) \n",
    "clf2 = SVC(kernel='poly', C=1, coef0=1, degree=5)\n",
    "clf2.fit(x_train2, y_train2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b68d8e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e191751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "170568f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "#rfc.fit(X_train, y_train)\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "ax = plt.gca()\n",
    "rfc_y_pred_proba = rfc.predict_proba(x_test2)\n",
    "rfc_y_pred_proba_positive = rfc_y_pred_proba[:, 1] \n",
    "\n",
    "\n",
    "rfc_disp = RocCurveDisplay.from_estimator(rfc, x_test2, y_test2, ax=ax, alpha=0.8)\n",
    "#clf_disp = RocCurveDisplay.from_estimator(clf, x_test2, y_test2, ax=ax, alpha=0.8)\n",
    "clf2_disp = RocCurveDisplay.from_estimator(clf2, x_test2, y_test2, ax=ax, alpha=0.8)\n",
    "\n",
    "y_pred= model.predict(x_test2)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test2,  y_pred)\n",
    "auc = metrics.roc_auc_score(y_test2, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"CCR(auc=\"+str(auc.round(3))+\")\")\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822c7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3822e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70732c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f779a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9c5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d1e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d06f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb787878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cf0d8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpr, tpr, thresholds = metrics.roc_curve(y_test2, model.predict(x_test2))\n",
    "#roc_auc = metrics.auc(fpr, tpr)\n",
    "#display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')\n",
    "#display.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "52b5c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.predict(x_test2)\n",
    "#rfc_pred = rfc.predict(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b90fbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test2 - rfc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "3baf09b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "#sns.lmplot(data=gg, x='weights', y='dist', hue='terms', legend = False )\n",
    "#sns.lineplot(data=gg, x='weights', y='dist', hue='terms',legend=None)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "p1 = ax.scatter(gg['weights'],gg['dist'],c=pd.factorize(gg['terms'])[0])\n",
    "\n",
    "texts = []\n",
    "for i, txt in enumerate(gg['terms']):\n",
    "    texts.append(ax.annotate(txt, xy=(gg['weights'][i], gg['dist'][i]),\\\n",
    "                             xytext=(gg['weights'][i],gg['dist'][i]+.1), size=10, rotation=0))\n",
    "    \n",
    "adjust_text(texts)\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Distance (Ang.)')\n",
    "#plt.xlim(0.003,0.009)\n",
    "#plt.ylim(3, 11)\n",
    "\n",
    "#sns.lmplot(x = 'weights', y='dist', data = gg, line_kws={'color': 'g'})\n",
    "# Annotate label points \n",
    "#for i, language in enumerate(gg['terms']):\n",
    "#    text.append(.annotate(language,xy= (gg['weights'][i], gg['dist'][i]), textcoords='offset points',\\\n",
    "#                 xytext=(10, 10), rotation=45, ha = 'right', va = 'center',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "99749e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(gg['energy_nowt'],gg['energy_wt'],c=pd.factorize(gg['terms'])[0])\n",
    "plt.xlabel(\"Energy_wt\")\n",
    "plt.ylabel(\"Energy_nowt\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "aeac020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEOCAYAAACAfcAXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABHhklEQVR4nO3dd3iUVfbA8e+ZNEJCSYDQQhWVmJAEASEqgZUiLi5IUURc+rquwWXZVcqKBRQBRQWElZ+NRVfFRarYqBFYpQUD0pHepIROEtLu749JxoRMkkkyk0k5n+eZh8ydd+6cS+C989733nPFGINSSqmKx+LuAJRSSrmHdgBKKVVBaQeglFIVlHYASilVQWkHoJRSFZR2AEopVUG5vQMQEQ8R+UlElrs7FqWUqkg83R0AMBLYA1Qt6MCaNWuaxo0buzwgpZQqT+Li4s4bY2rdXO7WDkBEgoHuwCTg7wUd37hxY7Zu3eryuJRSqjwRkaP2yt09BDQdGA1kuDkOpZSqcNzWAYjIg8BZY0xcAcc9ISJbRWTruXPnSig6pVR5dObMGR577DGaNm1Kq1atiIqKYvHixcTGxlKtWjUiIyMJDw+nc+fOnD17Ns969u7dS1RUFD4+PkybNi3Ha0OHDiUoKIiwsDC77502bRoiwvnz553atqJw5xXAPUAPETkCzAfuE5H/3HyQMeZdY0xrY0zrWrVyDWEppZRDjDE89NBDREdHc+jQIeLi4pg/fz4nTpwAoH379sTHx7Njxw7atGnD7Nmz86wrMDCQmTNn8swzz+R6bfDgwXz77bd233f8+HFWrlxJw4YNndOoYnJbB2CMGWeMCTbGNAYeBdYYYx53VzxKqfJtzZo1eHt78+STT9rKGjVqxNNPP53jOGMMV69eJSAgIM+6goKCaNOmDV5eXrlei46OJjAw0O77Ro0axWuvvYaIFLEVzlUaZgEppZTL7dq1izvvvDPP19evX09kZCQJCQn4+fnx6quvOvXzly1bRv369YmIiHD4PXGnT/LGD/9j/4XzNKkewKh293B3A+ddPbj7JjAAxphYY8yD7o5DKVVxxMTEEBERQZs2bYDfhoCOHz/OkCFDGD16tNM+KzExkUmTJjFx4kSH37PxxHH+uPgLNp48zoWkJOJOn2L4l4tZefAXp8VVKjoApZRytdDQULZt22Z7Pnv2bFavXo29ySU9evRg3bp1TvvsgwcPcvjwYSIiImjcuDEnTpzgzjvv5Ndff83zPZPWx5KclpajLDktjUnrY50Wl3YASqkK4b777iM5OZl33nnHVpaYmGj32A0bNnDLLbc47bNbtGjB2bNnOXLkCEeOHCE4OJht27ZRp06dPN+zP8H+LKHjVy6Tmp7ulLj0HoBSqkIQEZYsWWK7EVurVi38/PyYOnUq8Ns9AGMM1apV4/3338+zrl9//ZXWrVtz5coVLBYL06dPZ/fu3VStWpX+/fsTGxvL+fPnCQ4OZsKECQwbNqzQ8das7Mfpa1dzlVfx8cHT4pzv7lKWtoRs3bq10ZXASqmK4NOftzNpfSxJ2YaBfD09iWnTjqfatC1UXSISZ4xpfXO5XgEopVQp1D8snCs3bjB7yybSTQaCMLTlnTzZ+i6nfYZ2AEoplYe5c+cyY8aMHGX33HNPvovEnEVEeLL1XQxt2YqExEQCfX3x8XTuKVuHgJRSqpzLawhIZwEppVQFpR2AUsrlnJWE7fLly/zhD38gIiKC0NBQ5s6da3utoCRsKjftAJRSLuXMJGyzZ8/mjjvuYPv27cTGxvKPf/yDlJQUIP8kbMo+7QCUUi7lzCRsIsLVq1cxxnDt2jUCAwPxzLwxml8SNmWfzgJSSrmUM5OwjRgxgh49elCvXj2uXr3K559/jsVJi6IqIv2bU0qVqOIkYfvuu++IjIzk1KlTxMfHM2LECK5cuVJSoTvEw8ODyMhI22PKlCkAdOzYMdeWttnvgWQ9Vq1alWfdly5dom/fvjRv3pyQkBB+/PFHAC5cuECXLl249dZb6dKlCxcvXnQoVu0AlFIu5cwkbHPnzqV3796ICM2aNaNJkybs3bvXJXEXla+vL/Hx8bbH2LFj8z0+qwPMenTu3DnPY0eOHEm3bt3Yu3cv27dvJyQkBIApU6bQqVMnDhw4QKdOnWydTkG0A1BKuZQzk7A1bNiQ1atXA9aZRfv27aNp06Z2jzXGsGHVLp4d8j5/6TuL/8xZw/WrycVoiXtduXKFdevW2fIKeXt7U716dQCWLl3KoEGDABg0aBBLlixxqE69B6CUcilnJmF7/vnnGTx4MC1atMAYw9SpU6lZsyZAriRsXdo/zMVD1UlOSgXg5JHzrP4ynn8tiMG3so/L2puUlERkZKTt+bhx4+jXr1+ex2e1P8vChQvtdoKHDh2iVq1aDBkyhO3bt9OqVStmzJiBn58fZ86coW7dugDUrVs336m02bmtAxCRSsA6wCczji+MMS+6Kx6llOvUrVuX+fPn233t8uXLDtdTr149VqxYYfe1zz77zPbzxYRrDLx/GqkpqbaylJQ0Lpy7yool2+j5WJTDn1lYWUNAjmrfvj3Lly8v8Li0tDS2bdvG22+/Tdu2bRk5ciRTpkzh5ZdfLnKs7hwCugHcZ4yJACKBbiLSzo3xKKXKib07juPl5ZGr/EZyKlvW73dDRMUXHBxMcHAwbdtaM4H27dvXdm+ldu3anD59GoDTp08TFBTkUJ3u3BTeGGOuZT71ynyUncRESimXmTt3bo6ZMZGRkcTExDj8/uqB/tjLc2bxsFCrbnUnRlpy6tSpQ4MGDdi3bx8Aq1ev5o477gCsN8/nzZsHwLx58+jZs6dDdbo1GZyIeABxQDNgtjFmTH7HazI4pZQjjDEM7zGd08cvkJHx2znOp5IXb/3nzzS9Le+duACSU9JYsC6eb7fso5K3F49ER9C19W2ISIGf7eHhQYsWLWzPu3XrxpQpU+jYsSN79uzBy8sLgKioKGJiYujZsydNmjSxHT9+/Hj69u1rt+74+HiGDx9OSkoKTZs2Ze7cuQQEBJCQkMAjjzzCsWPHaNiwIQsWLMixKC6vZHClIhuoiFQHFgNPG2N23vTaE8ATAA0bNmx19OjRkg9QKVXmnD19iZf++gknj57H4mHBYhFGvtiT6K4t8n1falo6g16bz+FfL3Aj1boZi6+3J79vG8Jzj+U9RbM0K9UdAICIvAhcN8ZMy+sYvQJQShXWqeMJXL96gya31sbTzn2Bm323ZR8TP1lJ0o3UHOU+Xh58Pn4gDYOquyhS1yl1O4KJSC0g1RhzSUR8gc7AVHfFo5Qqn+o1qFGo43/YfSTXyR/AIkL8wZMu7wASEhLo1KlTrvLVq1dTo0bh2lIQd64DqAvMy7wPYAH+a4wpeC6UUkq5UFB1Pzw9LKSlZ+Qot1iEwCqVc5QlpaVijKGyl7fTPr9GjRqFmkZaHG7rAIwxO4CW7vp8pVTZc+bMGUaNGsXGjRsJCAjA29ub0aNHExAQYLuZmpGRQVBQEJ9++mme0yEvX77M448/zrFjx0hLS+OZZ55hyJAhJCcn896EkRw4/isZ6elUbxpB3TbdEKCStxftQhoBcPLaFf4R+zVbfz0JQMugukzr+ACNquadybQ00lQQSqkyoST2FfDx8WHDuu/5bu0G2gx8jmsn9pGWcJzgWtV5d9TDeHpYuJGeRu+ln7Dl1xOkmQzSTAZxZ07Re+mnJKXlHjoqzbQDUEqVCSWxr4CI4O/vz71hTVj+8hAa1KzKS4O6sWTCYJrUsU6rXHX0INdSU0jPNoEmA0NSWipfHdrn5Fa7luYCUkqVCSW1r0B6ejqtWrXil19+ISYmht7du+Z479Erl0i2800/MS2VY1cuFa1xbqJXAEqpMslV+wp4eHgQHx/PiRMn2Lx5Mzt35liaRGiNICp55v7u7OflxR01HEvBUFpoB6CUKhNKel+B6tWr07Fjx1z7DLcPbkyjqgF4W35bU+Bl8aBO5Sp0apR3KuvSSDsApVSZUBL7Cpw7d45Lly4B1rTOq1atonnz5jneaxHhvw/2Z0BIJIGVfAnwqcSjzVvw7wf68Oq21dy1cCZRi2bxenys3aGi0qTUrAR2hK4EVqpiO336NKNGjWLTpk22fQWefPJJateubZsGmn1fgdtuu81uPadOnWLw4MGcPn0aYwxjx47l8ccfZ8eOHQwaNIj09HQyMjJ45JFHeOGFFwqMKyU9nd9//QHHr10iJSMdAB+LB2GBdVjQ9Y8O5RBypVKfCsIR2gEopUqj5Uf3MHbjV1y/6Rt/ZU8vPuz4CG1rN3RTZFZ5dQA6BKSUUsW0I+FUrpM/QFpGBrsu/uqGiByj00CVUuXW3LlzmTFjRo6ye+65J99FYkXRyD8AX0+vXAvBvCweBPtVd+pnOZMOASmlVDFdSUkmeuk7XElJtu1q5SFCzUr+rH/oL3hZCs5C6ko6BKSUUi5S1bsSC7r+kdDAOnhZLHhZLLSqFcwXXf/o9pN/frQDUEqVmLTUNFbN/4HxD09n0uB32Ba7290hFcmZM2d47LHHaNq0Ka1atSIqKoqda9bzD98mnHnyVSpN+ZQ9/3iNwQ/14ezZs3nWs3fvXqKiovDx8WHatJxboQwdOpSgoCDCwsJylMfHx9OuXTsiIyNp3bo1mzdvLnI7tANQSpWI9LR0xvV6k1n/+IStq3ayfmkcEwbMYt4ri90dWqE4kpRux/btDiWlCwwMZObMmTzzzDO5Xhs8eHCuRWgAo0eP5sUXXyQ+Pp6JEyfmu+K5INoBKKVKxI9fx3Mg/ijJiTdsZTcSU/hi1necP3XRjZEVjjOT0gUFBdGmTRvbPsHZRUdH59jXN4uI2NJWXL58mXr16hW1KToLSClVMjZ+E0/y9Ru5yj09Pdi+fi+d+kW5IarCc2ZSuqKYPn06999/P8888wwZGRn88MMPRa7LbVcAItJARNaKyB4R2SUiI90Vi1LK9aoG+mPxyH3KuWGSeG3OxBzj6YsXLyY2NpZq1aoRGRlJeHg4nTt3znc8HSA2NpbIyEhCQ0Pp0KGDrXzGjBmEhYURGhrK9OnTndqu4iSlK4p33nmHt956i+PHj/PWW28xbNiwItflziGgNOAfxpgQoB0QIyJ3uDEepZQL3f/H9rk2ZTfGsCnhG3r0ebDYm7xcunSJp556imXLlrFr1y4WLFgAwM6dO3nvvffYvHkz27dvZ/ny5Rw4cKDI7XBmUrqimDdvHr179wbg4YcfLps3gY0xp40x2zJ/vgrsAeq7Kx6llGs1al6PkdMH4lPZm8pVKuHrX4kU/8vcEtaQmBExvx1XxPH0Tz/9lN69e9OwoTXtQtZ2kHv27KFdu3ZUrlwZT09POnTowOLFRb/x7MykdEVRr149vv/+e8B6P+LWW28tcl2l4h6AiDTGuj/wJjeHopRyoU79orjnwTvZufEAlXy9WbX5G44ePZLn8YUZT9+/fz+pqal07NiRq1evMnLkSAYOHEhYWBjPPfccCQkJ+Pr68vXXX9O6da41UQ4TEZYsWcKoUaN47bXXbEnppk6dmiPm7Enp8vLrr7/SunVrrly5gsViYfr06ezevZuqVavSv39/YmNjOX/+PMHBwUyYMIFhw4bx3nvvMXLkSNLS0qhUqRLvvvtukdvi9g5ARPyBhcDfjDFX7Lz+BPAEYOvZlVJlVyU/H1p3ss5tX7M15zTHmJgYNmzYgLe3N6+//jrt27dn+fLlAEydOpXRo0czZ84cu/WmpaURFxfH6tWrSUpKIioqinbt2hESEsKYMWPo0qUL/v7+RERE4GlnQ5fCqFu3LvPnz7f72uXLlx2up06dOrbhrpt99tlndsvvvfde4uLiHP6M/Lh1GqiIeGE9+X9ijFlk7xhjzLvGmNbGmNa1atUq2QCVUi7lzPH04OBgunXrhp+fHzVr1iQ6Oprt27cDMGzYMLZt28a6desIDAws1rBJeeLOWUACfADsMca86a44lFLu48zx9J49e7J+/XrS0tJITExk06ZNhISEANhmDx07doxFixbRv39/J7aiYHPnziUyMjLHIyYmpuA3upg7h4DuAf4I/Cwi8Zll/zTGfO2+kJRSJcmZ4+khISF069aN8PBwLBYLw4cPt6VR6NOnDwkJCXh5eTF79ux8bya7wpAhQxgyZEiJfqYjNBuoUkqVc5oNVCmlVA7aAShVwuxlkizKyte8MkkmJydz1113ERERQWhoKC+++KLttWeffZbmzZsTHh5Or169bBuglyWldTy9LNIhIKVKkDGGu+++m0GDBtmSiR09epRly5bRokULpk2bZpv2OG7cOLy9vZkwYYLdus6ePcvRo0dZsmQJAQEBtoySxhiuX7+Ov78/qamp3HvvvcyYMYN27dqxYsUK7rvvPjw9PRkzZgyAbbxdlV86BKRUKVASmSRFBH9/fwBSU1NJTU3FOukOunbtapsD365duzznoKuKQTsApUqQo5kkGzZsyKpVqxg6dGiRPic9PZ3IyEiCgoLo0qULbdu2zXXMhx9+yAMPPFCk+lX5oB2AUm7kqkySHh4exMfHc+LECTZv3szOnTtzvD5p0iQ8PT0ZMGBAsdugyi7tAJQqQSWdSbJ69ep07Ngxx85S8+bNY/ny5XzyySe2oSFVMWkHoFQJKolMkufOnbPN7klKSmLVqlU0b94cgG+//ZapU6eybNkyKleuXPgGqHJFZwEpVcJOnz7NqFGj2LRpk23l65NPPknt2rXp2bMnTZo0ybHy9bbbbrNbz82ZJP39/dm9ezdHjhxh0KBBpKenk5GRwSOPPMILL7wAQLNmzbhx4wY1atQArDeC80qupsqPvGYBaQeglFLlnE4DVUoplYPb9wNQSuVv7ty5zJgxI0fZPffck+/2iOXNyWtr2H1hFolpp/H3akhYjZHUrny3u8Mq83QISClVqh2/+g3bzk0k3STbyjzEh7a1p1HH7143RlZ26BCQUqpM2pkwPcfJHyDd3GBnwnT3BFSO5NsBiEiwiDwjIktFZIuIrBORf4lIdxHRzkOpcsrVCesAhg4dSlBQkC1nf5YFCxYQGhqKxWJh85aNJKXnXiMBcC31WPEbWsHleRIXkbnAh0AKMBXoDzwFrAK6ARtEJLokglSquDw8PHJkj5wyZQoAHTt25OZhxewnuazHqlWr8qw7rxMZwNtvv83tt99OaGhokVf1ljRjDA899BDR0dEcOnSIuLg45s+fb8sblLVaeceOHbRp0ybfexGBgYHMnDnTlqguu8GDB+dYoJYlLCyMRYsWER0djeCBt6Wa3bqTL1ZzW1ZVKJu/25vldxP4DWPMTjvlO4FFIuINFGuXdhH5EHgQOGuMyf2/Rykn8fX1JT4+3uHjs29GXpDBgwczYsQIBg4cmKN87dq1LF26lB07duDj45PvSag0yS9hXWxsrK0sK2Fds2bN8qwrKCiIoKAgvvrqq1yvRUdHc+TIkVzlWds4gjWxXfOAP7Hrwts5hoEs+PDaUyd4YmgfPv30U+C3rKoBAQE5fn/jxo1j9uzZeWZVzeqklixZkqPcx8eHNWvW5Miq+sADD9CuXbsy+7u9WZ5XAHmc/LO/nmKM+aWYn/9vrFcTSpVZ0dHRBAYG5ip/5513GDt2LD4+PoD1ZFgWlFTCOkfdUq0/dwTG4GWpiuCBjyWAa7u6UcU3yG1ZVcvq7/ZmRRrHF5GXnPHhxph1wAVn1KVUfpKSknIM6Xz++ef5Hp91kst6HDx4sNCfuX//ftavX0/btm3p0KEDW7ZsKfA9JTH2PmPGDMLCwggNDWX69OkFxuSqhHWOEhFurf44DzaO5Q9N1vP7xqu5cCTQrVlVi/K7LY2KeiM3zqlRKOViWUNAWY9+/frle3zWSS7rUZScPGlpaVy8eJGNGzfy+uuv88gjj5DftOuSGHvfuXMn7733Hps3b2b79u0sX76cAwcO5DimpBPWOUpE8LT42k1gV9JZVQv7uy2titQBGGO+dHYgeRGRJ0Rkq4hstfcPUKnSKjg4mN69eyMi3HXXXVgsFs6fP5/n8SWxWcyePXto164dlStXxtPTkw4dOrB48eIcx5REwrricndW1cL+bkurAjsAEZlp5/GyiPQsiQCNMe8aY1obY1rXqlWrJD5SKad46KGHWLNmDWAdMkhJSaFmzZp5Hl8SY+9hYWGsW7eOhIQEEhMT+frrrzl+/HiOY0SEJUuW8P3339OkSRPuuusuBg0aZNs6MiuOiIgIPv74Y9544408P+/XX38lODiYN998k1deeYXg4GCuXLkCQP/+/YmKimLfvn0EBwfzwQcfALB48WKCg4P58ccf6d69O/fff3+uet2dVbWwv9vSypFUEJWA5sCCzOd9gF3AMBH5nTHmby6KTSmnyboHkKVbt262qaDdu3e3fVOOiooiJibGdpLLMn78ePr27Wu37v79+xMbG8v58+cJDg5mwoQJDBs2jKFDhzJ06FDCwsLw9vZm3rx5hcq/HxMTw4YNG/D29ub111/PMbNl6tSpjB49utCZPENCQhgzZgxdunTB39+fiIgI2xaR2dWtW5f58+fbrePy5csOf16dOnXy3Hbys88+s1veq1cvevXqlW+9WZ3UqFGjeO2112xZVW/upLJnVc3LzVlVp0+fzu7duzl9+nSurKoPPvggQLF/t6WGMSbfB7AG8Mz23DOzzAPYXdD7C6j7M+A0kAqcAIbld3yrVq2MUuXVqlWrTHR0dI6yc+fOmUaNGpm1a9ea7t2728p3795tQkJCCqzzxRdfNK+//nqer48bN87Mnj276EGrMgHYauycUx25B1Af8Mv23A+oZ4xJB24Us/Ppb4ypa4zxMsYEG2M+KE59SpVlJTX2njV76NixYyxatIj+/fsXqZ7s5s6dm2PWVGRkJDExMcWuV7lWgcngRGQYMB6IBQSIBl7F+u39JWPMsy6O0UaTwSl3SUhIoFOnTrnKV69ebdtcxRlcvVlM1apVad++PQkJCXh5efHmm2/abVd5VJGzqhZrQxgRqQvchbUD2GyMOeX8EAumHYBSShVecbOBWoBzWBdtNdMcQEopVfY5Mg10KvA/4Dng2cxH7sxOSqkSp2PveSuJVdWXLl2ib9++NG/enJCQEH788UdXN8upHLkHsA8IN8YU64avM+gQkFLKEcYY7r77bgYNGmRbWJeVLK5FixZMmzYtR7I4b2/vPJPFnT17lqNHj7JkyRICAgJyrK4eNGgQ7du3Z/jw4aSkpJCYmEj16tVd3r7CKs4Q0CHAq8CjlFKqlCiJVdVXrlxh3bp1DBs2DABvb+9SefLPjyMdQCIQLyL/l301sKsDU8rZXD0kcPz4cX73u98REhJCaGhojhknzz//POHh4URGRtK1a1dOnXLLPIoKoyRWVR86dIhatWoxZMgQWrZsyfDhw7l+/Xpxwi5xjnQAy4CXgR+wJoGLA3QcRpUppgQSrXl6evLGG2+wZ88eNm7cyOzZs9m9ezcAzz77LDt27CA+Pp4HH3yQiRMnuq6xKhdXJItLS0tj27Zt/OUvf+Gnn37Cz8/Ptrq8rCiwAzDGzMv+wLoKuLbrQ1PKeUpiSKBu3bq2b51VqlQhJCSEkydPAlC1alXbcdevXy+baQPKkJJIFhccHExwcLAtRXTfvn1zfGZZ4NA0UBGpKSJ/EZF1WBeEaQegypSS3uTkyJEj/PTTT7aTA8Bzzz1HgwYN+OSTT/QKwMVKYlV1nTp1aNCgAfv27QOsiwLvuOOOogXsJvntCVxFRAaKyLfAZqAZ0NQYc4sxRqeBqjLNlZucXLt2jT59+jB9+vQc3/wnTZrE8ePHGTBgALNmzSp2GwqSYTLYf/UYuy4fIjUjzeWfV5qUVEbTt99+mwEDBhAeHk58fDz//Oc/S6R9zpJfNtCzWE/844ENxhgjIvmn6FOqlAoNDWXhwoW257Nnz+b8+fO0bp1rZhw9evSgT58+Rfqc1NRU+vTpw4ABA+jdu7fdYx577DG6d++e57RDZ/jl2gle2vkuienJCIIAo5sP5K4aoS77zNKmJDKaRkZGUpanpuc3BPRPrKmg3wHGiUjJ7/qglJOUxJCAMYZhw4YREhLC3//+9xyvZd91a9myZba88s6SmpbO2i0H+PirLWzY/gvjdswiIeUySek3SExP5np6Mq/umcvZZN2BVf3GkYVgTYH+wKPArcCLwGJjzH7Xh5eTLgQrHTw8PGjRooXt+aOPPsrYsWPp2LEj06ZNy/GtOjY21pbELMu0adPo3Lmz3bqHDh3K8uXLCQoKsm2/B3DhwgX69evHkSNHaNy4Mf/9738JCAhg5cqVjB07lpSUFFve/Pvuu89u3a5OtLZjxw7at29PixYtsFis361effVVfv/739OnTx/27duHxWKhUaNGzJkzh/r16zv+l56PMxeu8qeJ87mamExKajo+tyThc/dZ8MzIcZyXeNCvYVcGNOrmlM8tb8pzsrhiJYPLVkkLrJ1BP2NMiV8RaAdQOvj7+3Pt2rVc5Xl1ANlXXRZk3bp1+Pv7M3DgwBwdwOjRowkMDGTs2LFMmTKFixcvMnXqVH766Sdq165NvXr12LlzJ/fff79t5k1F8fRrX7B193EyMqz/l32aX6Vy24uIV+7/2w/WvZeYWx8u6RCVmxV6JbDYmadmjPnZGPPPrJO/vWOUKo7o6GgCAwNzlS9dupRBgwYB1uX3S5YsAaBly5bUq1cPsI7zJycnc+OG27OWlIiUlDSWr1/B2Rsb8fJKspWnnq5kzdt7k0oWb1oFhpRghKq0y+8ewFoReVpEGmYvFBFvEblPROYBg4rz4SLSTUT2icgvIjK2OHWpkpO1vWLW4/PPP8/3+KwZF1mPgwcPFvozz5w5Q926dQHrzT17K3UXLlxIy5Yt8fHxKXT99pTWRGvGGCZtnM3SI3cR2OBJRjy+hFf/+QG/f2g9YMi47MWNA36Y1N96AR+LN82qNKBNYNGnKXp4eOT4u8ha9NSxY8dcN0Kzr67OeqxatSrPuocOHUpQUBBhYWE5yp999lmaN29OeHg4vXr1su3Ru3nzZlu9ERERuTa2V47JbxZQN2Ao8JmINAEuYb0p7AGsAN4yxsQX9YNFxAOYDXTBuh3kFhFZZozZXdQ6Vcnw9fUlPj7e4eOz72XrKrt27WLMmDGsWLHCaXUOGTKEIUOGOK0+Z/nbj3O4p+aH1Pe+hIcYPCQdgPta7uD8ueps/l8LUjbWpGX12/ENuUZKRir3BbWhc5278BBHM8Dn5srf++DBgxkxYgQDBw7MUd6lSxcmT56Mp6cnY8aMYfLkyUydOpWwsDC2bt2Kp6cnp0+fJiIigj/84Q929zdWecvzb8sYkwz8C/iXiHgBNYEkY8wlJ332XcAvxphDACIyH+gJaAegcqlduzanT5+mbt26nD59mqCgINtrJ06coFevXnz00UdF3iaxrDh9/TKJlh8I9r6Ih+Qc4/e2ZND53p/YGXcnNar58WL3h6nm7+umSAsnOjqaI0eO5Crv2rWr7ed27drxxRdfAFC5cmVbeXJysq6sLiKHvg4YY1KNMaedePIH617Dx7M9P5FZplQuPXr0YN68eQDMmzePnj17AtZ87N27d2fy5Mncc8897gyxRKw4to8Ab/vTVwH8fZIZP/x+5k8e5PSTvzuG/rL78MMPeeCBB2zPN23aRGhoKC1atGDOnDn67b8Iin49WHz2uuxc0xZE5AkR2SoiW+3l8VAl7+YTwdixv92+6d69uy1HysMPW2eb3HwiyPoWZ0///v2Jiopi3759BAcH88EHHwAwduxYVq5cya233mqb+gkwa9YsfvnlF15++WVb/fll8iztMpK+IuNcFzJ+DSPjfHfMjdgcrwdV8ufX5CqkmNwnuwwDF5JuodNdt+Hp6eH02LKGgLIe/fr1y/f4rNXVWY/iXJ1NmjQJT09PBgwYYCtr27Ytu3btYsuWLUyePJnk5OQi119RubPLPAE0yPY8GMiVI9cY8y7wLlingZZMaCo/6enpdstjY2Ptlhdm1eVnn31mt7xGjRqsXr06V/n48eMZP368w/WXZhmJi+DKS0DmiSztAObiXyFgJuLTEYBujUN45ef6xFVtSDv/w3iQgQikG0g1nkQ1muym6F1n3rx5LF++nNWrV9sd6gkJCcHPz4+dO3faXdmt8uZoMrhGItI582dfEanihM/eAtwqIk1ExBvrQrNlTqhXqVLvSuoZdl76in1X1pCSkYQxBq69ge3kb5OMufrbngMiwvTWf2bxoTZ8daEFJ1OrczHNlx2Xm9AwYAnBVcNLtB2u9u233zJ16lSWLVuWY9z/8OHDpKVZ8xsdPXqUffv20bhxYzdFWXYVeAUgIn8CngACgVuwflOfA3QqzgcbY9JEZATwHdaZRR8aY3YVp05VNiQkJNCpU+5/PqtXr6ZGjRpuiKhk/Xju38Rd+C+CxfqN1rxFj+AXqZeRYP8NaUdyPG1TryGxD05iyd6dnLp4hW5Nm9Ptlpoujztr6C9Lt27dbFNBu3fvbkuPHRUVRUxMjG3oL8v48ePp27ev3br79+9PbGws58+fJzg4mAkTJjBs2DBGjBjBjRs36NKlC2C9ETxnzhw2bNjAlClT8PLywmKx8K9//YuaNV3/d1DeOJIKIh7rjJ1NxpiWmWU/G2Na5PtGF9CVwKqsO5n4M0uOjyPN5Pym723x5YlqZxA78yySkurww4aXadmpBTXr5V4kp1RBirMn8A1jTEq2ijyxc7NWKVWw3Ze+Jc3YW6ksXPB6EMg5cyc5ycKscf68HfM+g5qN4JNX8r6BrlRhOdIBfC8i/wR8RaQLsAD40rVhKeU87ljBmhfryd/O9ycDlz1/B1X+ARKAQbhw1ou3x9Zn1X/9SLqWTEpyKp9NWcLP6/c43PbSJCEhIdfK6sjISBIS8hj6Ui7nyCygscAw4Gfgz8DXwPuuDEopZ3LHCta83Fb1dxy+tpHUm4aA0kmjgV9LLB7tMZX/yE+r4pjYdwaJV3Mel5J0g28+WE2L9mUvp0+NGjUK9XtQrudIB+CL9Qbte2BL4eAL5L0aRakKIq8VrHlp6h9FQ7/WHLu+lVSTjAUPLOJJx9pP4+PhD1hn+txINGBnyqMxkHQtKVe5UkXhyBDQanIOTPoCeV8TK1XKuHsFa3YiFrrXf5EHgycQXr0nfwtdyZxHjjCg49gcw1NjXv8755NzLmi7YM4Sa5by8dZ3izU89fzzzxMeHk5kZCRdu3bl1Cnr8puVK1fSqlUrWrRoQatWrVizZo3T2q1KJ0euACoZY2zJ340x10Skcn5vUKo0KW3J60SEhn6taOjXCl/fcfy8PffsZw9PD/o+24OVb/6P1JQ0MtIz8Pb1ppF/E/Yd3I2HR8ErffMannr22Wd5+eWXAZg5cyYTJ05kzpw51KxZky+//LJC761Q0TjSAVwXkTuNMdsARKQVoNegSrnYPT3vouejf+CbD1Zz7eJ1fJt58M0PXzp08oe8h6eyb1R//fp12+rali1b2sqz763grPTaqvRxpAP4G7BARLLSNNQF8k8CopRyyM2Lq8aNG5cjx07j0Ab85c3BgHWG0nOvjclx/MKFC4uUY+e5557jo48+olq1aqxduzbX687eW0GVUsaYAh+AFxAGtAC8HHmPKx6tWrUyShWWxWIxERERtseYMWOMMcZ06NDBBAUFmfr165v69eubvn37mrVr15qqVavmOH7BggV51v3oo4+aOnXqGE9PT1O/fn3z/vvvFyo2Pz8/u+UdOnQwW7ZsyVG2du1a071790LVf/jwYRMaGprn66+++qp54YUXcpTt3LnTNG3a1Pzyyy+F+ixVegFbjZ1zqqPJ4NoAjbFeMbQUEYwxH7mgP1IVwJkzZxg1ahQbN24kICAAb29vRo8eTUBAgG1z9oyMDIKCgvj0009z5P7PbunSpTz//PNYLBY8PT2ZPn069957b67j3JG87nriDeYv28qaH/ZRyceLXt0i+f3vwrBYSlfe+scee4zu3bszYcIEoGLtraAcmAUkIh8D04B7sXYEbQBNuaeKxBjDQw89RHR0NIcOHSIuLo758+dz4sQJ4LcUwjt27KBNmzbMnj07z7o6derE9u3biY+P58MPP2T48OEl1Yx83UhJ44mxn/DJks0cPXmBfYfOMP2DNUz+17fuDg2AAwcO2H5etmwZzZs3Byre3grKsWmgrYF7jDFPGWOeznz81dWBqfJpzZo1eHt78+STT9rKGjVqxNNPP53jOGMMV69eJSAgIM+6/P39bTcws9/MdLbCrmBdvWEvZ85fJSX1tyuP5BuprN6wj+OnLuY41l17K4SFhREeHs6KFSuYMWMGUP72VlAFc2QIaCdQBzjt4lhUBbBr1y7uvPPOPF/POsElJCTg5+fHq6++mm99ixcvZty4cZw9e5avvvrK2eEChV/BuuXnoyTfSM1V7uEh7Np/igb1fuvU3DE8tXDhQrvl5WlvBeUYR64AagK7ReQ7EVmW9XB1YKpiiImJISIigjZt2gC/DQEdP36cIUOGMHr06Hzf36tXL/bu3cuSJUt4/vnnSyLkAtWpWRVPz9z/tQSoEeBf8gEplQdHrgBecnUQquIIDQ3N8Q109uzZnD9/3u5OTj169KBPnz4O1RsdHc3Bgwc5f/682/PC9+gSzoKv4khLy7CVWSxCFf9K3BnWIJ93Fl5F31tBFU+BVwDGmO/tPUoiOFX+3HfffSQnJ/POO+/YyhIT7aeV2rBhQ74zUX755Zesacps27aNlJSUUnHSqxtUjcljehFY3Q/fSl74eHtyS6NavD2xHx4ezt2GO2t46uZHafh7UGWAvbmh2R9AO6zbN14DUoB04EpB7yugzoeBXUAG0NrR9+k6gPLh1KlTpl+/fqZx48amTZs2pmPHjmb+/Pk55uCHh4eb9u3bm3379uVZz5QpU8wdd9xhIiIiTLt27cz69etLsBUFS0/PMIePnzenz1xydygl5uY1F5MnTzbG5L2u4eY1FytXrsyz7iFDhphatWrlWtcQHx9v2rVrZ8LCwsyDDz5oLl++7PyGlXHksQ7AkZP1VqAZ8BPWrRuHAK8W9L4C6gwBbgditQNQJcWVJ6eLFy+aPn36mNtvv900b97c/PDDD8aYindycuXCtu+//97ExcXl6gBat25tYmNjjTHGfPDBB2b8+PGFjLr8y6sDcGghmDHmFxHxMMakA3NF5IdiXnXsAVw2bU8pe1yZFG7kyJF069aNL774gpSUFNuw1vDhw5k2bRodOnTgww8/5PXXX7clYlOFk1duo3379hEdHQ1Aly5duP/++/Xv2EGODEgmiog3EC8ir4nIKMDPxXEpZTN37txcc/BjYmLcHZbNlStXWLduHcOGDQPA29ub6tWrA7lPTnlNwSwv3JF6OywsjGXLrBMTFyxYwPHjx4sUe0XkyBXAH7F2FCOAUUADoHdBbxKRVVjXD9zsOWPMUkcDFJEngCcAGjZs6OjbVDkyZMgQhgwZUux6Ckq8drOsk1OWvBKvHTp0iFq1ajFkyBC2b99Oq1atmDFjBn5+fraTU8+ePSvEyckdqbc//PBD/vrXvzJx4kR69OiBt7d3seqrSBzpAB4yxswAkoEJACIyEpiR35uMMZ2LHx4YY94F3gVo3bq1bkavisxVJ6e0tDS2bdvG22+/Tdu2bRk5ciRTpkzh5Zdf1pNTCWjevDkrVqwAYP/+/S5bEFgeOTIENMhO2WAnx6FUmZWVrqFt27YA9O3bl23btgG/nZzi4uLo37+/Jlhzgax0FRkZGbzyyis50oyo/OXZAYhIfxH5EmiSfQWwiMQC9pOgOEhEeonICSAK+EpEvitOfUq5U506dWjQoAH79u0DrIuw7rjjDqDinZzckdvos88+47bbbqN58+bUq1fPKcOFFYUYY39URUQaAU2AycDYbC9dBXYYY9JcH15OrVu3Nlu3bi3pj1XlhIeHBy1atLA979atG1OmTKFjx47s2bMHLy8vAKKiooiJibGlps4yfvx4+vbta7fu+Ph4hg8fTkpKCk2bNmXu3LkEBAQwY8YMW0bT3r17M3nyZJ39pkqciMQZY3Itt8+zA8j2Rj8gyRiTISK3Ac2Bb4wxubNduZh2AEopVXh5dQCO3AReB7QXkQBgNdaFYf2AAc4NUSlV0Wluo5LlSAcgxphEERkGvG2MeU1EfnJ1YEqVNnpycr3Cpt5WxeNQByAiUVi/8Q8rxPuUKlf05KTKG0emgf4NGAcsNsbsEpGmwFqXRqWUUsrlHE0H3cMYMzXz+SGjW0KqIjhz5gyPPfYYTZs2pVWrVkRFRbF48WJiY2OpVq0akZGRhIeH07lz5wK3IoyNjSUyMpLQ0FA6dOhgK//222+5/fbbadasGVOmTHF1k5Qq0/JbBzA9888vb1oHoDuCqUIzTtwM/tKlSzz11FMsW7aMXbt2sWDBAsC6vWJMTAzffPMNu3fv5rPPPmP37t0l0j6lyqL8xvI/zvxzWkkEosq3/DaDz77/rcncDL5Zs2Z51vXpp5/Su3dvW26ooKAgADZv3kyzZs1o2rQpAI8++ihLly61LcpSSuWUZwdgjInL/PN7EamV+fO5kgpMlS/O3Ax+//79pKam0rFjR65evcrIkSMZOHAgJ0+epEGD37ZcDA4OZtOmTU5th1LlSX5DQCIiL4nIeWAvsF9EzonICyUXniqvirMZfFpaGnFxcXz11Vd89913vPzyy+zfvx97ixp11a1SectvCOhvwD1AG2PMYYDMGUDviMgoY8xbJRCfKiecuRl8cHAwNWvWxM/PDz8/P6Kjo9m+fTvBwcE50i2fOHGCevXq5RvXqRMX2PTDL3h7e3JPh9upHqBbXaiKI79ZQAOB/lknf7DOAAIez3xNKYc5czP4nj17sn79etLS0khMTGTTpk2EhITQpk0bDhw4wOHDh0lJSWH+/Pn06NEjz3o+/nAdf3r8Xd7/12rembmCAb3fZv3aPUVvpFJlTH5XAF7GmPM3FxpjzomIlwtjUuWQiLBkyRJGjRrFa6+9Rq1atfDz82Pq1KnAb/cAjDFUq1aN999/P8+6QkJC6NatG+Hh4VgsFoYPH05YWBgAs2bN4v777yc9PZ2hQ4cSGhpqt479e0/z+cc/kJKSM6fhlIlLiWzVmCpVfZ3UcqVKr/yygW4zxti9a5ffa66kyeDKnrSMNE4n/4qfZ2UCvQPdHY7NnJkrWfT5Jm7+5+/r68Vfn/09nbu1sP9GpcqgoiSDixCRK/bqAio5LTJVbv3v/I98fPQTjDGkm3Sa+DXh6VufoqpXFXeHRnp6ht1ygzV3v1IVQZ73AIwxHsaYqnYeVYwxOgSk8nXw2iH+feQjktKTSM5IJtWkcvDaQd7an+9Oojm4cjP4DveF4O2T+59xRrqhTbu81yAoVZ64JambiLwO/AFIAQ4CQ4wxl9wRi3KNb3/9jpSMlBxl6aRzIukkp5N+pa5vnQLrcNZm8PaEhjeg24MRfLt8Oyk3UvHwsGDxsBDzt/sJCNSZQKpicFdWz5XAOGNMmohMxZpsboybYlEukHDjgt1yD/HgUuolhzoAVxIRRvy9G10eCOfH9fvx9vGkY6c7qBdceu5TKOVqbukAjDErsj3dCNjfZ0+VWS2qhXE08RhpN+0cmpaRRsPKDd0UVW63h9Tj9pD81wooVV45kg7a1YYC37g7COVcXWp3ws/TD0/xsJX5WLz5Q73u+HlWdmNkSqksLrsCEJFVgL3r/OeMMUszj3kOSAM+yaeeJ4AnAFvyL1X6+Xv580rYS3x1+hu2X9pBFc8qdKvblVYBJT57WCmVhwI3hXfZB4sMAp4EOhlj7C8JvYmuA1BKqcIrzqbwrgimG9abvh0cPfkrpZRyLnfdA5gFVAFWiki8iMxxUxxKKVVhuWsWkK60UUopNysNs4CUUkq5gXYASilVQWkHoJRSFZR2AEopVUFpB6CUUhWUdgBKKVVBaQeglFIVlHYASilVQWkHoJRSFZR2AGWMh4dHji0Sp0yZAkDHjh25OVFebGws1apVy3H8qlWr8qz7rbfeIjQ0lLCwMPr3709ycjIAzz//POHh4URGRtK1a1dOnTrlugYqpUqMu3YEU0Xk6+tLfHy8w8e3b9+e5cuXF3jcyZMnmTlzJrt378bX15dHHnmE+fPnM3jwYJ599llefvllAGbOnMnEiROZM0fTNylV1ukVgLJJS0sjKSmJtLQ0EhMTqVfPulNW1apVbcdcv34dESnRuNxx1bNgwQJCQ0OxWCy5PkOp8kKvAMqYpKQkIiMjbc/HjRtHv3798jx+/fr1OY5fuHAht9xyS67j6tevzzPPPEPDhg3x9fWla9eudO3a1fb6c889x0cffUS1atVYu3atU9riKHdc9YSFhbFo0SL+/Oc/FyNypUo3vQIoY7JOhlmP/E7+YD0ZZj/e3skf4OLFiyxdupTDhw9z6tQprl+/zn/+8x/b65MmTeL48eMMGDCAWbNmObVN7pTXVU9ISAi33367m6NTyrW0A1AArFq1iiZNmlCrVi28vLzo3bs3P/zwQ67jHnvsMRYuXFiisWVd9WQ9Pv/883yPz7rqyXocPHjQ7nHZr3rq1q1LtWrVclz1KFXe6RCQAqz7LW/cuJHExER8fX1ZvXo1rVtbd5A7cOAAt956KwDLli2jefPmJRqbq4aAsl/1VK9enYcffpj//Oc/PP7448WIVqmywy1XACLysojsyNwNbIWI1HNHHGXRzd+Gx44da3ute/fuBAcHExwczMMPPwzk/jb8xRdf2K23bdu29O3blzvvvJMWLVqQkZHBE088AcDYsWMJCwsjPDycFStWMGPGDNc3tAQ4etWjVHnlriuA140xzwOIyF+BF7BuEO9yHh4etGjRwvb80UcfZezYsXTs2JFp06bZvvWCdUZJz549adKkia1s2rRpdO7c2W7dQ4cOZfny5QQFBbFz505b+fPPP8/SpUuxWCwEBQXx73//m3r16nHkyJEcY83t2rUrcHplenq63fLY2Fi75ZcvX863vuwmTJjAhAkTcpWX9JBPScnvqkepCsEY49YHMA54x5FjW7VqZYrLz8/PbnmHDh3Mli1bcpStXbvWdO/e3eG6v//+exMXF2dCQ0NzlF++fNn284wZM8yf//xnY4wxhw8fznVsaZeckmpe/WqtuXPi2+aO598yj7/3udl3+pxLP9NisZiIiAjbY8yYMcYY6+8sKCjI1K9f39SvX9/07dvXrF271lStWjXH8QsWLMiz7hdeeMHcfvvtJjQ01Dz++OMmOTnZGGPMokWLTP369Y23t7cJCgoyXbt2dWkblXIlYKuxc0512z0AEZkEDAQuA79zVxzOFB0dzZEjR3KVu3sefXYJCQl06tQpV/nq1aupUaNGge8fOX85Gw8d40aa9Upk69GTPPbefJb/dRB1qlVxerzgnqueXr160atXL4frUaosclkHICKrgDp2XnrOGLPUGPMc8JyIjANGAC/mUc8TwBNgvWQvLlfNoy9IXvPoDx8+TMuWLalatSqvvPIK7du3L3TdhVGjRo1C3VDN7mjCJTYeOm47+WdJSc/gPxvjeeb+4sd+5sQFPnrjG37asI8q1SvTe3hHuj7S1q2dplLllcs6AGOM/YHy3D4FviKPDsAY8y7wLkDr1q1NceNy1YySgkyaNIlJkyYxefJkZs2axYQJE6hbty7Hjh2jRo0axMXF8dBDD7Fr164cVwylyaHzF/DysHAjLWd5ano6u0+dLXb9F85e5ukH3+D6lSQyMgwXz13lnZcWc/yXswx/rkeR6y3uVY9S5ZW7ZgHdmu1pD2CvO+Jwh+zz6H18fGwnoFatWnHLLbewf/9+d4aXr6Y1A0lNz8hV7uXhwR31gopd/6L3vyfp+g0yMn7r528kpfDlR+u5cvF6kevNuuq5+aEnf1XRuWsh2BQR2SkiO4CuwEg3xVEiDhw4YPs5+zz6c+fO2ca3Dx06xIEDB2jatKlbYnREoxrViWraAB9Pjxzl3p4ePN4ustj1/7zpIGmpucf7Pb09ObLvdLHrV0rl5JYOwBjTxxgTZowJN8b8wRhzsqQ+21Xz6AH69+9PVFQU+/btIzg4mA8++ADIex79unXrCA8PJyIigr59+zJnzhwCAwNd2Prim/7ogzx6VwR+3l5YRGjduD6f/qmfU24A12tcE7HkHutPS02nVr3qxa5fKZWTWGcIlQ2tW7c2mpmx/Ppl5wmeeXgmN5JSbWVe3h6EtrmFyZ/8xY2RKVW2iUicMSbXIpdynwsoOXktCecHcu5cT65dfZeMjER3h6Ty0CwsmHGzBhEYVBVvHy88vT24675Qxs8Z4u7QlCqXyvUVwJUrb3D92r8wJimzpBKenk2oFbQcEd8ixaAzSlwvIyODC2ev4OtXCb8qldwdjlJlXl5XAOU2GVx6+nmuXZ0F3MhWmkx6+hESExfh5zegSPUWZx69cozFYqFmneruDkOpcq/cDgGlpGxBxDtXuTFJJCevcENESilVupTbDsBiCQTsDW954GGpVdLhKKVUqVNuOwBv7zZYLNWAm6YVihd+foPdEZJSSpUq5bYDELFQo+Z/8fBojEhlRKog4ke1alPx8g5zd3hKKeV25fYmMICnZxOCam8gLXUXGeYa3t4RRZ79o5RS5U257gAARES/8SullB3ldghIKaVU/rQDKMM8PDxy5CmaMmUKAB07duTmBXOxsbFUq1Ytx/GrVq3Ks+6hQ4cSFBREWFjOq6cFCxYQGhqKxWLJ9RkAx44dw9/fn2nTpjmhhUopVyr3Q0DlmSv3Nhg8eDAjRoxg4MCBOcrDwsJYtGgRf/7zn+2+b9SoUTzwwAMOx6SUch/tAJRdeW1vGRISkud7lixZQtOmTfHz83NhZEopZ9EhoDLs5tTWn3/+eb7H35za+uDBg06L5fr160ydOpUXX7S7sZtSqhTSK4AyzF3bW9rz4osvMmrUKPz9/V1Sv1LK+dzaAYjIM8DrQC1jzHl3xqKKZ9OmTXzxxReMHj2aS5cuYbFYqFSpEiNGjHB3aEqpPLitAxCRBkAX4Ji7YlDOs379etvPL730Ev7+/nryV6qUc+c9gLeA0djP2KYc4I7tLRcvXkxwcDA//vgj3bt35/7773dtI5VSLuOWDWFEpAfQyRgzUkSOAK0dGQLSLSGVUqrwSnxDGBFZBdSx89JzwD+Brg7W8wTwBEDDhg2dFl9ZdfFaEr9eukqDmtXwr+Tj7nCUUmVYiV8BiEgLYDWQtTlvMHAKuMsY82t+763IVwCpaem8+N+VfLd9P14eHqSlpzOgfUv+1v1eRKTgCuzQ7S2VqhhKzZaQxpifgaCs54UZAqrI3vhyHSt2HCAlLZ2UtHQAPt0QT+3q/jx2b8si1anbWypVselCsDIgLT2DhZt2ciM1LUd5cmoa/14b56aolFJlndsXghljGrs7htIuJS2N1PR0u69dSkwu4WiUUuWFXgGUAb7eXtQNqGr3tYhG9u6zK6VUwbQDKANEhPG976OSl6dth2OLCJW9vXimRwe3xqaUKrvcPgSkHHNP88bMfeph3lu9mcNnLxDWoA5/6nwXTYIC3R2aUqqM0g6gDAlrWIcZQ3q4OwylVDmhQ0BKKVVBaQeglFIVlHYASilVQWkHoJRSFZR2AEopVUG5JR10UYnIOeCou+MoQE2gPOU10vaUfuWtTdoe52tkjKl1c2GZ6gDKAhHZai/rXlml7Sn9ylubtD0lR4eAlFKqgtIOQCmlKijtAJzvXXcH4GTantKvvLVJ21NC9B6AUkpVUHoFoJRSFZR2AEopVUFpB6BUGSYikv1PpQpDOwAXE5G6IuLr7jicpby1B8pem0TEQ0RGisgXQAyAKUc380SkpYjc5u44iitb59xGRN4Ukd4i4pX9NXfTDsBFRKSZiKwEfgDeEZEId8dUHOWtPVCm29QZuB/r7JLemZ1BTTfHVGyZv48NwL+BF0RkoJtDKjIREWOMEZFo4EMgGevv7TUoPR22dgBOlNW7Z+oJ/GyMaQIcBMaISG33RFY0IpL930eZb48dPSibbRoIrDDGrADGA3WxtqVMERE/EYnKVnQ7EGeMiQDeBkaISFjmsaXiG3N+RKSyiDwpIp8CwzPPB3cCc4wx/wSeAzqKSIfM493eJu0AiklEIkRktohsBcaLSNvMlyoBlzN/ng2kA/eUhl96fkQkUkT+T0T+B4wVkZDMl8pkewBEpLmILBCRaSLine0lb8pmm34EGmf+/BNwAGgpIh5ui6iQROSfwCHg62yd7gPAPgBjzCZgE/DXzNdK9blKROoAy4GOwMfAE0BvIApIBTDGXASW8lub3P7vrFT/pZZ2InIrMBI4CQwGrgOjMl/2BM6KSGVjzAVgFxAKlNpNfDPb8wLWb8MjAH9gdObLHpSx9gCISADQB6gCtAHqZ3vZhzLYJqwnTn8RqWGMScp8DnCHG2MqrB+ALsBC4PHMskNA92zHzAYeBDDGpJdodIV3GRhnjHnUGPMN1thbAV8Dw7Id93/APQDGmIwSj/Im2gEUgzHmAPAnY8yrxpidWL9B/i/z5ZNAUyA48/lmIITMbwOl1EHgdWPMa8aYn4DvgAwRqYQ1C2sTylZ7sr51fWKM6QYkAG2yfVM+SRlsE/AzcAPolPn8IlAbOOe2iApvvTFmB7CS34avPsR6BeYDYIzZCxwXkbvdFGNhJAObs1097gDaGmPmAc0yrxAwxpwGtovIvW6KMwftAIpIRCyZN3rSReQWEZmP9dtzUxG5A1gGVAVaAxhj1gDtgVJx8+dmIuJhjMkwxvyY7QTZCKhkjEnGejnuRxlpD+S4h3Es8891wH1Y25H1vEy1KdNJYCO/DSWcw9oBXHVbRIWU7Rv9KqCKiEQYYy4BW4Ensx36E9arz1LNZMpWNBJYlPnz18DfAEQkEDhOKUlr7+nuAMqSzN7dYoxJv+ny7TwwDfgj8AfgfaxjgQuxzmY4i/UG19dYrxJKhZvaY4sr28/hWL8VY4zZKyLfYL1RWirbA3n+jrL+Yy7FejO7sYjsNMbsF5FvgdGluU03y2zXxyLSTUS+wjq09TLWq4IyxRiTICIbsf7f2Q5MBJ7KvIEagPUK7X/5VFGqZM78CQbqAN9kFr8EPCEiy7EOQcYbY467KcQctANwUNa0LjJPDiLSDus/2jXAYmPM1szyJcBTQAdjzAoR8cd6Q6gaMN4Yk+iO+G+WX3uMMQtFpC7Wb8NZwwwYY5ZnXp6XuvZA/m0SkaXGmMMichpokTn8gDHmy9LcpgIMxjruvz/zXkBZ9S4wM/OkfwnrzKaxQBLw99IwVl5ILbEO0x0WkeHACeCfQD9grzFmmzuDy06TwRWCiDTBOgXvAaxjfquAt40xl7Md0xvrjau/GWOuZJZZSuM/YjvtWQn8yxhzUUSGAS2At4B/AIeMMdMz31cq2wMF/45E5CHg78Ap4Lgx5tnM8lLbpvJORB4FPgUSsV7JvF6WfxeZM+iaAkew/jubkPWFo7TRKwAHiUhnYDrWIYI/GWN+zvaaH9Y5vr/DegPx/7JO/lA67vbfrID2VAaGA82xzmP+EuvUNqB0tgcKbJMFaJv5eiLWjsGWpre0tqm8E5FwYBDwJ6w365PdHFKxZF7FrMV6Q/s/xphSPSynVwAOEhFPY0zaTWUeWIf9MkRkALDLGBPvlgALyYH2jAa+zpzdVCbk06aMzLHZZkDl0vptTKmSph1AIWU/obg7Fmcob+2B8tkmpVxBOwCllKqgdB2AUkpVUNoBKKVUBaUdgFJKVVDaASilVAWlHYBSSlVQ2gGoCktE3hKRv2V7/p2IvJ/t+Rsi8vc83jsxc+FZfvW/JCLP2CmvLiJPFSN0pZxCOwBVkf0A3A22lcI1se4HkOVu8khEZox5wRizqoifWx1rviil3Eo7AFWR/Y/MDgDriX8ncFVEAjITxIUAiMj3IhKXeYVQN7Ps3yLSN/Pn34vIXhHZICIzM7M+ZrlDRGJF5JCIZKVvngLcIiLxIvK6WDelX5f5fKeItC+JxiuluYBUhWWMOSUiaSLSEGtH8CPWdL1RWHd42oM1GV5PY8w5EekHTAKGZtWRuVnO/wHRmdlGP7vpY5pjzRFVBdgnIu9gzXQZZoyJzKzjH8B3xphJmauYK7us0Uplox2AquiyrgLuBt7E2gHcjbUDOAl0BVZmbvTkAZy+6f3NsWZKPZz5/DOsqaWzfJWZEOxG5p4D9jad3wJ8mJlIbElZySelyj7tAFRFl3UfoAXWIaDjWNNfX8G610N9Y0xUPu8vaGPv7Nkg07Hzf84Ys05EorHuh/uxiLxujPnI8SYoVTR6D0BVdP/Dun/DhcxdxC5gvUkbBXwO1BKRKLCm+hWR0JvevxfrNqCNM5/3c+Azr2IdEiKz3kbAWWPMe8AHWFNwK+VyegWgKrqfsc7++fSmMn9jzNnMG70zRaQa1v8v04FdWQcaY5Iyp3R+KyLnydxCMz+Z2yD+T0R2Yt02cCfwrIikAtewbmijlMtpNlCliklE/I0x1zL3I54NHDDGvOXuuJQqiA4BKVV8fxKReKxXBtWwzgpSqtTTKwCllKqg9ApAKaUqKO0AlFKqgtIOQCmlKijtAJRSqoLSDkAppSoo7QCUUqqC+n/++71d9vGXGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "#sns.lmplot(data=gg, x='weights', y='dist', hue='terms', legend = False )\n",
    "#sns.lineplot(data=gg, x='weights', y='dist', hue='terms',legend=None)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "p1 = ax.scatter(gg['energy_nowt'],gg['energy_wt'],c=pd.factorize(gg['terms'])[0])\n",
    "\n",
    "texts = []\n",
    "for i, txt in enumerate(gg['terms']):\n",
    "    texts.append(ax.annotate(txt, xy=(gg['energy_nowt'][i], gg['energy_wt'][i]),\\\n",
    "                             xytext=(gg['energy_nowt'][i],gg['energy_wt'][i]+.1), size=10, rotation=0))\n",
    "    \n",
    "adjust_text(texts)\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Distance (Ang.)')\n",
    "#plt.xlim(0.003,0.009)\n",
    "#plt.ylim(3, 11)\n",
    "\n",
    "#sns.lmplot(x = 'weights', y='dist', data = gg, line_kws={'color': 'g'})\n",
    "# Annotate label points \n",
    "#for i, language in enumerate(gg['terms']):\n",
    "#    text.append(.annotate(language,xy= (gg['weights'][i], gg['dist'][i]), textcoords='offset points',\\\n",
    "#                 xytext=(10, 10), rotation=45, ha = 'right', va = 'center',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be759bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2 0.683 (0.008)\n",
      ">3 0.699 (0.006)\n",
      ">4 0.707 (0.003)\n",
      ">5 0.720 (0.011)\n",
      ">6 0.717 (0.004)\n",
      ">7 0.727 (0.009)\n",
      ">8 0.725 (0.006)\n",
      ">9 0.730 (0.006)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explore the number of selected features for RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = x_train2, y_train2\n",
    "\treturn X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(2, 10):\n",
    "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "\t\tmodel = DecisionTreeClassifier()\n",
    "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.savefig('RandomForest.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a41f747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 664, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " Conv1D_1 (Conv1D)              (None, 661, 16)      80          ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 661, 16)     64          ['Conv1D_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 661, 16)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " Conv1D_2 (Conv1D)              (None, 658, 12)      780         ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 329, 12)     0           ['Conv1D_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 12)          0           ['max_pooling1d_9[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " Flatten_1 (Flatten)            (None, 12)           0           ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)                (None, 12)           156         ['Flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 12)           0           ['Dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " Dense_3 (Dense)                (None, 1)            13          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1)            0           ['Dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " Contextual_Weight (Dense)      (None, 664)          1328        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 664)          0           ['Contextual_Weight[0][0]']      \n",
      "                                                                                                  \n",
      " Flatten_2 (Flatten)            (None, 664)          0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 664)          0           ['dropout_5[0][0]',              \n",
      "                                                                  'Flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " Sum (Dense)                    (None, 1)            664         ['multiply_4[0][0]']             \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            2           ['Sum[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,087\n",
      "Trainable params: 3,055\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# evaluate the models and store results\u001b[39;00m\n\u001b[1;32m     71\u001b[0m results, names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m     73\u001b[0m \tscores \u001b[38;5;241m=\u001b[39m evaluate_model(model, X, y)\n\u001b[1;32m     74\u001b[0m \tresults\u001b[38;5;241m.\u001b[39mappend(scores)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# explore the number of selected features for RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = x_train2, y_train2\n",
    "\treturn X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "#\tmodels = dict()\n",
    "#\tfor i in range(2, 10):\n",
    "#\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "#\t\tmodel = DecisionTreeClassifier()\n",
    "#\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "    layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer17)\n",
    "    #model.layers[16].trainable = False\n",
    "\n",
    "    models.summary()\n",
    "    models.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.savefig('RandomForest.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "35b2f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 664, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " Conv1D_1 (Conv1D)              (None, 661, 16)      80          ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 661, 16)     64          ['Conv1D_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_18 (MaxPooling1D  (None, 661, 16)     0           ['batch_normalization_9[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv1D_2 (Conv1D)              (None, 658, 12)      780         ['max_pooling1d_18[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_19 (MaxPooling1D  (None, 329, 12)     0           ['Conv1D_2[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_9 (Gl  (None, 12)          0           ['max_pooling1d_19[0][0]']       \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " Flatten_1 (Flatten)            (None, 12)           0           ['global_average_pooling1d_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)                (None, 12)           156         ['Flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 12)           0           ['Dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " Dense_3 (Dense)                (None, 1)            13          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1)            0           ['Dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " Contextual_Weight (Dense)      (None, 664)          1328        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 664)          0           ['Contextual_Weight[0][0]']      \n",
      "                                                                                                  \n",
      " Flatten_2 (Flatten)            (None, 664)          0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)          (None, 664)          0           ['dropout_5[0][0]',              \n",
      "                                                                  'Flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " Sum (Dense)                    (None, 1)            664         ['multiply_9[0][0]']             \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            2           ['Sum[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,087\n",
      "Trainable params: 3,055\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "247/247 [==============================] - 2s 4ms/step - loss: 0.2046 - accuracy: 0.6998 - val_loss: 0.1837 - val_accuracy: 0.7690\n",
      "Epoch 2/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1656 - accuracy: 0.7771 - val_loss: 0.1627 - val_accuracy: 0.7842\n",
      "Epoch 3/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1597 - accuracy: 0.7829 - val_loss: 0.1616 - val_accuracy: 0.7700\n",
      "Epoch 4/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1589 - accuracy: 0.7758 - val_loss: 0.1568 - val_accuracy: 0.7852\n",
      "Epoch 5/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1569 - accuracy: 0.7817 - val_loss: 0.1556 - val_accuracy: 0.7842\n",
      "Epoch 6/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1544 - accuracy: 0.7832 - val_loss: 0.1561 - val_accuracy: 0.7801\n",
      "Epoch 7/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1546 - accuracy: 0.7839 - val_loss: 0.1542 - val_accuracy: 0.7832\n",
      "Epoch 8/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1541 - accuracy: 0.7850 - val_loss: 0.1550 - val_accuracy: 0.7812\n",
      "Epoch 9/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1517 - accuracy: 0.7908 - val_loss: 0.1554 - val_accuracy: 0.7761\n",
      "Epoch 10/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1509 - accuracy: 0.7918 - val_loss: 0.1620 - val_accuracy: 0.7822\n",
      "Epoch 11/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1505 - accuracy: 0.7882 - val_loss: 0.1532 - val_accuracy: 0.7903\n",
      "Epoch 12/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1494 - accuracy: 0.7915 - val_loss: 0.1534 - val_accuracy: 0.7862\n",
      "Epoch 13/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1487 - accuracy: 0.7964 - val_loss: 0.1547 - val_accuracy: 0.7852\n",
      "Epoch 14/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.7834 - val_loss: 0.1523 - val_accuracy: 0.7913\n",
      "Epoch 15/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1481 - accuracy: 0.7920 - val_loss: 0.1515 - val_accuracy: 0.7893\n",
      "Epoch 16/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1466 - accuracy: 0.7961 - val_loss: 0.1542 - val_accuracy: 0.7882\n",
      "Epoch 17/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1467 - accuracy: 0.7956 - val_loss: 0.1554 - val_accuracy: 0.7862\n",
      "Epoch 18/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1461 - accuracy: 0.7956 - val_loss: 0.1541 - val_accuracy: 0.7913\n",
      "Epoch 19/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1455 - accuracy: 0.7951 - val_loss: 0.1554 - val_accuracy: 0.7781\n",
      "Epoch 20/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1451 - accuracy: 0.7961 - val_loss: 0.1551 - val_accuracy: 0.7812\n",
      "Epoch 21/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1442 - accuracy: 0.8009 - val_loss: 0.1530 - val_accuracy: 0.7923\n",
      "Epoch 22/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.7969 - val_loss: 0.1530 - val_accuracy: 0.7913\n",
      "Epoch 23/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.8037 - val_loss: 0.1534 - val_accuracy: 0.7913\n",
      "Epoch 24/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.8042 - val_loss: 0.1529 - val_accuracy: 0.7903\n",
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.8027 - val_loss: 0.1537 - val_accuracy: 0.7822\n",
      "Epoch 26/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.8009 - val_loss: 0.1513 - val_accuracy: 0.7923\n",
      "Epoch 27/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.7986 - val_loss: 0.1511 - val_accuracy: 0.7832\n",
      "Epoch 28/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.8027 - val_loss: 0.1538 - val_accuracy: 0.7964\n",
      "Epoch 29/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.8007 - val_loss: 0.1516 - val_accuracy: 0.7872\n",
      "Epoch 30/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.8047 - val_loss: 0.1519 - val_accuracy: 0.7943\n",
      "Epoch 31/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.8067 - val_loss: 0.1521 - val_accuracy: 0.7832\n",
      "Epoch 32/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.8060 - val_loss: 0.1538 - val_accuracy: 0.7872\n",
      "Epoch 33/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1406 - accuracy: 0.8042 - val_loss: 0.1509 - val_accuracy: 0.7812\n",
      "Epoch 34/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8029 - val_loss: 0.1522 - val_accuracy: 0.7852\n",
      "Epoch 35/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.8050 - val_loss: 0.1538 - val_accuracy: 0.7801\n",
      "Epoch 36/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1406 - accuracy: 0.8080 - val_loss: 0.1517 - val_accuracy: 0.7832\n",
      "Epoch 37/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8017 - val_loss: 0.1497 - val_accuracy: 0.7862\n",
      "Epoch 38/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8019 - val_loss: 0.1527 - val_accuracy: 0.7852\n",
      "Epoch 39/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8042 - val_loss: 0.1500 - val_accuracy: 0.7893\n",
      "Epoch 40/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8123 - val_loss: 0.1535 - val_accuracy: 0.7872\n",
      "Epoch 41/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8040 - val_loss: 0.1529 - val_accuracy: 0.7832\n",
      "Epoch 42/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8065 - val_loss: 0.1517 - val_accuracy: 0.7812\n",
      "Epoch 43/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8032 - val_loss: 0.1507 - val_accuracy: 0.7791\n",
      "Epoch 44/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8085 - val_loss: 0.1504 - val_accuracy: 0.7771\n",
      "Epoch 45/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8090 - val_loss: 0.1517 - val_accuracy: 0.7812\n",
      "Epoch 46/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8032 - val_loss: 0.1540 - val_accuracy: 0.7862\n",
      "Epoch 47/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1379 - accuracy: 0.8060 - val_loss: 0.1523 - val_accuracy: 0.7791\n",
      "Epoch 48/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8075 - val_loss: 0.1509 - val_accuracy: 0.7822\n",
      "Epoch 49/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8067 - val_loss: 0.1491 - val_accuracy: 0.7872\n",
      "Epoch 50/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8095 - val_loss: 0.1518 - val_accuracy: 0.7812\n",
      "Epoch 51/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8045 - val_loss: 0.1512 - val_accuracy: 0.7903\n",
      "Epoch 52/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8067 - val_loss: 0.1512 - val_accuracy: 0.7842\n",
      "Epoch 53/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8083 - val_loss: 0.1495 - val_accuracy: 0.7872\n",
      "Epoch 54/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8113 - val_loss: 0.1518 - val_accuracy: 0.7842\n",
      "Epoch 55/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8080 - val_loss: 0.1512 - val_accuracy: 0.7862\n",
      "Epoch 56/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8065 - val_loss: 0.1521 - val_accuracy: 0.7882\n",
      "Epoch 57/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8070 - val_loss: 0.1505 - val_accuracy: 0.7801\n",
      "Epoch 58/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8100 - val_loss: 0.1495 - val_accuracy: 0.7842\n",
      "Epoch 59/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8057 - val_loss: 0.1500 - val_accuracy: 0.7872\n",
      "Epoch 60/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8103 - val_loss: 0.1506 - val_accuracy: 0.7862\n",
      "Epoch 61/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8103 - val_loss: 0.1483 - val_accuracy: 0.7903\n",
      "Epoch 62/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8072 - val_loss: 0.1509 - val_accuracy: 0.7812\n",
      "Epoch 63/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8126 - val_loss: 0.1486 - val_accuracy: 0.7903\n",
      "Epoch 64/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8118 - val_loss: 0.1493 - val_accuracy: 0.7842\n",
      "Epoch 65/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8078 - val_loss: 0.1484 - val_accuracy: 0.7832\n",
      "Epoch 66/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8121 - val_loss: 0.1495 - val_accuracy: 0.7893\n",
      "Epoch 67/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8095 - val_loss: 0.1500 - val_accuracy: 0.7882\n",
      "Epoch 68/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8088 - val_loss: 0.1531 - val_accuracy: 0.7822\n",
      "Epoch 69/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1346 - accuracy: 0.8105 - val_loss: 0.1517 - val_accuracy: 0.7913\n",
      "Epoch 70/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1348 - accuracy: 0.8085 - val_loss: 0.1507 - val_accuracy: 0.7882\n",
      "Epoch 71/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8143 - val_loss: 0.1526 - val_accuracy: 0.7791\n",
      "Epoch 72/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8118 - val_loss: 0.1504 - val_accuracy: 0.7943\n",
      "Epoch 73/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1347 - accuracy: 0.8176 - val_loss: 0.1531 - val_accuracy: 0.7903\n",
      "Epoch 74/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1351 - accuracy: 0.8126 - val_loss: 0.1491 - val_accuracy: 0.7882\n",
      "Epoch 75/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8141 - val_loss: 0.1563 - val_accuracy: 0.7964\n",
      "Epoch 76/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1346 - accuracy: 0.8110 - val_loss: 0.1478 - val_accuracy: 0.7893\n",
      "Epoch 77/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8113 - val_loss: 0.1477 - val_accuracy: 0.7882\n",
      "Epoch 78/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.8121 - val_loss: 0.1469 - val_accuracy: 0.7903\n",
      "Epoch 79/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1341 - accuracy: 0.8148 - val_loss: 0.1512 - val_accuracy: 0.7852\n",
      "Epoch 80/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1347 - accuracy: 0.8131 - val_loss: 0.1479 - val_accuracy: 0.7872\n",
      "Epoch 81/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1349 - accuracy: 0.8159 - val_loss: 0.1477 - val_accuracy: 0.7913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.8121 - val_loss: 0.1473 - val_accuracy: 0.7953\n",
      "Epoch 83/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8176 - val_loss: 0.1498 - val_accuracy: 0.7842\n",
      "Epoch 84/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.8128 - val_loss: 0.1465 - val_accuracy: 0.7872\n",
      "Epoch 85/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1345 - accuracy: 0.8118 - val_loss: 0.1516 - val_accuracy: 0.7903\n",
      "Epoch 86/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8131 - val_loss: 0.1519 - val_accuracy: 0.7872\n",
      "Epoch 87/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1332 - accuracy: 0.8197 - val_loss: 0.1495 - val_accuracy: 0.7913\n",
      "Epoch 88/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8197 - val_loss: 0.1490 - val_accuracy: 0.7872\n",
      "Epoch 89/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1338 - accuracy: 0.8118 - val_loss: 0.1470 - val_accuracy: 0.7832\n",
      "Epoch 90/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1328 - accuracy: 0.8161 - val_loss: 0.1471 - val_accuracy: 0.7913\n",
      "Epoch 91/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1339 - accuracy: 0.8176 - val_loss: 0.1464 - val_accuracy: 0.7882\n",
      "Epoch 92/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.8159 - val_loss: 0.1504 - val_accuracy: 0.7872\n",
      "Epoch 93/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.8189 - val_loss: 0.1512 - val_accuracy: 0.7822\n",
      "Epoch 94/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1340 - accuracy: 0.8110 - val_loss: 0.1472 - val_accuracy: 0.7923\n",
      "Epoch 95/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1334 - accuracy: 0.8166 - val_loss: 0.1466 - val_accuracy: 0.7923\n",
      "Epoch 96/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8181 - val_loss: 0.1468 - val_accuracy: 0.7893\n",
      "Epoch 97/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8166 - val_loss: 0.1469 - val_accuracy: 0.7862\n",
      "Epoch 98/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8174 - val_loss: 0.1470 - val_accuracy: 0.7933\n",
      "Epoch 99/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1334 - accuracy: 0.8174 - val_loss: 0.1475 - val_accuracy: 0.7893\n",
      "Epoch 100/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1329 - accuracy: 0.8181 - val_loss: 0.1498 - val_accuracy: 0.7872\n",
      "Epoch 101/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.8166 - val_loss: 0.1478 - val_accuracy: 0.7893\n",
      "Epoch 102/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8181 - val_loss: 0.1488 - val_accuracy: 0.7923\n",
      "Epoch 103/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.8169 - val_loss: 0.1470 - val_accuracy: 0.7852\n",
      "Epoch 104/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1318 - accuracy: 0.8194 - val_loss: 0.1508 - val_accuracy: 0.7923\n",
      "Epoch 105/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8202 - val_loss: 0.1477 - val_accuracy: 0.7882\n",
      "Epoch 106/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8181 - val_loss: 0.1475 - val_accuracy: 0.7882\n",
      "Epoch 107/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8153 - val_loss: 0.1533 - val_accuracy: 0.7953\n",
      "Epoch 108/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8169 - val_loss: 0.1512 - val_accuracy: 0.7872\n",
      "Epoch 109/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8166 - val_loss: 0.1506 - val_accuracy: 0.7933\n",
      "Epoch 110/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8159 - val_loss: 0.1538 - val_accuracy: 0.7943\n",
      "Epoch 111/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8171 - val_loss: 0.1462 - val_accuracy: 0.7933\n",
      "Epoch 112/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8153 - val_loss: 0.1496 - val_accuracy: 0.7822\n",
      "Epoch 113/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1326 - accuracy: 0.8186 - val_loss: 0.1456 - val_accuracy: 0.7984\n",
      "Epoch 114/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1326 - accuracy: 0.8209 - val_loss: 0.1481 - val_accuracy: 0.7913\n",
      "Epoch 115/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8197 - val_loss: 0.1475 - val_accuracy: 0.7852\n",
      "Epoch 116/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1318 - accuracy: 0.8166 - val_loss: 0.1506 - val_accuracy: 0.7893\n",
      "Epoch 117/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8179 - val_loss: 0.1493 - val_accuracy: 0.7852\n",
      "Epoch 118/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1314 - accuracy: 0.8176 - val_loss: 0.1483 - val_accuracy: 0.7943\n",
      "Epoch 119/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.8159 - val_loss: 0.1494 - val_accuracy: 0.7832\n",
      "Epoch 120/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1318 - accuracy: 0.8133 - val_loss: 0.1456 - val_accuracy: 0.7923\n",
      "Epoch 121/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8181 - val_loss: 0.1462 - val_accuracy: 0.7842\n",
      "Epoch 122/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.8204 - val_loss: 0.1493 - val_accuracy: 0.7923\n",
      "Epoch 123/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8232 - val_loss: 0.1485 - val_accuracy: 0.7943\n",
      "Epoch 124/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8207 - val_loss: 0.1461 - val_accuracy: 0.7913\n",
      "Epoch 125/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8174 - val_loss: 0.1454 - val_accuracy: 0.7943\n",
      "Epoch 126/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8159 - val_loss: 0.1467 - val_accuracy: 0.7953\n",
      "Epoch 127/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.8174 - val_loss: 0.1511 - val_accuracy: 0.7964\n",
      "Epoch 128/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8186 - val_loss: 0.1475 - val_accuracy: 0.7893\n",
      "Epoch 129/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8186 - val_loss: 0.1479 - val_accuracy: 0.7893\n",
      "Epoch 130/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1296 - accuracy: 0.8202 - val_loss: 0.1467 - val_accuracy: 0.7882\n",
      "Epoch 131/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.8212 - val_loss: 0.1463 - val_accuracy: 0.7893\n",
      "Epoch 132/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8181 - val_loss: 0.1461 - val_accuracy: 0.7933\n",
      "Epoch 133/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.8179 - val_loss: 0.1453 - val_accuracy: 0.7933\n",
      "Epoch 134/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1311 - accuracy: 0.8191 - val_loss: 0.1435 - val_accuracy: 0.7953\n",
      "Epoch 135/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1308 - accuracy: 0.8207 - val_loss: 0.1483 - val_accuracy: 0.7852\n",
      "Epoch 136/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1300 - accuracy: 0.8219 - val_loss: 0.1453 - val_accuracy: 0.7862\n",
      "Epoch 137/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.8189 - val_loss: 0.1448 - val_accuracy: 0.7984\n",
      "Epoch 138/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1299 - accuracy: 0.8232 - val_loss: 0.1479 - val_accuracy: 0.7903\n",
      "Epoch 139/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1291 - accuracy: 0.8214 - val_loss: 0.1462 - val_accuracy: 0.7923\n",
      "Epoch 140/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1301 - accuracy: 0.8227 - val_loss: 0.1451 - val_accuracy: 0.7903\n",
      "Epoch 141/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.8199 - val_loss: 0.1477 - val_accuracy: 0.7893\n",
      "Epoch 142/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8229 - val_loss: 0.1468 - val_accuracy: 0.7893\n",
      "Epoch 143/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8224 - val_loss: 0.1514 - val_accuracy: 0.7923\n",
      "Epoch 144/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1308 - accuracy: 0.8179 - val_loss: 0.1455 - val_accuracy: 0.7872\n",
      "Epoch 145/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8240 - val_loss: 0.1465 - val_accuracy: 0.7943\n",
      "Epoch 146/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1299 - accuracy: 0.8171 - val_loss: 0.1450 - val_accuracy: 0.7822\n",
      "Epoch 147/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1292 - accuracy: 0.8255 - val_loss: 0.1517 - val_accuracy: 0.7903\n",
      "Epoch 148/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8194 - val_loss: 0.1458 - val_accuracy: 0.7872\n",
      "Epoch 149/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1296 - accuracy: 0.8204 - val_loss: 0.1447 - val_accuracy: 0.7943\n",
      "Epoch 150/150\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1295 - accuracy: 0.8227 - val_loss: 0.1451 - val_accuracy: 0.7913\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<keras.engine.functional.Functional object at 0x7fd7b06e4d90>' (type <class 'keras.engine.functional.Functional'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [139]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m repeats:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# evaluate using a given number of repeats\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# summarize\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m mean=\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m se=\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (r, mean(scores), sem(scores)))\n",
      "Input \u001b[0;32mIn [139]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(X, y, repeats)\u001b[0m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train2, y_train2, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# evaluate model\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/joblib/parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    830\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    831\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 833\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:268\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m--> 268\u001b[0m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/base.py:77\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             )\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     84\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m     85\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.functional.Functional object at 0x7fd7b06e4d90>' (type <class 'keras.engine.functional.Functional'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "\n",
    "# compare the number of repeats for repeated k-fold cross-validation\n",
    "from scipy.stats import sem\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, repeats):\n",
    "\t# prepare the cross-validation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    # create model\n",
    "    layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "    layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer17)\n",
    "    #model.layers[16].trainable = False\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train2, y_train2, batch_size=16, epochs=150, validation_split=0.20)\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1,verbose=0)\n",
    "    return scores\n",
    " \n",
    "# create dataset\n",
    "X, y = x_train2, y_train2\n",
    "# configurations to test\n",
    "repeats = range(1,5)\n",
    "results = list()\n",
    "for r in repeats:\n",
    "    # evaluate using a given number of repeats\n",
    "    scores = evaluate_model(X, y, r)\n",
    "    # summarize\n",
    "    print('>%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))\n",
    "    # store\n",
    "    results.append(scores)\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "89a723e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(filter1=1, filter2=1, filter3=1, filter4=1, filter5=1, stride1=1, stride2=1):#neurons1=1, neurons2=1):\n",
    "    layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "    layer17 = Dense(1, name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer17)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c257450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b1cd9ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 664, 1), dtype=tf.float32, name='input'), name='input', description=\"created by layer 'input'\") at layer \"Conv1D_1\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(SGD(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m##, momentum=0.9\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 34\u001b[0m \u001b[43mdefine_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36mdefine_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     layer17 \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m'\u001b[39m)(layer16)\n\u001b[0;32m---> 29\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer17\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(SGD(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m##, momentum=0.9\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/engine/functional.py:146\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[1;32m    144\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)]):\n\u001b[1;32m    145\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(inputs, outputs)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/engine/functional.py:229\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.10/lib/python3.10/site-packages/keras/engine/functional.py:1036\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[1;32m   1035\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[0;32m-> 1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for tensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. The following previous layers \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwere accessed without issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[1;32m   1041\u001b[0m   computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 664, 1), dtype=tf.float32, name='input'), name='input', description=\"created by layer 'input'\") at layer \"Conv1D_1\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "def define_model():\n",
    "    layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "#    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "    layer17 = Dense(1, activation='sigmoid', name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer17)\n",
    "    #model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "\n",
    "    return model\n",
    "define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9dcae5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.7691\n",
      "Epoch 1: val_loss improved from inf to 0.17629, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.1746 - accuracy: 0.7657 - val_loss: 0.1763 - val_accuracy: 0.7629\n",
      "Epoch 2/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.7686\n",
      "Epoch 2: val_loss improved from 0.17629 to 0.16177, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1624 - accuracy: 0.7708 - val_loss: 0.1618 - val_accuracy: 0.7619\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.7686\n",
      "Epoch 3: val_loss improved from 0.16177 to 0.16026, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1590 - accuracy: 0.7700 - val_loss: 0.1603 - val_accuracy: 0.7700\n",
      "Epoch 4/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1570 - accuracy: 0.7718\n",
      "Epoch 4: val_loss improved from 0.16026 to 0.15827, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1569 - accuracy: 0.7730 - val_loss: 0.1583 - val_accuracy: 0.7649\n",
      "Epoch 5/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1564 - accuracy: 0.7756\n",
      "Epoch 5: val_loss did not improve from 0.15827\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1554 - accuracy: 0.7771 - val_loss: 0.1597 - val_accuracy: 0.7660\n",
      "Epoch 6/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1542 - accuracy: 0.7773\n",
      "Epoch 6: val_loss did not improve from 0.15827\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1550 - accuracy: 0.7758 - val_loss: 0.1591 - val_accuracy: 0.7680\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.7840\n",
      "Epoch 7: val_loss improved from 0.15827 to 0.15577, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1537 - accuracy: 0.7824 - val_loss: 0.1558 - val_accuracy: 0.7771\n",
      "Epoch 8/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.7850\n",
      "Epoch 8: val_loss did not improve from 0.15577\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1519 - accuracy: 0.7844 - val_loss: 0.1567 - val_accuracy: 0.7771\n",
      "Epoch 9/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1527 - accuracy: 0.7870\n",
      "Epoch 9: val_loss did not improve from 0.15577\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1516 - accuracy: 0.7885 - val_loss: 0.1568 - val_accuracy: 0.7822\n",
      "Epoch 10/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1512 - accuracy: 0.7876\n",
      "Epoch 10: val_loss did not improve from 0.15577\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1512 - accuracy: 0.7872 - val_loss: 0.1561 - val_accuracy: 0.7741\n",
      "Epoch 11/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1514 - accuracy: 0.7831\n",
      "Epoch 11: val_loss improved from 0.15577 to 0.15381, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1509 - accuracy: 0.7844 - val_loss: 0.1538 - val_accuracy: 0.7882\n",
      "Epoch 12/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.7895\n",
      "Epoch 12: val_loss did not improve from 0.15381\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1483 - accuracy: 0.7910 - val_loss: 0.1553 - val_accuracy: 0.7801\n",
      "Epoch 13/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.7929\n",
      "Epoch 13: val_loss did not improve from 0.15381\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1486 - accuracy: 0.7908 - val_loss: 0.1553 - val_accuracy: 0.7832\n",
      "Epoch 14/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1481 - accuracy: 0.7991\n",
      "Epoch 14: val_loss did not improve from 0.15381\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.7994 - val_loss: 0.1543 - val_accuracy: 0.7872\n",
      "Epoch 15/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1456 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.15381\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1465 - accuracy: 0.7956 - val_loss: 0.1542 - val_accuracy: 0.7832\n",
      "Epoch 16/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.7989\n",
      "Epoch 16: val_loss improved from 0.15381 to 0.15298, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.7961 - val_loss: 0.1530 - val_accuracy: 0.7913\n",
      "Epoch 17/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.7963\n",
      "Epoch 17: val_loss improved from 0.15298 to 0.15281, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.7958 - val_loss: 0.1528 - val_accuracy: 0.7801\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.7976\n",
      "Epoch 18: val_loss did not improve from 0.15281\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.7971 - val_loss: 0.1546 - val_accuracy: 0.7791\n",
      "Epoch 19/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1465 - accuracy: 0.7943\n",
      "Epoch 19: val_loss improved from 0.15281 to 0.15161, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1457 - accuracy: 0.7953 - val_loss: 0.1516 - val_accuracy: 0.7872\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.7984\n",
      "Epoch 20: val_loss did not improve from 0.15161\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1442 - accuracy: 0.7979 - val_loss: 0.1562 - val_accuracy: 0.7812\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.7924\n",
      "Epoch 21: val_loss did not improve from 0.15161\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.7915 - val_loss: 0.1529 - val_accuracy: 0.7913\n",
      "Epoch 22/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.7986\n",
      "Epoch 22: val_loss did not improve from 0.15161\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.7991 - val_loss: 0.1535 - val_accuracy: 0.7822\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.8028\n",
      "Epoch 23: val_loss improved from 0.15161 to 0.15115, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1431 - accuracy: 0.8004 - val_loss: 0.1511 - val_accuracy: 0.7913\n",
      "Epoch 24/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1425 - accuracy: 0.8036\n",
      "Epoch 24: val_loss did not improve from 0.15115\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.8019 - val_loss: 0.1539 - val_accuracy: 0.7741\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.8047\n",
      "Epoch 25: val_loss improved from 0.15115 to 0.15078, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.8040 - val_loss: 0.1508 - val_accuracy: 0.7842\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.7958\n",
      "Epoch 26: val_loss improved from 0.15078 to 0.15028, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1431 - accuracy: 0.7958 - val_loss: 0.1503 - val_accuracy: 0.7974\n",
      "Epoch 27/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.7947\n",
      "Epoch 27: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7958 - val_loss: 0.1534 - val_accuracy: 0.7791\n",
      "Epoch 28/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1427 - accuracy: 0.8007\n",
      "Epoch 28: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.7999 - val_loss: 0.1512 - val_accuracy: 0.7852\n",
      "Epoch 29/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1416 - accuracy: 0.8036\n",
      "Epoch 29: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.8029 - val_loss: 0.1530 - val_accuracy: 0.7781\n",
      "Epoch 30/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.7988\n",
      "Epoch 30: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.7986 - val_loss: 0.1509 - val_accuracy: 0.7903\n",
      "Epoch 31/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1413 - accuracy: 0.8046\n",
      "Epoch 31: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.8062 - val_loss: 0.1514 - val_accuracy: 0.7933\n",
      "Epoch 32/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.8041\n",
      "Epoch 32: val_loss did not improve from 0.15028\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8057 - val_loss: 0.1545 - val_accuracy: 0.7862\n",
      "Epoch 33/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1405 - accuracy: 0.8002\n",
      "Epoch 33: val_loss improved from 0.15028 to 0.14884, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8017 - val_loss: 0.1488 - val_accuracy: 0.7741\n",
      "Epoch 34/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1405 - accuracy: 0.8067\n",
      "Epoch 34: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8072 - val_loss: 0.1495 - val_accuracy: 0.7933\n",
      "Epoch 35/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8018\n",
      "Epoch 35: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.8019 - val_loss: 0.1503 - val_accuracy: 0.7923\n",
      "Epoch 36/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1392 - accuracy: 0.8032\n",
      "Epoch 36: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8032 - val_loss: 0.1521 - val_accuracy: 0.7893\n",
      "Epoch 37/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.8072\n",
      "Epoch 37: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.8057 - val_loss: 0.1517 - val_accuracy: 0.7923\n",
      "Epoch 38/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.8033\n",
      "Epoch 38: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8052 - val_loss: 0.1497 - val_accuracy: 0.7812\n",
      "Epoch 39/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1393 - accuracy: 0.8038\n",
      "Epoch 39: val_loss improved from 0.14884 to 0.14870, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8042 - val_loss: 0.1487 - val_accuracy: 0.7882\n",
      "Epoch 40/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.8067\n",
      "Epoch 40: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.8055 - val_loss: 0.1494 - val_accuracy: 0.7923\n",
      "Epoch 41/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1397 - accuracy: 0.8009\n",
      "Epoch 41: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8032 - val_loss: 0.1507 - val_accuracy: 0.7882\n",
      "Epoch 42/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.8083\n",
      "Epoch 42: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8070 - val_loss: 0.1513 - val_accuracy: 0.7882\n",
      "Epoch 43/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8075\n",
      "Epoch 43: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8093 - val_loss: 0.1515 - val_accuracy: 0.7882\n",
      "Epoch 44/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8075\n",
      "Epoch 44: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8083 - val_loss: 0.1546 - val_accuracy: 0.7822\n",
      "Epoch 45/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.8114\n",
      "Epoch 45: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8095 - val_loss: 0.1509 - val_accuracy: 0.7882\n",
      "Epoch 46/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1376 - accuracy: 0.8054\n",
      "Epoch 46: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8047 - val_loss: 0.1493 - val_accuracy: 0.7872\n",
      "Epoch 47/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.8109\n",
      "Epoch 47: val_loss did not improve from 0.14870\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.8095 - val_loss: 0.1505 - val_accuracy: 0.7852\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.8086\n",
      "Epoch 48: val_loss improved from 0.14870 to 0.14793, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8083 - val_loss: 0.1479 - val_accuracy: 0.7974\n",
      "Epoch 49/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.8130\n",
      "Epoch 49: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8128 - val_loss: 0.1507 - val_accuracy: 0.7832\n",
      "Epoch 50/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1392 - accuracy: 0.8093\n",
      "Epoch 50: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8090 - val_loss: 0.1491 - val_accuracy: 0.7882\n",
      "Epoch 51/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8062\n",
      "Epoch 51: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8067 - val_loss: 0.1505 - val_accuracy: 0.7812\n",
      "Epoch 52/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1377 - accuracy: 0.8099\n",
      "Epoch 52: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8103 - val_loss: 0.1498 - val_accuracy: 0.7862\n",
      "Epoch 53/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.8114\n",
      "Epoch 53: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8103 - val_loss: 0.1480 - val_accuracy: 0.7822\n",
      "Epoch 54/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8128\n",
      "Epoch 54: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8110 - val_loss: 0.1489 - val_accuracy: 0.7852\n",
      "Epoch 55/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.8130\n",
      "Epoch 55: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8123 - val_loss: 0.1485 - val_accuracy: 0.7882\n",
      "Epoch 56/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8080\n",
      "Epoch 56: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8070 - val_loss: 0.1496 - val_accuracy: 0.7832\n",
      "Epoch 57/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.8059\n",
      "Epoch 57: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8055 - val_loss: 0.1502 - val_accuracy: 0.7842\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8099\n",
      "Epoch 58: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8100 - val_loss: 0.1506 - val_accuracy: 0.7923\n",
      "Epoch 59/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.8094\n",
      "Epoch 59: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8080 - val_loss: 0.1495 - val_accuracy: 0.7872\n",
      "Epoch 60/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8141\n",
      "Epoch 60: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8133 - val_loss: 0.1488 - val_accuracy: 0.7933\n",
      "Epoch 61/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8138\n",
      "Epoch 61: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8131 - val_loss: 0.1483 - val_accuracy: 0.7852\n",
      "Epoch 62/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.8130\n",
      "Epoch 62: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8136 - val_loss: 0.1484 - val_accuracy: 0.7852\n",
      "Epoch 63/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.8101\n",
      "Epoch 63: val_loss did not improve from 0.14793\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.8113 - val_loss: 0.1495 - val_accuracy: 0.7842\n",
      "Epoch 64/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8125\n",
      "Epoch 64: val_loss improved from 0.14793 to 0.14773, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8121 - val_loss: 0.1477 - val_accuracy: 0.7862\n",
      "Epoch 65/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8128\n",
      "Epoch 65: val_loss improved from 0.14773 to 0.14716, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8136 - val_loss: 0.1472 - val_accuracy: 0.7903\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.8122\n",
      "Epoch 66: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8110 - val_loss: 0.1479 - val_accuracy: 0.7964\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8162\n",
      "Epoch 67: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8161 - val_loss: 0.1518 - val_accuracy: 0.7842\n",
      "Epoch 68/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1367 - accuracy: 0.8128\n",
      "Epoch 68: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8153 - val_loss: 0.1501 - val_accuracy: 0.7872\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.8122\n",
      "Epoch 69: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1350 - accuracy: 0.8105 - val_loss: 0.1490 - val_accuracy: 0.7832\n",
      "Epoch 70/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.8096\n",
      "Epoch 70: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8095 - val_loss: 0.1490 - val_accuracy: 0.7801\n",
      "Epoch 71/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1344 - accuracy: 0.8138\n",
      "Epoch 71: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8123 - val_loss: 0.1477 - val_accuracy: 0.7893\n",
      "Epoch 72/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1366 - accuracy: 0.8114\n",
      "Epoch 72: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8126 - val_loss: 0.1481 - val_accuracy: 0.7903\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.8201\n",
      "Epoch 73: val_loss did not improve from 0.14716\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1345 - accuracy: 0.8194 - val_loss: 0.1602 - val_accuracy: 0.7720\n",
      "Epoch 74/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.8146\n",
      "Epoch 74: val_loss improved from 0.14716 to 0.14692, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8153 - val_loss: 0.1469 - val_accuracy: 0.7882\n",
      "Epoch 75/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1345 - accuracy: 0.8165\n",
      "Epoch 75: val_loss did not improve from 0.14692\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1350 - accuracy: 0.8148 - val_loss: 0.1505 - val_accuracy: 0.7862\n",
      "Epoch 76/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.8167\n",
      "Epoch 76: val_loss did not improve from 0.14692\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1346 - accuracy: 0.8143 - val_loss: 0.1532 - val_accuracy: 0.7741\n",
      "Epoch 77/150\n",
      "244/247 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.8192\n",
      "Epoch 77: val_loss did not improve from 0.14692\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1346 - accuracy: 0.8181 - val_loss: 0.1472 - val_accuracy: 0.7923\n",
      "Epoch 78/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1350 - accuracy: 0.8154\n",
      "Epoch 78: val_loss did not improve from 0.14692\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1353 - accuracy: 0.8148 - val_loss: 0.1485 - val_accuracy: 0.7832\n",
      "Epoch 79/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.8130\n",
      "Epoch 79: val_loss improved from 0.14692 to 0.14662, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1349 - accuracy: 0.8148 - val_loss: 0.1466 - val_accuracy: 0.7862\n",
      "Epoch 80/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.8133\n",
      "Epoch 80: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1349 - accuracy: 0.8148 - val_loss: 0.1503 - val_accuracy: 0.7852\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.8151\n",
      "Epoch 81: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1351 - accuracy: 0.8146 - val_loss: 0.1490 - val_accuracy: 0.7893\n",
      "Epoch 82/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.8164\n",
      "Epoch 82: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1341 - accuracy: 0.8159 - val_loss: 0.1489 - val_accuracy: 0.7893\n",
      "Epoch 83/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.8146\n",
      "Epoch 83: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1334 - accuracy: 0.8153 - val_loss: 0.1488 - val_accuracy: 0.7862\n",
      "Epoch 84/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1354 - accuracy: 0.8165\n",
      "Epoch 84: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1349 - accuracy: 0.8174 - val_loss: 0.1489 - val_accuracy: 0.7903\n",
      "Epoch 85/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.8156\n",
      "Epoch 85: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.8138 - val_loss: 0.1491 - val_accuracy: 0.7872\n",
      "Epoch 86/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.8172\n",
      "Epoch 86: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1339 - accuracy: 0.8174 - val_loss: 0.1502 - val_accuracy: 0.7801\n",
      "Epoch 87/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.8170\n",
      "Epoch 87: val_loss did not improve from 0.14662\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1344 - accuracy: 0.8164 - val_loss: 0.1482 - val_accuracy: 0.7893\n",
      "Epoch 88/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.8188\n",
      "Epoch 88: val_loss improved from 0.14662 to 0.14580, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8194 - val_loss: 0.1458 - val_accuracy: 0.7943\n",
      "Epoch 89/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.8146\n",
      "Epoch 89: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8151 - val_loss: 0.1473 - val_accuracy: 0.7913\n",
      "Epoch 90/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.8230\n",
      "Epoch 90: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8219 - val_loss: 0.1465 - val_accuracy: 0.7933\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.8243\n",
      "Epoch 91: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.8214 - val_loss: 0.1509 - val_accuracy: 0.7872\n",
      "Epoch 92/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.8162\n",
      "Epoch 92: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1339 - accuracy: 0.8171 - val_loss: 0.1488 - val_accuracy: 0.7862\n",
      "Epoch 93/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.8162\n",
      "Epoch 93: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8169 - val_loss: 0.1465 - val_accuracy: 0.7923\n",
      "Epoch 94/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1322 - accuracy: 0.8233\n",
      "Epoch 94: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8235 - val_loss: 0.1483 - val_accuracy: 0.7862\n",
      "Epoch 95/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1330 - accuracy: 0.8162\n",
      "Epoch 95: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8153 - val_loss: 0.1477 - val_accuracy: 0.7842\n",
      "Epoch 96/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.8109\n",
      "Epoch 96: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1327 - accuracy: 0.8138 - val_loss: 0.1533 - val_accuracy: 0.7791\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.8201\n",
      "Epoch 97: val_loss did not improve from 0.14580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1334 - accuracy: 0.8171 - val_loss: 0.1481 - val_accuracy: 0.7812\n",
      "Epoch 98/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.8236\n",
      "Epoch 98: val_loss improved from 0.14580 to 0.14570, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1323 - accuracy: 0.8212 - val_loss: 0.1457 - val_accuracy: 0.7923\n",
      "Epoch 99/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.8146\n",
      "Epoch 99: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.8143 - val_loss: 0.1487 - val_accuracy: 0.7801\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.8164\n",
      "Epoch 100: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8179 - val_loss: 0.1465 - val_accuracy: 0.7862\n",
      "Epoch 101/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1331 - accuracy: 0.8204\n",
      "Epoch 101: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.8212 - val_loss: 0.1478 - val_accuracy: 0.7933\n",
      "Epoch 102/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.8198\n",
      "Epoch 102: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1331 - accuracy: 0.8212 - val_loss: 0.1481 - val_accuracy: 0.7923\n",
      "Epoch 103/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1338 - accuracy: 0.8151\n",
      "Epoch 103: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1329 - accuracy: 0.8164 - val_loss: 0.1465 - val_accuracy: 0.7893\n",
      "Epoch 104/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.8217\n",
      "Epoch 104: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1323 - accuracy: 0.8197 - val_loss: 0.1482 - val_accuracy: 0.7852\n",
      "Epoch 105/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.8196\n",
      "Epoch 105: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.8207 - val_loss: 0.1547 - val_accuracy: 0.7670\n",
      "Epoch 106/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.8217\n",
      "Epoch 106: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.8209 - val_loss: 0.1493 - val_accuracy: 0.7832\n",
      "Epoch 107/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.8226\n",
      "Epoch 107: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.8214 - val_loss: 0.1464 - val_accuracy: 0.7872\n",
      "Epoch 108/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.8217\n",
      "Epoch 108: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.8202 - val_loss: 0.1510 - val_accuracy: 0.7801\n",
      "Epoch 109/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1327 - accuracy: 0.8178\n",
      "Epoch 109: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1323 - accuracy: 0.8191 - val_loss: 0.1480 - val_accuracy: 0.7974\n",
      "Epoch 110/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.8259\n",
      "Epoch 110: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.8229 - val_loss: 0.1479 - val_accuracy: 0.7882\n",
      "Epoch 111/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.8193\n",
      "Epoch 111: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.8181 - val_loss: 0.1466 - val_accuracy: 0.7903\n",
      "Epoch 112/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.8229\n",
      "Epoch 112: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8209 - val_loss: 0.1465 - val_accuracy: 0.7882\n",
      "Epoch 113/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1333 - accuracy: 0.8175\n",
      "Epoch 113: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1327 - accuracy: 0.8179 - val_loss: 0.1478 - val_accuracy: 0.7903\n",
      "Epoch 114/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1327 - accuracy: 0.8178\n",
      "Epoch 114: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.8197 - val_loss: 0.1464 - val_accuracy: 0.7943\n",
      "Epoch 115/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1313 - accuracy: 0.8207\n",
      "Epoch 115: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8217 - val_loss: 0.1486 - val_accuracy: 0.7933\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1324 - accuracy: 0.8199\n",
      "Epoch 116: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8212 - val_loss: 0.1496 - val_accuracy: 0.7812\n",
      "Epoch 117/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.8191\n",
      "Epoch 117: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1328 - accuracy: 0.8174 - val_loss: 0.1468 - val_accuracy: 0.7964\n",
      "Epoch 118/150\n",
      "244/247 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8227\n",
      "Epoch 118: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.8229 - val_loss: 0.1464 - val_accuracy: 0.7842\n",
      "Epoch 119/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1327 - accuracy: 0.8186\n",
      "Epoch 119: val_loss improved from 0.14570 to 0.14569, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8197 - val_loss: 0.1457 - val_accuracy: 0.7943\n",
      "Epoch 120/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1317 - accuracy: 0.8204\n",
      "Epoch 120: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1317 - accuracy: 0.8199 - val_loss: 0.1467 - val_accuracy: 0.7832\n",
      "Epoch 121/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1323 - accuracy: 0.8186\n",
      "Epoch 121: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8189 - val_loss: 0.1465 - val_accuracy: 0.7822\n",
      "Epoch 122/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1321 - accuracy: 0.8222\n",
      "Epoch 122: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8222 - val_loss: 0.1477 - val_accuracy: 0.7903\n",
      "Epoch 123/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1321 - accuracy: 0.8210\n",
      "Epoch 123: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1318 - accuracy: 0.8207 - val_loss: 0.1461 - val_accuracy: 0.7822\n",
      "Epoch 124/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1323 - accuracy: 0.8215\n",
      "Epoch 124: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1318 - accuracy: 0.8224 - val_loss: 0.1477 - val_accuracy: 0.7872\n",
      "Epoch 125/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.8197\n",
      "Epoch 125: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.8209 - val_loss: 0.1461 - val_accuracy: 0.7872\n",
      "Epoch 126/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1298 - accuracy: 0.8252\n",
      "Epoch 126: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1309 - accuracy: 0.8232 - val_loss: 0.1499 - val_accuracy: 0.7832\n",
      "Epoch 127/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.8206\n",
      "Epoch 127: val_loss did not improve from 0.14569\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1319 - accuracy: 0.8219 - val_loss: 0.1468 - val_accuracy: 0.7842\n",
      "Epoch 128/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.8191\n",
      "Epoch 128: val_loss improved from 0.14569 to 0.14542, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.8194 - val_loss: 0.1454 - val_accuracy: 0.7903\n",
      "Epoch 129/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1313 - accuracy: 0.8241\n",
      "Epoch 129: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.8242 - val_loss: 0.1470 - val_accuracy: 0.7822\n",
      "Epoch 130/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.8196\n",
      "Epoch 130: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8207 - val_loss: 0.1475 - val_accuracy: 0.7741\n",
      "Epoch 131/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1316 - accuracy: 0.8217\n",
      "Epoch 131: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1315 - accuracy: 0.8219 - val_loss: 0.1456 - val_accuracy: 0.7943\n",
      "Epoch 132/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.8217\n",
      "Epoch 132: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8217 - val_loss: 0.1478 - val_accuracy: 0.7842\n",
      "Epoch 133/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1317 - accuracy: 0.8197\n",
      "Epoch 133: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8207 - val_loss: 0.1458 - val_accuracy: 0.7852\n",
      "Epoch 134/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.8243\n",
      "Epoch 134: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.8247 - val_loss: 0.1460 - val_accuracy: 0.7872\n",
      "Epoch 135/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.8237\n",
      "Epoch 135: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8240 - val_loss: 0.1470 - val_accuracy: 0.7832\n",
      "Epoch 136/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.8188\n",
      "Epoch 136: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8194 - val_loss: 0.1457 - val_accuracy: 0.7842\n",
      "Epoch 137/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1300 - accuracy: 0.8242\n",
      "Epoch 137: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8240 - val_loss: 0.1481 - val_accuracy: 0.7812\n",
      "Epoch 138/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.8217\n",
      "Epoch 138: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8222 - val_loss: 0.1477 - val_accuracy: 0.7872\n",
      "Epoch 139/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.8256\n",
      "Epoch 139: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.8255 - val_loss: 0.1468 - val_accuracy: 0.7882\n",
      "Epoch 140/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1301 - accuracy: 0.8246\n",
      "Epoch 140: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1302 - accuracy: 0.8240 - val_loss: 0.1476 - val_accuracy: 0.7852\n",
      "Epoch 141/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.8258\n",
      "Epoch 141: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.8265 - val_loss: 0.1469 - val_accuracy: 0.7842\n",
      "Epoch 142/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.8249\n",
      "Epoch 142: val_loss did not improve from 0.14542\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1299 - accuracy: 0.8252 - val_loss: 0.1496 - val_accuracy: 0.7882\n",
      "Epoch 143/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1317 - accuracy: 0.8217\n",
      "Epoch 143: val_loss improved from 0.14542 to 0.14415, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1309 - accuracy: 0.8224 - val_loss: 0.1442 - val_accuracy: 0.7923\n",
      "Epoch 144/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.8270\n",
      "Epoch 144: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1291 - accuracy: 0.8260 - val_loss: 0.1463 - val_accuracy: 0.7862\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.8207\n",
      "Epoch 145: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8207 - val_loss: 0.1481 - val_accuracy: 0.7852\n",
      "Epoch 146/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1316 - accuracy: 0.8204\n",
      "Epoch 146: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8227 - val_loss: 0.1464 - val_accuracy: 0.7882\n",
      "Epoch 147/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.8214\n",
      "Epoch 147: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1296 - accuracy: 0.8214 - val_loss: 0.1488 - val_accuracy: 0.7832\n",
      "Epoch 148/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.8218\n",
      "Epoch 148: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.8232 - val_loss: 0.1482 - val_accuracy: 0.7842\n",
      "Epoch 149/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.8239\n",
      "Epoch 149: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1298 - accuracy: 0.8235 - val_loss: 0.1460 - val_accuracy: 0.7842\n",
      "Epoch 150/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1288 - accuracy: 0.8267\n",
      "Epoch 150: val_loss did not improve from 0.14415\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1295 - accuracy: 0.8245 - val_loss: 0.1490 - val_accuracy: 0.7832\n",
      "> 83.181\n",
      "Epoch 1/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1761 - accuracy: 0.7596\n",
      "Epoch 1: val_loss improved from inf to 0.17039, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.1750 - accuracy: 0.7617 - val_loss: 0.1704 - val_accuracy: 0.7599\n",
      "Epoch 2/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.7678\n",
      "Epoch 2: val_loss improved from 0.17039 to 0.16067, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1621 - accuracy: 0.7693 - val_loss: 0.1607 - val_accuracy: 0.7649\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.7722\n",
      "Epoch 3: val_loss did not improve from 0.16067\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1590 - accuracy: 0.7718 - val_loss: 0.1608 - val_accuracy: 0.7700\n",
      "Epoch 4/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.7746\n",
      "Epoch 4: val_loss improved from 0.16067 to 0.15889, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1576 - accuracy: 0.7746 - val_loss: 0.1589 - val_accuracy: 0.7670\n",
      "Epoch 5/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.7780\n",
      "Epoch 5: val_loss improved from 0.15889 to 0.15833, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1552 - accuracy: 0.7766 - val_loss: 0.1583 - val_accuracy: 0.7801\n",
      "Epoch 6/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.7777\n",
      "Epoch 6: val_loss improved from 0.15833 to 0.15641, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1543 - accuracy: 0.7804 - val_loss: 0.1564 - val_accuracy: 0.7720\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.7793\n",
      "Epoch 7: val_loss improved from 0.15641 to 0.15552, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1531 - accuracy: 0.7791 - val_loss: 0.1555 - val_accuracy: 0.7791\n",
      "Epoch 8/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.7848\n",
      "Epoch 8: val_loss improved from 0.15552 to 0.15451, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1522 - accuracy: 0.7847 - val_loss: 0.1545 - val_accuracy: 0.7730\n",
      "Epoch 9/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.7816\n",
      "Epoch 9: val_loss did not improve from 0.15451\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1515 - accuracy: 0.7829 - val_loss: 0.1553 - val_accuracy: 0.7822\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.7908\n",
      "Epoch 10: val_loss did not improve from 0.15451\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1502 - accuracy: 0.7888 - val_loss: 0.1554 - val_accuracy: 0.7801\n",
      "Epoch 11/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.7897\n",
      "Epoch 11: val_loss did not improve from 0.15451\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1500 - accuracy: 0.7893 - val_loss: 0.1553 - val_accuracy: 0.7781\n",
      "Epoch 12/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.7856\n",
      "Epoch 12: val_loss did not improve from 0.15451\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1487 - accuracy: 0.7890 - val_loss: 0.1563 - val_accuracy: 0.7822\n",
      "Epoch 13/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.7916\n",
      "Epoch 13: val_loss did not improve from 0.15451\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1475 - accuracy: 0.7923 - val_loss: 0.1556 - val_accuracy: 0.7832\n",
      "Epoch 14/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.7895\n",
      "Epoch 14: val_loss improved from 0.15451 to 0.15191, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1480 - accuracy: 0.7893 - val_loss: 0.1519 - val_accuracy: 0.7842\n",
      "Epoch 15/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.7971\n",
      "Epoch 15: val_loss did not improve from 0.15191\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1471 - accuracy: 0.7958 - val_loss: 0.1560 - val_accuracy: 0.7771\n",
      "Epoch 16/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.7929\n",
      "Epoch 16: val_loss did not improve from 0.15191\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1469 - accuracy: 0.7931 - val_loss: 0.1552 - val_accuracy: 0.7882\n",
      "Epoch 17/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.7981\n",
      "Epoch 17: val_loss did not improve from 0.15191\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1458 - accuracy: 0.7974 - val_loss: 0.1530 - val_accuracy: 0.7852\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.7979\n",
      "Epoch 18: val_loss did not improve from 0.15191\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1458 - accuracy: 0.7986 - val_loss: 0.1537 - val_accuracy: 0.7862\n",
      "Epoch 19/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.7968\n",
      "Epoch 19: val_loss improved from 0.15191 to 0.15035, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1458 - accuracy: 0.7979 - val_loss: 0.1504 - val_accuracy: 0.7842\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.7931\n",
      "Epoch 20: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.7938 - val_loss: 0.1545 - val_accuracy: 0.7872\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.7931\n",
      "Epoch 21: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.7943 - val_loss: 0.1509 - val_accuracy: 0.7882\n",
      "Epoch 22/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.8013\n",
      "Epoch 22: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.8009 - val_loss: 0.1534 - val_accuracy: 0.7832\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.8005\n",
      "Epoch 23: val_loss did not improve from 0.15035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.8012 - val_loss: 0.1517 - val_accuracy: 0.7852\n",
      "Epoch 24/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.7979\n",
      "Epoch 24: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7976 - val_loss: 0.1522 - val_accuracy: 0.7791\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.8005\n",
      "Epoch 25: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.8017 - val_loss: 0.1509 - val_accuracy: 0.7862\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.8007\n",
      "Epoch 26: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.8014 - val_loss: 0.1532 - val_accuracy: 0.7761\n",
      "Epoch 27/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.8010\n",
      "Epoch 27: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.8027 - val_loss: 0.1519 - val_accuracy: 0.7872\n",
      "Epoch 28/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1413 - accuracy: 0.8036\n",
      "Epoch 28: val_loss did not improve from 0.15035\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.8034 - val_loss: 0.1523 - val_accuracy: 0.7791\n",
      "Epoch 29/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8007\n",
      "Epoch 29: val_loss improved from 0.15035 to 0.14947, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.7994 - val_loss: 0.1495 - val_accuracy: 0.7812\n",
      "Epoch 30/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.8036\n",
      "Epoch 30: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1421 - accuracy: 0.8034 - val_loss: 0.1531 - val_accuracy: 0.7781\n",
      "Epoch 31/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.8044\n",
      "Epoch 31: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.8037 - val_loss: 0.1501 - val_accuracy: 0.7872\n",
      "Epoch 32/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.7999\n",
      "Epoch 32: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.8004 - val_loss: 0.1498 - val_accuracy: 0.7832\n",
      "Epoch 33/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8047\n",
      "Epoch 33: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.8042 - val_loss: 0.1599 - val_accuracy: 0.7791\n",
      "Epoch 34/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.8007\n",
      "Epoch 34: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.8009 - val_loss: 0.1502 - val_accuracy: 0.7822\n",
      "Epoch 35/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.8026\n",
      "Epoch 35: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.8032 - val_loss: 0.1501 - val_accuracy: 0.7872\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8081\n",
      "Epoch 36: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.8062 - val_loss: 0.1515 - val_accuracy: 0.7822\n",
      "Epoch 37/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8049\n",
      "Epoch 37: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8045 - val_loss: 0.1511 - val_accuracy: 0.7791\n",
      "Epoch 38/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.7973\n",
      "Epoch 38: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.7981 - val_loss: 0.1524 - val_accuracy: 0.7801\n",
      "Epoch 39/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1406 - accuracy: 0.8072\n",
      "Epoch 39: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.8075 - val_loss: 0.1517 - val_accuracy: 0.7832\n",
      "Epoch 40/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8047\n",
      "Epoch 40: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.8047 - val_loss: 0.1521 - val_accuracy: 0.7812\n",
      "Epoch 41/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.8047\n",
      "Epoch 41: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.8045 - val_loss: 0.1501 - val_accuracy: 0.7862\n",
      "Epoch 42/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8033\n",
      "Epoch 42: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8024 - val_loss: 0.1519 - val_accuracy: 0.7751\n",
      "Epoch 43/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8073\n",
      "Epoch 43: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8078 - val_loss: 0.1505 - val_accuracy: 0.7903\n",
      "Epoch 44/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8033\n",
      "Epoch 44: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8037 - val_loss: 0.1516 - val_accuracy: 0.7822\n",
      "Epoch 45/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1386 - accuracy: 0.8080\n",
      "Epoch 45: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8080 - val_loss: 0.1513 - val_accuracy: 0.7791\n",
      "Epoch 46/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8033\n",
      "Epoch 46: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.8027 - val_loss: 0.1499 - val_accuracy: 0.7812\n",
      "Epoch 47/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8120\n",
      "Epoch 47: val_loss did not improve from 0.14947\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8078 - val_loss: 0.1586 - val_accuracy: 0.7710\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8060\n",
      "Epoch 48: val_loss improved from 0.14947 to 0.14896, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8075 - val_loss: 0.1490 - val_accuracy: 0.7862\n",
      "Epoch 49/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.8057\n",
      "Epoch 49: val_loss improved from 0.14896 to 0.14714, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8062 - val_loss: 0.1471 - val_accuracy: 0.7832\n",
      "Epoch 50/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8065\n",
      "Epoch 50: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8070 - val_loss: 0.1517 - val_accuracy: 0.7852\n",
      "Epoch 51/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8057\n",
      "Epoch 51: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.8065 - val_loss: 0.1496 - val_accuracy: 0.7801\n",
      "Epoch 52/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8052\n",
      "Epoch 52: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8057 - val_loss: 0.1497 - val_accuracy: 0.7842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.8107\n",
      "Epoch 53: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8121 - val_loss: 0.1519 - val_accuracy: 0.7872\n",
      "Epoch 54/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.8109\n",
      "Epoch 54: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1379 - accuracy: 0.8118 - val_loss: 0.1542 - val_accuracy: 0.7761\n",
      "Epoch 55/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8109\n",
      "Epoch 55: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8113 - val_loss: 0.1511 - val_accuracy: 0.7771\n",
      "Epoch 56/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.8117\n",
      "Epoch 56: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8103 - val_loss: 0.1492 - val_accuracy: 0.7822\n",
      "Epoch 57/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8130\n",
      "Epoch 57: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8113 - val_loss: 0.1496 - val_accuracy: 0.7812\n",
      "Epoch 58/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.8117\n",
      "Epoch 58: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8118 - val_loss: 0.1489 - val_accuracy: 0.7862\n",
      "Epoch 59/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8125\n",
      "Epoch 59: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8121 - val_loss: 0.1496 - val_accuracy: 0.7791\n",
      "Epoch 60/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.8070\n",
      "Epoch 60: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8085 - val_loss: 0.1526 - val_accuracy: 0.7741\n",
      "Epoch 61/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.8125\n",
      "Epoch 61: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8116 - val_loss: 0.1482 - val_accuracy: 0.7822\n",
      "Epoch 62/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8091\n",
      "Epoch 62: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8083 - val_loss: 0.1482 - val_accuracy: 0.7923\n",
      "Epoch 63/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.8146\n",
      "Epoch 63: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8123 - val_loss: 0.1505 - val_accuracy: 0.7832\n",
      "Epoch 64/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8094\n",
      "Epoch 64: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8098 - val_loss: 0.1492 - val_accuracy: 0.7852\n",
      "Epoch 65/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.8104\n",
      "Epoch 65: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8108 - val_loss: 0.1491 - val_accuracy: 0.7882\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.8115\n",
      "Epoch 66: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8121 - val_loss: 0.1527 - val_accuracy: 0.7730\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8120\n",
      "Epoch 67: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8141 - val_loss: 0.1486 - val_accuracy: 0.7872\n",
      "Epoch 68/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.8091\n",
      "Epoch 68: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8098 - val_loss: 0.1508 - val_accuracy: 0.7751\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8143\n",
      "Epoch 69: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8146 - val_loss: 0.1519 - val_accuracy: 0.7842\n",
      "Epoch 70/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8149\n",
      "Epoch 70: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8146 - val_loss: 0.1491 - val_accuracy: 0.7862\n",
      "Epoch 71/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.8101\n",
      "Epoch 71: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1353 - accuracy: 0.8105 - val_loss: 0.1495 - val_accuracy: 0.7832\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8109\n",
      "Epoch 72: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8126 - val_loss: 0.1498 - val_accuracy: 0.7791\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8141\n",
      "Epoch 73: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8148 - val_loss: 0.1478 - val_accuracy: 0.7812\n",
      "Epoch 74/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.8141\n",
      "Epoch 74: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8141 - val_loss: 0.1485 - val_accuracy: 0.7882\n",
      "Epoch 75/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8120\n",
      "Epoch 75: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8128 - val_loss: 0.1503 - val_accuracy: 0.7801\n",
      "Epoch 76/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.8151\n",
      "Epoch 76: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.8166 - val_loss: 0.1480 - val_accuracy: 0.7832\n",
      "Epoch 77/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.8154\n",
      "Epoch 77: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1349 - accuracy: 0.8153 - val_loss: 0.1483 - val_accuracy: 0.7862\n",
      "Epoch 78/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.8091\n",
      "Epoch 78: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8103 - val_loss: 0.1481 - val_accuracy: 0.7812\n",
      "Epoch 79/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1351 - accuracy: 0.8128\n",
      "Epoch 79: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8121 - val_loss: 0.1511 - val_accuracy: 0.7791\n",
      "Epoch 80/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.8125\n",
      "Epoch 80: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8136 - val_loss: 0.1496 - val_accuracy: 0.7842\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.8180\n",
      "Epoch 81: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1341 - accuracy: 0.8171 - val_loss: 0.1497 - val_accuracy: 0.7882\n",
      "Epoch 82/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.8172\n",
      "Epoch 82: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1347 - accuracy: 0.8169 - val_loss: 0.1499 - val_accuracy: 0.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.8162\n",
      "Epoch 83: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.8171 - val_loss: 0.1492 - val_accuracy: 0.7771\n",
      "Epoch 84/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.8133\n",
      "Epoch 84: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1350 - accuracy: 0.8128 - val_loss: 0.1482 - val_accuracy: 0.7913\n",
      "Epoch 85/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.8167\n",
      "Epoch 85: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1344 - accuracy: 0.8166 - val_loss: 0.1487 - val_accuracy: 0.7832\n",
      "Epoch 86/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1334 - accuracy: 0.8177\n",
      "Epoch 86: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1339 - accuracy: 0.8171 - val_loss: 0.1490 - val_accuracy: 0.7903\n",
      "Epoch 87/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.8188\n",
      "Epoch 87: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8191 - val_loss: 0.1505 - val_accuracy: 0.7832\n",
      "Epoch 88/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.8149\n",
      "Epoch 88: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1345 - accuracy: 0.8161 - val_loss: 0.1499 - val_accuracy: 0.7862\n",
      "Epoch 89/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.8188\n",
      "Epoch 89: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1335 - accuracy: 0.8179 - val_loss: 0.1481 - val_accuracy: 0.7893\n",
      "Epoch 90/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.8177\n",
      "Epoch 90: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1334 - accuracy: 0.8164 - val_loss: 0.1490 - val_accuracy: 0.7852\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.8196\n",
      "Epoch 91: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.8184 - val_loss: 0.1481 - val_accuracy: 0.7923\n",
      "Epoch 92/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.8130\n",
      "Epoch 92: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1341 - accuracy: 0.8128 - val_loss: 0.1491 - val_accuracy: 0.7903\n",
      "Epoch 93/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.8167\n",
      "Epoch 93: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1330 - accuracy: 0.8191 - val_loss: 0.1499 - val_accuracy: 0.7893\n",
      "Epoch 94/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.8141\n",
      "Epoch 94: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1338 - accuracy: 0.8141 - val_loss: 0.1496 - val_accuracy: 0.7852\n",
      "Epoch 95/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.8162\n",
      "Epoch 95: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.8164 - val_loss: 0.1508 - val_accuracy: 0.7893\n",
      "Epoch 96/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.8164\n",
      "Epoch 96: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1337 - accuracy: 0.8166 - val_loss: 0.1508 - val_accuracy: 0.7812\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.8162\n",
      "Epoch 97: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8159 - val_loss: 0.1502 - val_accuracy: 0.7812\n",
      "Epoch 98/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.8141\n",
      "Epoch 98: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.8143 - val_loss: 0.1513 - val_accuracy: 0.7812\n",
      "Epoch 99/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.8219\n",
      "Epoch 99: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1333 - accuracy: 0.8212 - val_loss: 0.1520 - val_accuracy: 0.7741\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.8167\n",
      "Epoch 100: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1329 - accuracy: 0.8169 - val_loss: 0.1485 - val_accuracy: 0.7893\n",
      "Epoch 101/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.8203\n",
      "Epoch 101: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1328 - accuracy: 0.8194 - val_loss: 0.1501 - val_accuracy: 0.7852\n",
      "Epoch 102/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.8201\n",
      "Epoch 102: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1332 - accuracy: 0.8194 - val_loss: 0.1486 - val_accuracy: 0.7822\n",
      "Epoch 103/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.8188\n",
      "Epoch 103: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8186 - val_loss: 0.1504 - val_accuracy: 0.7832\n",
      "Epoch 104/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.8169\n",
      "Epoch 104: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8179 - val_loss: 0.1519 - val_accuracy: 0.7791\n",
      "Epoch 105/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.8230\n",
      "Epoch 105: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.8209 - val_loss: 0.1491 - val_accuracy: 0.7852\n",
      "Epoch 106/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.8214\n",
      "Epoch 106: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1317 - accuracy: 0.8204 - val_loss: 0.1515 - val_accuracy: 0.7801\n",
      "Epoch 107/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.8217\n",
      "Epoch 107: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.8214 - val_loss: 0.1482 - val_accuracy: 0.7832\n",
      "Epoch 108/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.8188\n",
      "Epoch 108: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1326 - accuracy: 0.8179 - val_loss: 0.1545 - val_accuracy: 0.7801\n",
      "Epoch 109/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.8180\n",
      "Epoch 109: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1329 - accuracy: 0.8184 - val_loss: 0.1499 - val_accuracy: 0.7781\n",
      "Epoch 110/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.8159\n",
      "Epoch 110: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8164 - val_loss: 0.1538 - val_accuracy: 0.7933\n",
      "Epoch 111/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.8180\n",
      "Epoch 111: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1325 - accuracy: 0.8179 - val_loss: 0.1503 - val_accuracy: 0.7852\n",
      "Epoch 112/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.8224\n",
      "Epoch 112: val_loss did not improve from 0.14714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1323 - accuracy: 0.8224 - val_loss: 0.1492 - val_accuracy: 0.7791\n",
      "Epoch 113/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.8241\n",
      "Epoch 113: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1314 - accuracy: 0.8235 - val_loss: 0.1477 - val_accuracy: 0.7872\n",
      "Epoch 114/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.8177\n",
      "Epoch 114: val_loss did not improve from 0.14714\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1331 - accuracy: 0.8202 - val_loss: 0.1484 - val_accuracy: 0.7822\n",
      "Epoch 115/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.8240\n",
      "Epoch 115: val_loss improved from 0.14714 to 0.14672, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1314 - accuracy: 0.8235 - val_loss: 0.1467 - val_accuracy: 0.7913\n",
      "Epoch 116/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.8211\n",
      "Epoch 116: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8194 - val_loss: 0.1516 - val_accuracy: 0.7893\n",
      "Epoch 117/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.8206\n",
      "Epoch 117: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.8202 - val_loss: 0.1499 - val_accuracy: 0.7842\n",
      "Epoch 118/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.8219\n",
      "Epoch 118: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8237 - val_loss: 0.1543 - val_accuracy: 0.7751\n",
      "Epoch 119/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.8248\n",
      "Epoch 119: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8240 - val_loss: 0.1510 - val_accuracy: 0.7771\n",
      "Epoch 120/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8261\n",
      "Epoch 120: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8262 - val_loss: 0.1512 - val_accuracy: 0.7872\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8256\n",
      "Epoch 121: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8255 - val_loss: 0.1493 - val_accuracy: 0.7862\n",
      "Epoch 122/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.8222\n",
      "Epoch 122: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8207 - val_loss: 0.1505 - val_accuracy: 0.7812\n",
      "Epoch 123/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.8209\n",
      "Epoch 123: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1306 - accuracy: 0.8214 - val_loss: 0.1490 - val_accuracy: 0.7852\n",
      "Epoch 124/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.8214\n",
      "Epoch 124: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8207 - val_loss: 0.1503 - val_accuracy: 0.7872\n",
      "Epoch 125/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.8243\n",
      "Epoch 125: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.8247 - val_loss: 0.1522 - val_accuracy: 0.7862\n",
      "Epoch 126/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.8251\n",
      "Epoch 126: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8245 - val_loss: 0.1478 - val_accuracy: 0.7882\n",
      "Epoch 127/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8261\n",
      "Epoch 127: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1315 - accuracy: 0.8245 - val_loss: 0.1487 - val_accuracy: 0.7852\n",
      "Epoch 128/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.8232\n",
      "Epoch 128: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.8227 - val_loss: 0.1499 - val_accuracy: 0.7842\n",
      "Epoch 129/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8248\n",
      "Epoch 129: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1301 - accuracy: 0.8262 - val_loss: 0.1487 - val_accuracy: 0.7832\n",
      "Epoch 130/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.8258\n",
      "Epoch 130: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1298 - accuracy: 0.8270 - val_loss: 0.1476 - val_accuracy: 0.7882\n",
      "Epoch 131/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1300 - accuracy: 0.8227\n",
      "Epoch 131: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1298 - accuracy: 0.8245 - val_loss: 0.1493 - val_accuracy: 0.7882\n",
      "Epoch 132/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.8235\n",
      "Epoch 132: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8250 - val_loss: 0.1500 - val_accuracy: 0.7913\n",
      "Epoch 133/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.8172\n",
      "Epoch 133: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.8191 - val_loss: 0.1491 - val_accuracy: 0.7913\n",
      "Epoch 134/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.8214\n",
      "Epoch 134: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.8240 - val_loss: 0.1496 - val_accuracy: 0.7842\n",
      "Epoch 135/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.8230\n",
      "Epoch 135: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1296 - accuracy: 0.8235 - val_loss: 0.1488 - val_accuracy: 0.7872\n",
      "Epoch 136/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.8227\n",
      "Epoch 136: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1308 - accuracy: 0.8222 - val_loss: 0.1500 - val_accuracy: 0.7801\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.8285\n",
      "Epoch 137: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.8265 - val_loss: 0.1504 - val_accuracy: 0.7771\n",
      "Epoch 138/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1313 - accuracy: 0.8238\n",
      "Epoch 138: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.8242 - val_loss: 0.1493 - val_accuracy: 0.7872\n",
      "Epoch 139/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.8235\n",
      "Epoch 139: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.8237 - val_loss: 0.1497 - val_accuracy: 0.7862\n",
      "Epoch 140/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.8258\n",
      "Epoch 140: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1300 - accuracy: 0.8270 - val_loss: 0.1493 - val_accuracy: 0.7852\n",
      "Epoch 141/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.8251\n",
      "Epoch 141: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1309 - accuracy: 0.8260 - val_loss: 0.1506 - val_accuracy: 0.7832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.8232\n",
      "Epoch 142: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1300 - accuracy: 0.8247 - val_loss: 0.1468 - val_accuracy: 0.7903\n",
      "Epoch 143/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.8261\n",
      "Epoch 143: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1294 - accuracy: 0.8257 - val_loss: 0.1470 - val_accuracy: 0.7872\n",
      "Epoch 144/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.8227\n",
      "Epoch 144: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1301 - accuracy: 0.8227 - val_loss: 0.1487 - val_accuracy: 0.7842\n",
      "Epoch 145/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.8237\n",
      "Epoch 145: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1296 - accuracy: 0.8247 - val_loss: 0.1485 - val_accuracy: 0.7832\n",
      "Epoch 146/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.8225\n",
      "Epoch 146: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1299 - accuracy: 0.8235 - val_loss: 0.1502 - val_accuracy: 0.7791\n",
      "Epoch 147/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.8264\n",
      "Epoch 147: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1297 - accuracy: 0.8275 - val_loss: 0.1519 - val_accuracy: 0.7872\n",
      "Epoch 148/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.8253\n",
      "Epoch 148: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.8257 - val_loss: 0.1518 - val_accuracy: 0.7771\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.8290\n",
      "Epoch 149: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1297 - accuracy: 0.8267 - val_loss: 0.1492 - val_accuracy: 0.7812\n",
      "Epoch 150/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.8332\n",
      "Epoch 150: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1295 - accuracy: 0.8323 - val_loss: 0.1475 - val_accuracy: 0.7943\n",
      "> 82.573\n",
      "Epoch 1/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.7589\n",
      "Epoch 1: val_loss improved from inf to 0.19292, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.2327 - accuracy: 0.7586 - val_loss: 0.1929 - val_accuracy: 0.7568\n",
      "Epoch 2/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.7584\n",
      "Epoch 2: val_loss improved from 0.19292 to 0.18331, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1855 - accuracy: 0.7586 - val_loss: 0.1833 - val_accuracy: 0.7568\n",
      "Epoch 3/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.7571\n",
      "Epoch 3: val_loss improved from 0.18331 to 0.18090, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1813 - accuracy: 0.7586 - val_loss: 0.1809 - val_accuracy: 0.7568\n",
      "Epoch 4/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.7587\n",
      "Epoch 4: val_loss improved from 0.18090 to 0.17751, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1783 - accuracy: 0.7586 - val_loss: 0.1775 - val_accuracy: 0.7568\n",
      "Epoch 5/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1742 - accuracy: 0.7587\n",
      "Epoch 5: val_loss improved from 0.17751 to 0.17348, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1742 - accuracy: 0.7586 - val_loss: 0.1735 - val_accuracy: 0.7568\n",
      "Epoch 6/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1709 - accuracy: 0.7566\n",
      "Epoch 6: val_loss improved from 0.17348 to 0.16897, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1697 - accuracy: 0.7586 - val_loss: 0.1690 - val_accuracy: 0.7568\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.7581\n",
      "Epoch 7: val_loss improved from 0.16897 to 0.16543, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1659 - accuracy: 0.7586 - val_loss: 0.1654 - val_accuracy: 0.7568\n",
      "Epoch 8/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.7578\n",
      "Epoch 8: val_loss improved from 0.16543 to 0.16291, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.7586 - val_loss: 0.1629 - val_accuracy: 0.7568\n",
      "Epoch 9/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1614 - accuracy: 0.7569\n",
      "Epoch 9: val_loss improved from 0.16291 to 0.16147, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1609 - accuracy: 0.7586 - val_loss: 0.1615 - val_accuracy: 0.7568\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.7581\n",
      "Epoch 10: val_loss improved from 0.16147 to 0.16052, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1591 - accuracy: 0.7586 - val_loss: 0.1605 - val_accuracy: 0.7568\n",
      "Epoch 11/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1588 - accuracy: 0.7569\n",
      "Epoch 11: val_loss improved from 0.16052 to 0.15972, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1578 - accuracy: 0.7586 - val_loss: 0.1597 - val_accuracy: 0.7568\n",
      "Epoch 12/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1568 - accuracy: 0.7623\n",
      "Epoch 12: val_loss improved from 0.15972 to 0.15903, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1568 - accuracy: 0.7619 - val_loss: 0.1590 - val_accuracy: 0.7700\n",
      "Epoch 13/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1563 - accuracy: 0.7699\n",
      "Epoch 13: val_loss improved from 0.15903 to 0.15867, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1558 - accuracy: 0.7710 - val_loss: 0.1587 - val_accuracy: 0.7660\n",
      "Epoch 14/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1556 - accuracy: 0.7656\n",
      "Epoch 14: val_loss improved from 0.15867 to 0.15837, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1551 - accuracy: 0.7667 - val_loss: 0.1584 - val_accuracy: 0.7629\n",
      "Epoch 15/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1544 - accuracy: 0.7666\n",
      "Epoch 15: val_loss improved from 0.15837 to 0.15835, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1542 - accuracy: 0.7655 - val_loss: 0.1583 - val_accuracy: 0.7619\n",
      "Epoch 16/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1543 - accuracy: 0.7618\n",
      "Epoch 16: val_loss improved from 0.15835 to 0.15799, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1537 - accuracy: 0.7644 - val_loss: 0.1580 - val_accuracy: 0.7609\n",
      "Epoch 17/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1539 - accuracy: 0.7649\n",
      "Epoch 17: val_loss did not improve from 0.15799\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1531 - accuracy: 0.7662 - val_loss: 0.1581 - val_accuracy: 0.7649\n",
      "Epoch 18/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1538 - accuracy: 0.7613\n",
      "Epoch 18: val_loss improved from 0.15799 to 0.15775, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1528 - accuracy: 0.7627 - val_loss: 0.1578 - val_accuracy: 0.7629\n",
      "Epoch 19/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.7679\n",
      "Epoch 19: val_loss improved from 0.15775 to 0.15755, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1521 - accuracy: 0.7655 - val_loss: 0.1575 - val_accuracy: 0.7649\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.7662\n",
      "Epoch 20: val_loss improved from 0.15755 to 0.15728, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1519 - accuracy: 0.7687 - val_loss: 0.1573 - val_accuracy: 0.7690\n",
      "Epoch 21/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1499 - accuracy: 0.7698\n",
      "Epoch 21: val_loss improved from 0.15728 to 0.15726, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1513 - accuracy: 0.7670 - val_loss: 0.1573 - val_accuracy: 0.7700\n",
      "Epoch 22/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1514 - accuracy: 0.7718\n",
      "Epoch 22: val_loss improved from 0.15726 to 0.15696, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1511 - accuracy: 0.7725 - val_loss: 0.1570 - val_accuracy: 0.7700\n",
      "Epoch 23/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1506 - accuracy: 0.7716\n",
      "Epoch 23: val_loss improved from 0.15696 to 0.15676, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1505 - accuracy: 0.7720 - val_loss: 0.1568 - val_accuracy: 0.7710\n",
      "Epoch 24/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.7769\n",
      "Epoch 24: val_loss improved from 0.15676 to 0.15649, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.7771 - val_loss: 0.1565 - val_accuracy: 0.7720\n",
      "Epoch 25/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1499 - accuracy: 0.7786\n",
      "Epoch 25: val_loss did not improve from 0.15649\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.7789 - val_loss: 0.1568 - val_accuracy: 0.7690\n",
      "Epoch 26/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1485 - accuracy: 0.7773\n",
      "Epoch 26: val_loss improved from 0.15649 to 0.15612, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1493 - accuracy: 0.7753 - val_loss: 0.1561 - val_accuracy: 0.7700\n",
      "Epoch 27/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1493 - accuracy: 0.7791\n",
      "Epoch 27: val_loss improved from 0.15612 to 0.15603, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1489 - accuracy: 0.7804 - val_loss: 0.1560 - val_accuracy: 0.7670\n",
      "Epoch 28/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.7799\n",
      "Epoch 28: val_loss improved from 0.15603 to 0.15565, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1488 - accuracy: 0.7796 - val_loss: 0.1557 - val_accuracy: 0.7710\n",
      "Epoch 29/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1470 - accuracy: 0.7819\n",
      "Epoch 29: val_loss improved from 0.15565 to 0.15551, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.7814 - val_loss: 0.1555 - val_accuracy: 0.7660\n",
      "Epoch 30/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1470 - accuracy: 0.7826\n",
      "Epoch 30: val_loss improved from 0.15551 to 0.15543, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1480 - accuracy: 0.7817 - val_loss: 0.1554 - val_accuracy: 0.7649\n",
      "Epoch 31/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1465 - accuracy: 0.7839\n",
      "Epoch 31: val_loss did not improve from 0.15543\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.7817 - val_loss: 0.1556 - val_accuracy: 0.7751\n",
      "Epoch 32/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.7857\n",
      "Epoch 32: val_loss improved from 0.15543 to 0.15484, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1474 - accuracy: 0.7862 - val_loss: 0.1548 - val_accuracy: 0.7690\n",
      "Epoch 33/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.7819\n",
      "Epoch 33: val_loss improved from 0.15484 to 0.15452, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1470 - accuracy: 0.7804 - val_loss: 0.1545 - val_accuracy: 0.7700\n",
      "Epoch 34/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1469 - accuracy: 0.7845\n",
      "Epoch 34: val_loss did not improve from 0.15452\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.7842 - val_loss: 0.1547 - val_accuracy: 0.7710\n",
      "Epoch 35/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1467 - accuracy: 0.7844\n",
      "Epoch 35: val_loss improved from 0.15452 to 0.15422, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.7850 - val_loss: 0.1542 - val_accuracy: 0.7741\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.7869\n",
      "Epoch 36: val_loss did not improve from 0.15422\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1461 - accuracy: 0.7877 - val_loss: 0.1543 - val_accuracy: 0.7720\n",
      "Epoch 37/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1465 - accuracy: 0.7852\n",
      "Epoch 37: val_loss improved from 0.15422 to 0.15397, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7872 - val_loss: 0.1540 - val_accuracy: 0.7761\n",
      "Epoch 38/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1454 - accuracy: 0.7865\n",
      "Epoch 38: val_loss improved from 0.15397 to 0.15392, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7860 - val_loss: 0.1539 - val_accuracy: 0.7741\n",
      "Epoch 39/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1472 - accuracy: 0.7873\n",
      "Epoch 39: val_loss improved from 0.15392 to 0.15386, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1451 - accuracy: 0.7905 - val_loss: 0.1539 - val_accuracy: 0.7761\n",
      "Epoch 40/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1464 - accuracy: 0.7828\n",
      "Epoch 40: val_loss improved from 0.15386 to 0.15360, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1448 - accuracy: 0.7855 - val_loss: 0.1536 - val_accuracy: 0.7771\n",
      "Epoch 41/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.7904\n",
      "Epoch 41: val_loss improved from 0.15360 to 0.15305, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.7895 - val_loss: 0.1530 - val_accuracy: 0.7751\n",
      "Epoch 42/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1451 - accuracy: 0.7893\n",
      "Epoch 42: val_loss improved from 0.15305 to 0.15293, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.7903 - val_loss: 0.1529 - val_accuracy: 0.7771\n",
      "Epoch 43/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1438 - accuracy: 0.7932\n",
      "Epoch 43: val_loss did not improve from 0.15293\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.7928 - val_loss: 0.1531 - val_accuracy: 0.7832\n",
      "Epoch 44/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.7902\n",
      "Epoch 44: val_loss improved from 0.15293 to 0.15247, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.7905 - val_loss: 0.1525 - val_accuracy: 0.7852\n",
      "Epoch 45/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1426 - accuracy: 0.7954\n",
      "Epoch 45: val_loss did not improve from 0.15247\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.7931 - val_loss: 0.1525 - val_accuracy: 0.7791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1422 - accuracy: 0.7942\n",
      "Epoch 46: val_loss improved from 0.15247 to 0.15235, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1435 - accuracy: 0.7931 - val_loss: 0.1524 - val_accuracy: 0.7812\n",
      "Epoch 47/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.7920\n",
      "Epoch 47: val_loss improved from 0.15235 to 0.15217, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.7933 - val_loss: 0.1522 - val_accuracy: 0.7801\n",
      "Epoch 48/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1440 - accuracy: 0.7952\n",
      "Epoch 48: val_loss did not improve from 0.15217\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1431 - accuracy: 0.7956 - val_loss: 0.1523 - val_accuracy: 0.7801\n",
      "Epoch 49/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1429 - accuracy: 0.7949\n",
      "Epoch 49: val_loss improved from 0.15217 to 0.15204, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.7941 - val_loss: 0.1520 - val_accuracy: 0.7832\n",
      "Epoch 50/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.7937\n",
      "Epoch 50: val_loss improved from 0.15204 to 0.15160, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1427 - accuracy: 0.7941 - val_loss: 0.1516 - val_accuracy: 0.7812\n",
      "Epoch 51/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1423 - accuracy: 0.7943\n",
      "Epoch 51: val_loss did not improve from 0.15160\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.7941 - val_loss: 0.1517 - val_accuracy: 0.7812\n",
      "Epoch 52/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1422 - accuracy: 0.7951\n",
      "Epoch 52: val_loss improved from 0.15160 to 0.15134, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.7938 - val_loss: 0.1513 - val_accuracy: 0.7822\n",
      "Epoch 53/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.7929\n",
      "Epoch 53: val_loss did not improve from 0.15134\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.7943 - val_loss: 0.1516 - val_accuracy: 0.7771\n",
      "Epoch 54/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.7967\n",
      "Epoch 54: val_loss improved from 0.15134 to 0.15106, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.7956 - val_loss: 0.1511 - val_accuracy: 0.7771\n",
      "Epoch 55/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.7946\n",
      "Epoch 55: val_loss did not improve from 0.15106\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7946 - val_loss: 0.1511 - val_accuracy: 0.7812\n",
      "Epoch 56/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.7973\n",
      "Epoch 56: val_loss did not improve from 0.15106\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.7979 - val_loss: 0.1514 - val_accuracy: 0.7862\n",
      "Epoch 57/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.7949\n",
      "Epoch 57: val_loss did not improve from 0.15106\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1417 - accuracy: 0.7956 - val_loss: 0.1511 - val_accuracy: 0.7852\n",
      "Epoch 58/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1414 - accuracy: 0.7997\n",
      "Epoch 58: val_loss improved from 0.15106 to 0.15044, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.7986 - val_loss: 0.1504 - val_accuracy: 0.7812\n",
      "Epoch 59/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1421 - accuracy: 0.7940\n",
      "Epoch 59: val_loss did not improve from 0.15044\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.7953 - val_loss: 0.1511 - val_accuracy: 0.7842\n",
      "Epoch 60/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1414 - accuracy: 0.7971\n",
      "Epoch 60: val_loss did not improve from 0.15044\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.7971 - val_loss: 0.1506 - val_accuracy: 0.7801\n",
      "Epoch 61/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.7967\n",
      "Epoch 61: val_loss did not improve from 0.15044\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.7971 - val_loss: 0.1505 - val_accuracy: 0.7781\n",
      "Epoch 62/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.7975\n",
      "Epoch 62: val_loss improved from 0.15044 to 0.15020, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.7981 - val_loss: 0.1502 - val_accuracy: 0.7801\n",
      "Epoch 63/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1407 - accuracy: 0.7979\n",
      "Epoch 63: val_loss improved from 0.15020 to 0.14982, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.7981 - val_loss: 0.1498 - val_accuracy: 0.7842\n",
      "Epoch 64/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1409 - accuracy: 0.7975\n",
      "Epoch 64: val_loss improved from 0.14982 to 0.14974, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.7986 - val_loss: 0.1497 - val_accuracy: 0.7822\n",
      "Epoch 65/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.7981\n",
      "Epoch 65: val_loss did not improve from 0.14974\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.7989 - val_loss: 0.1499 - val_accuracy: 0.7801\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.7999\n",
      "Epoch 66: val_loss improved from 0.14974 to 0.14972, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.7986 - val_loss: 0.1497 - val_accuracy: 0.7822\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.7999\n",
      "Epoch 67: val_loss did not improve from 0.14972\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.7981 - val_loss: 0.1500 - val_accuracy: 0.7771\n",
      "Epoch 68/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1384 - accuracy: 0.8019\n",
      "Epoch 68: val_loss did not improve from 0.14972\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.7996 - val_loss: 0.1498 - val_accuracy: 0.7801\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.7986\n",
      "Epoch 69: val_loss improved from 0.14972 to 0.14942, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.7999 - val_loss: 0.1494 - val_accuracy: 0.7812\n",
      "Epoch 70/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1402 - accuracy: 0.8004\n",
      "Epoch 70: val_loss did not improve from 0.14942\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8007 - val_loss: 0.1500 - val_accuracy: 0.7832\n",
      "Epoch 71/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8015\n",
      "Epoch 71: val_loss improved from 0.14942 to 0.14912, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.7996 - val_loss: 0.1491 - val_accuracy: 0.7832\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8023\n",
      "Epoch 72: val_loss improved from 0.14912 to 0.14909, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.8034 - val_loss: 0.1491 - val_accuracy: 0.7822\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8002\n",
      "Epoch 73: val_loss did not improve from 0.14909\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.7999 - val_loss: 0.1506 - val_accuracy: 0.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.7992\n",
      "Epoch 74: val_loss improved from 0.14909 to 0.14909, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.7994 - val_loss: 0.1491 - val_accuracy: 0.7822\n",
      "Epoch 75/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.8002\n",
      "Epoch 75: val_loss did not improve from 0.14909\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8029 - val_loss: 0.1491 - val_accuracy: 0.7791\n",
      "Epoch 76/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8044\n",
      "Epoch 76: val_loss improved from 0.14909 to 0.14893, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8047 - val_loss: 0.1489 - val_accuracy: 0.7832\n",
      "Epoch 77/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1403 - accuracy: 0.7988\n",
      "Epoch 77: val_loss did not improve from 0.14893\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8019 - val_loss: 0.1492 - val_accuracy: 0.7822\n",
      "Epoch 78/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.8026\n",
      "Epoch 78: val_loss did not improve from 0.14893\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8022 - val_loss: 0.1491 - val_accuracy: 0.7791\n",
      "Epoch 79/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8051\n",
      "Epoch 79: val_loss improved from 0.14893 to 0.14886, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8027 - val_loss: 0.1489 - val_accuracy: 0.7801\n",
      "Epoch 80/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1384 - accuracy: 0.8025\n",
      "Epoch 80: val_loss did not improve from 0.14886\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8017 - val_loss: 0.1490 - val_accuracy: 0.7771\n",
      "Epoch 81/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.7997\n",
      "Epoch 81: val_loss did not improve from 0.14886\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8019 - val_loss: 0.1490 - val_accuracy: 0.7801\n",
      "Epoch 82/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1392 - accuracy: 0.8043\n",
      "Epoch 82: val_loss improved from 0.14886 to 0.14872, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8042 - val_loss: 0.1487 - val_accuracy: 0.7801\n",
      "Epoch 83/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.8022\n",
      "Epoch 83: val_loss improved from 0.14872 to 0.14860, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8045 - val_loss: 0.1486 - val_accuracy: 0.7801\n",
      "Epoch 84/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1374 - accuracy: 0.8046\n",
      "Epoch 84: val_loss improved from 0.14860 to 0.14856, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8037 - val_loss: 0.1486 - val_accuracy: 0.7812\n",
      "Epoch 85/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.8038\n",
      "Epoch 85: val_loss improved from 0.14856 to 0.14836, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8029 - val_loss: 0.1484 - val_accuracy: 0.7791\n",
      "Epoch 86/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1382 - accuracy: 0.8027\n",
      "Epoch 86: val_loss did not improve from 0.14836\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8037 - val_loss: 0.1486 - val_accuracy: 0.7801\n",
      "Epoch 87/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1374 - accuracy: 0.8044\n",
      "Epoch 87: val_loss improved from 0.14836 to 0.14815, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.8022 - val_loss: 0.1481 - val_accuracy: 0.7822\n",
      "Epoch 88/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8044\n",
      "Epoch 88: val_loss improved from 0.14815 to 0.14813, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8037 - val_loss: 0.1481 - val_accuracy: 0.7801\n",
      "Epoch 89/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1384 - accuracy: 0.8044\n",
      "Epoch 89: val_loss did not improve from 0.14813\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8050 - val_loss: 0.1489 - val_accuracy: 0.7751\n",
      "Epoch 90/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.8040\n",
      "Epoch 90: val_loss did not improve from 0.14813\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8045 - val_loss: 0.1481 - val_accuracy: 0.7822\n",
      "Epoch 91/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1367 - accuracy: 0.8043\n",
      "Epoch 91: val_loss did not improve from 0.14813\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8019 - val_loss: 0.1482 - val_accuracy: 0.7812\n",
      "Epoch 92/150\n",
      "245/247 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.8074\n",
      "Epoch 92: val_loss improved from 0.14813 to 0.14801, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8065 - val_loss: 0.1480 - val_accuracy: 0.7832\n",
      "Epoch 93/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1378 - accuracy: 0.8033\n",
      "Epoch 93: val_loss did not improve from 0.14801\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8027 - val_loss: 0.1481 - val_accuracy: 0.7801\n",
      "Epoch 94/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1377 - accuracy: 0.8054\n",
      "Epoch 94: val_loss improved from 0.14801 to 0.14791, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8055 - val_loss: 0.1479 - val_accuracy: 0.7791\n",
      "Epoch 95/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8064\n",
      "Epoch 95: val_loss did not improve from 0.14791\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8047 - val_loss: 0.1485 - val_accuracy: 0.7832\n",
      "Epoch 96/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1366 - accuracy: 0.8069\n",
      "Epoch 96: val_loss improved from 0.14791 to 0.14766, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8032 - val_loss: 0.1477 - val_accuracy: 0.7801\n",
      "Epoch 97/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.8091\n",
      "Epoch 97: val_loss did not improve from 0.14766\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8078 - val_loss: 0.1483 - val_accuracy: 0.7801\n",
      "Epoch 98/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8033\n",
      "Epoch 98: val_loss did not improve from 0.14766\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8037 - val_loss: 0.1481 - val_accuracy: 0.7822\n",
      "Epoch 99/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1378 - accuracy: 0.8067\n",
      "Epoch 99: val_loss did not improve from 0.14766\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8072 - val_loss: 0.1479 - val_accuracy: 0.7781\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8086\n",
      "Epoch 100: val_loss did not improve from 0.14766\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8067 - val_loss: 0.1477 - val_accuracy: 0.7812\n",
      "Epoch 101/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8075\n",
      "Epoch 101: val_loss did not improve from 0.14766\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8078 - val_loss: 0.1484 - val_accuracy: 0.7812\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8044\n",
      "Epoch 102: val_loss improved from 0.14766 to 0.14758, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8040 - val_loss: 0.1476 - val_accuracy: 0.7852\n",
      "Epoch 103/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1383 - accuracy: 0.8045\n",
      "Epoch 103: val_loss did not improve from 0.14758\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8052 - val_loss: 0.1481 - val_accuracy: 0.7771\n",
      "Epoch 104/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1368 - accuracy: 0.8109\n",
      "Epoch 104: val_loss improved from 0.14758 to 0.14741, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8108 - val_loss: 0.1474 - val_accuracy: 0.7852\n",
      "Epoch 105/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1376 - accuracy: 0.8041\n",
      "Epoch 105: val_loss improved from 0.14741 to 0.14733, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8052 - val_loss: 0.1473 - val_accuracy: 0.7893\n",
      "Epoch 106/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.8052\n",
      "Epoch 106: val_loss did not improve from 0.14733\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8062 - val_loss: 0.1477 - val_accuracy: 0.7852\n",
      "Epoch 107/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.8051\n",
      "Epoch 107: val_loss did not improve from 0.14733\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8050 - val_loss: 0.1480 - val_accuracy: 0.7852\n",
      "Epoch 108/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.8078\n",
      "Epoch 108: val_loss did not improve from 0.14733\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8067 - val_loss: 0.1474 - val_accuracy: 0.7761\n",
      "Epoch 109/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8060\n",
      "Epoch 109: val_loss did not improve from 0.14733\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8052 - val_loss: 0.1477 - val_accuracy: 0.7832\n",
      "Epoch 110/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1379 - accuracy: 0.8049\n",
      "Epoch 110: val_loss improved from 0.14733 to 0.14730, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8060 - val_loss: 0.1473 - val_accuracy: 0.7781\n",
      "Epoch 111/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8082\n",
      "Epoch 111: val_loss did not improve from 0.14730\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8085 - val_loss: 0.1475 - val_accuracy: 0.7812\n",
      "Epoch 112/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.8072\n",
      "Epoch 112: val_loss improved from 0.14730 to 0.14707, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8060 - val_loss: 0.1471 - val_accuracy: 0.7781\n",
      "Epoch 113/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1375 - accuracy: 0.8025\n",
      "Epoch 113: val_loss did not improve from 0.14707\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8040 - val_loss: 0.1473 - val_accuracy: 0.7842\n",
      "Epoch 114/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8062\n",
      "Epoch 114: val_loss did not improve from 0.14707\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8057 - val_loss: 0.1473 - val_accuracy: 0.7852\n",
      "Epoch 115/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8075\n",
      "Epoch 115: val_loss did not improve from 0.14707\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8083 - val_loss: 0.1485 - val_accuracy: 0.7812\n",
      "Epoch 116/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.8085\n",
      "Epoch 116: val_loss did not improve from 0.14707\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8088 - val_loss: 0.1474 - val_accuracy: 0.7801\n",
      "Epoch 117/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1366 - accuracy: 0.8093\n",
      "Epoch 117: val_loss improved from 0.14707 to 0.14666, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8083 - val_loss: 0.1467 - val_accuracy: 0.7801\n",
      "Epoch 118/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.8070\n",
      "Epoch 118: val_loss improved from 0.14666 to 0.14658, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.8060 - val_loss: 0.1466 - val_accuracy: 0.7842\n",
      "Epoch 119/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.8060\n",
      "Epoch 119: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8060 - val_loss: 0.1471 - val_accuracy: 0.7862\n",
      "Epoch 120/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.8133\n",
      "Epoch 120: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8100 - val_loss: 0.1471 - val_accuracy: 0.7842\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.8060\n",
      "Epoch 121: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.8047 - val_loss: 0.1471 - val_accuracy: 0.7812\n",
      "Epoch 122/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1367 - accuracy: 0.8080\n",
      "Epoch 122: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8083 - val_loss: 0.1472 - val_accuracy: 0.7842\n",
      "Epoch 123/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1375 - accuracy: 0.8030\n",
      "Epoch 123: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8052 - val_loss: 0.1474 - val_accuracy: 0.7832\n",
      "Epoch 124/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.8086\n",
      "Epoch 124: val_loss did not improve from 0.14658\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8085 - val_loss: 0.1467 - val_accuracy: 0.7801\n",
      "Epoch 125/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.8090\n",
      "Epoch 125: val_loss improved from 0.14658 to 0.14635, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8090 - val_loss: 0.1464 - val_accuracy: 0.7842\n",
      "Epoch 126/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8065\n",
      "Epoch 126: val_loss improved from 0.14635 to 0.14626, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8067 - val_loss: 0.1463 - val_accuracy: 0.7771\n",
      "Epoch 127/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.8080\n",
      "Epoch 127: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8065 - val_loss: 0.1464 - val_accuracy: 0.7822\n",
      "Epoch 128/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.8083\n",
      "Epoch 128: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8085 - val_loss: 0.1472 - val_accuracy: 0.7832\n",
      "Epoch 129/150\n",
      "247/247 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.8090\n",
      "Epoch 129: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8090 - val_loss: 0.1465 - val_accuracy: 0.7812\n",
      "Epoch 130/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.8109\n",
      "Epoch 130: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8103 - val_loss: 0.1464 - val_accuracy: 0.7781\n",
      "Epoch 131/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1357 - accuracy: 0.8064\n",
      "Epoch 131: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8055 - val_loss: 0.1464 - val_accuracy: 0.7812\n",
      "Epoch 132/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8069\n",
      "Epoch 132: val_loss did not improve from 0.14626\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8067 - val_loss: 0.1466 - val_accuracy: 0.7781\n",
      "Epoch 133/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.8101\n",
      "Epoch 133: val_loss improved from 0.14626 to 0.14598, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8100 - val_loss: 0.1460 - val_accuracy: 0.7822\n",
      "Epoch 134/150\n",
      "235/247 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.8085\n",
      "Epoch 134: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8057 - val_loss: 0.1476 - val_accuracy: 0.7822\n",
      "Epoch 135/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.8054\n",
      "Epoch 135: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8070 - val_loss: 0.1467 - val_accuracy: 0.7761\n",
      "Epoch 136/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.8051\n",
      "Epoch 136: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8055 - val_loss: 0.1467 - val_accuracy: 0.7781\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.8088\n",
      "Epoch 137: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8078 - val_loss: 0.1462 - val_accuracy: 0.7832\n",
      "Epoch 138/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.8096\n",
      "Epoch 138: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8070 - val_loss: 0.1478 - val_accuracy: 0.7801\n",
      "Epoch 139/150\n",
      "245/247 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.8074\n",
      "Epoch 139: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8078 - val_loss: 0.1466 - val_accuracy: 0.7822\n",
      "Epoch 140/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.8086\n",
      "Epoch 140: val_loss did not improve from 0.14598\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8078 - val_loss: 0.1464 - val_accuracy: 0.7801\n",
      "Epoch 141/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1351 - accuracy: 0.8109\n",
      "Epoch 141: val_loss improved from 0.14598 to 0.14568, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8103 - val_loss: 0.1457 - val_accuracy: 0.7812\n",
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.8078\n",
      "Epoch 142: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8090 - val_loss: 0.1462 - val_accuracy: 0.7832\n",
      "Epoch 143/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1351 - accuracy: 0.8072\n",
      "Epoch 143: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1353 - accuracy: 0.8075 - val_loss: 0.1459 - val_accuracy: 0.7801\n",
      "Epoch 144/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8062\n",
      "Epoch 144: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8070 - val_loss: 0.1460 - val_accuracy: 0.7822\n",
      "Epoch 145/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.8094\n",
      "Epoch 145: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8093 - val_loss: 0.1458 - val_accuracy: 0.7842\n",
      "Epoch 146/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.8046\n",
      "Epoch 146: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8062 - val_loss: 0.1463 - val_accuracy: 0.7791\n",
      "Epoch 147/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1350 - accuracy: 0.8091\n",
      "Epoch 147: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1354 - accuracy: 0.8083 - val_loss: 0.1460 - val_accuracy: 0.7791\n",
      "Epoch 148/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1354 - accuracy: 0.8078\n",
      "Epoch 148: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8067 - val_loss: 0.1465 - val_accuracy: 0.7822\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.8073\n",
      "Epoch 149: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8060 - val_loss: 0.1462 - val_accuracy: 0.7822\n",
      "Epoch 150/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1360 - accuracy: 0.8091\n",
      "Epoch 150: val_loss did not improve from 0.14568\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8083 - val_loss: 0.1461 - val_accuracy: 0.7822\n",
      "> 79.635\n",
      "Epoch 1/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.3117 - accuracy: 0.7606\n",
      "Epoch 1: val_loss improved from inf to 0.21028, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.3088 - accuracy: 0.7586 - val_loss: 0.2103 - val_accuracy: 0.7568\n",
      "Epoch 2/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1920 - accuracy: 0.7593\n",
      "Epoch 2: val_loss improved from 0.21028 to 0.18524, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1920 - accuracy: 0.7586 - val_loss: 0.1852 - val_accuracy: 0.7568\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.7594\n",
      "Epoch 3: val_loss improved from 0.18524 to 0.18170, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1819 - accuracy: 0.7586 - val_loss: 0.1817 - val_accuracy: 0.7568\n",
      "Epoch 4/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.7579\n",
      "Epoch 4: val_loss improved from 0.18170 to 0.17972, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1794 - accuracy: 0.7586 - val_loss: 0.1797 - val_accuracy: 0.7568\n",
      "Epoch 5/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1781 - accuracy: 0.7568\n",
      "Epoch 5: val_loss improved from 0.17972 to 0.17786, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1772 - accuracy: 0.7586 - val_loss: 0.1779 - val_accuracy: 0.7568\n",
      "Epoch 6/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.7573\n",
      "Epoch 6: val_loss improved from 0.17786 to 0.17604, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1749 - accuracy: 0.7586 - val_loss: 0.1760 - val_accuracy: 0.7568\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.7594\n",
      "Epoch 7: val_loss improved from 0.17604 to 0.17376, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1727 - accuracy: 0.7586 - val_loss: 0.1738 - val_accuracy: 0.7568\n",
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.7563\n",
      "Epoch 8: val_loss improved from 0.17376 to 0.17076, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1700 - accuracy: 0.7586 - val_loss: 0.1708 - val_accuracy: 0.7568\n",
      "Epoch 9/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.7594\n",
      "Epoch 9: val_loss improved from 0.17076 to 0.16779, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1670 - accuracy: 0.7586 - val_loss: 0.1678 - val_accuracy: 0.7568\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.7597\n",
      "Epoch 10: val_loss improved from 0.16779 to 0.16521, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1641 - accuracy: 0.7586 - val_loss: 0.1652 - val_accuracy: 0.7568\n",
      "Epoch 11/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.7597\n",
      "Epoch 11: val_loss improved from 0.16521 to 0.16332, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1618 - accuracy: 0.7586 - val_loss: 0.1633 - val_accuracy: 0.7568\n",
      "Epoch 12/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.7581\n",
      "Epoch 12: val_loss improved from 0.16332 to 0.16196, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1599 - accuracy: 0.7586 - val_loss: 0.1620 - val_accuracy: 0.7568\n",
      "Epoch 13/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1576 - accuracy: 0.7634\n",
      "Epoch 13: val_loss improved from 0.16196 to 0.16099, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1585 - accuracy: 0.7614 - val_loss: 0.1610 - val_accuracy: 0.7619\n",
      "Epoch 14/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.7683\n",
      "Epoch 14: val_loss improved from 0.16099 to 0.16013, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1572 - accuracy: 0.7680 - val_loss: 0.1601 - val_accuracy: 0.7700\n",
      "Epoch 15/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.7717\n",
      "Epoch 15: val_loss improved from 0.16013 to 0.15944, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1561 - accuracy: 0.7718 - val_loss: 0.1594 - val_accuracy: 0.7629\n",
      "Epoch 16/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.7675\n",
      "Epoch 16: val_loss improved from 0.15944 to 0.15879, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1551 - accuracy: 0.7672 - val_loss: 0.1588 - val_accuracy: 0.7629\n",
      "Epoch 17/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1547 - accuracy: 0.7650\n",
      "Epoch 17: val_loss improved from 0.15879 to 0.15857, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1545 - accuracy: 0.7655 - val_loss: 0.1586 - val_accuracy: 0.7619\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.7654\n",
      "Epoch 18: val_loss improved from 0.15857 to 0.15817, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1539 - accuracy: 0.7662 - val_loss: 0.1582 - val_accuracy: 0.7579\n",
      "Epoch 19/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1539 - accuracy: 0.7610\n",
      "Epoch 19: val_loss improved from 0.15817 to 0.15772, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1530 - accuracy: 0.7639 - val_loss: 0.1577 - val_accuracy: 0.7599\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.7673\n",
      "Epoch 20: val_loss improved from 0.15772 to 0.15729, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1527 - accuracy: 0.7672 - val_loss: 0.1573 - val_accuracy: 0.7619\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.7694\n",
      "Epoch 21: val_loss improved from 0.15729 to 0.15716, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.7682 - val_loss: 0.1572 - val_accuracy: 0.7649\n",
      "Epoch 22/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.7657\n",
      "Epoch 22: val_loss improved from 0.15716 to 0.15694, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1516 - accuracy: 0.7662 - val_loss: 0.1569 - val_accuracy: 0.7670\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.7691\n",
      "Epoch 23: val_loss improved from 0.15694 to 0.15660, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1510 - accuracy: 0.7690 - val_loss: 0.1566 - val_accuracy: 0.7649\n",
      "Epoch 24/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.7699\n",
      "Epoch 24: val_loss improved from 0.15660 to 0.15629, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1507 - accuracy: 0.7695 - val_loss: 0.1563 - val_accuracy: 0.7649\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.7728\n",
      "Epoch 25: val_loss improved from 0.15629 to 0.15602, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1502 - accuracy: 0.7728 - val_loss: 0.1560 - val_accuracy: 0.7639\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.7717\n",
      "Epoch 26: val_loss improved from 0.15602 to 0.15580, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.7715 - val_loss: 0.1558 - val_accuracy: 0.7700\n",
      "Epoch 27/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.7759\n",
      "Epoch 27: val_loss did not improve from 0.15580\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1493 - accuracy: 0.7753 - val_loss: 0.1559 - val_accuracy: 0.7700\n",
      "Epoch 28/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.7777\n",
      "Epoch 28: val_loss improved from 0.15580 to 0.15557, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1490 - accuracy: 0.7779 - val_loss: 0.1556 - val_accuracy: 0.7710\n",
      "Epoch 29/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.7812\n",
      "Epoch 29: val_loss improved from 0.15557 to 0.15529, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.7804 - val_loss: 0.1553 - val_accuracy: 0.7730\n",
      "Epoch 30/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1488 - accuracy: 0.7782\n",
      "Epoch 30: val_loss improved from 0.15529 to 0.15481, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1484 - accuracy: 0.7789 - val_loss: 0.1548 - val_accuracy: 0.7741\n",
      "Epoch 31/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.7811\n",
      "Epoch 31: val_loss improved from 0.15481 to 0.15462, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1480 - accuracy: 0.7809 - val_loss: 0.1546 - val_accuracy: 0.7730\n",
      "Epoch 32/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.7775\n",
      "Epoch 32: val_loss improved from 0.15462 to 0.15454, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1475 - accuracy: 0.7786 - val_loss: 0.1545 - val_accuracy: 0.7710\n",
      "Epoch 33/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.7832\n",
      "Epoch 33: val_loss improved from 0.15454 to 0.15419, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1474 - accuracy: 0.7827 - val_loss: 0.1542 - val_accuracy: 0.7730\n",
      "Epoch 34/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.7814\n",
      "Epoch 34: val_loss improved from 0.15419 to 0.15396, saving model to bestparams2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1470 - accuracy: 0.7832 - val_loss: 0.1540 - val_accuracy: 0.7730\n",
      "Epoch 35/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1468 - accuracy: 0.7847\n",
      "Epoch 35: val_loss improved from 0.15396 to 0.15384, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1465 - accuracy: 0.7842 - val_loss: 0.1538 - val_accuracy: 0.7720\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.7824\n",
      "Epoch 36: val_loss did not improve from 0.15384\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.7842 - val_loss: 0.1539 - val_accuracy: 0.7781\n",
      "Epoch 37/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.7848\n",
      "Epoch 37: val_loss improved from 0.15384 to 0.15344, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1461 - accuracy: 0.7839 - val_loss: 0.1534 - val_accuracy: 0.7741\n",
      "Epoch 38/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1454 - accuracy: 0.7902\n",
      "Epoch 38: val_loss improved from 0.15344 to 0.15300, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1456 - accuracy: 0.7890 - val_loss: 0.1530 - val_accuracy: 0.7761\n",
      "Epoch 39/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.7884\n",
      "Epoch 39: val_loss did not improve from 0.15300\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1453 - accuracy: 0.7875 - val_loss: 0.1531 - val_accuracy: 0.7812\n",
      "Epoch 40/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.7890\n",
      "Epoch 40: val_loss improved from 0.15300 to 0.15297, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1451 - accuracy: 0.7882 - val_loss: 0.1530 - val_accuracy: 0.7791\n",
      "Epoch 41/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.7900\n",
      "Epoch 41: val_loss did not improve from 0.15297\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1448 - accuracy: 0.7888 - val_loss: 0.1530 - val_accuracy: 0.7791\n",
      "Epoch 42/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.7913\n",
      "Epoch 42: val_loss improved from 0.15297 to 0.15270, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.7910 - val_loss: 0.1527 - val_accuracy: 0.7771\n",
      "Epoch 43/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1442 - accuracy: 0.7899\n",
      "Epoch 43: val_loss improved from 0.15270 to 0.15224, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1443 - accuracy: 0.7890 - val_loss: 0.1522 - val_accuracy: 0.7822\n",
      "Epoch 44/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.7926\n",
      "Epoch 44: val_loss improved from 0.15224 to 0.15180, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.7920 - val_loss: 0.1518 - val_accuracy: 0.7801\n",
      "Epoch 45/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.7918\n",
      "Epoch 45: val_loss did not improve from 0.15180\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.7905 - val_loss: 0.1523 - val_accuracy: 0.7832\n",
      "Epoch 46/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.7942\n",
      "Epoch 46: val_loss did not improve from 0.15180\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.7943 - val_loss: 0.1520 - val_accuracy: 0.7832\n",
      "Epoch 47/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.7955\n",
      "Epoch 47: val_loss improved from 0.15180 to 0.15167, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1434 - accuracy: 0.7941 - val_loss: 0.1517 - val_accuracy: 0.7862\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.7945\n",
      "Epoch 48: val_loss improved from 0.15167 to 0.15145, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.7936 - val_loss: 0.1515 - val_accuracy: 0.7832\n",
      "Epoch 49/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1432 - accuracy: 0.7925\n",
      "Epoch 49: val_loss improved from 0.15145 to 0.15144, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.7933 - val_loss: 0.1514 - val_accuracy: 0.7812\n",
      "Epoch 50/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1441 - accuracy: 0.7916\n",
      "Epoch 50: val_loss improved from 0.15144 to 0.15136, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.7936 - val_loss: 0.1514 - val_accuracy: 0.7812\n",
      "Epoch 51/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1433 - accuracy: 0.7951\n",
      "Epoch 51: val_loss did not improve from 0.15136\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.7969 - val_loss: 0.1515 - val_accuracy: 0.7852\n",
      "Epoch 52/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.7960\n",
      "Epoch 52: val_loss improved from 0.15136 to 0.15126, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.7961 - val_loss: 0.1513 - val_accuracy: 0.7801\n",
      "Epoch 53/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1417 - accuracy: 0.7975\n",
      "Epoch 53: val_loss improved from 0.15126 to 0.15077, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.7971 - val_loss: 0.1508 - val_accuracy: 0.7812\n",
      "Epoch 54/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1425 - accuracy: 0.7967\n",
      "Epoch 54: val_loss did not improve from 0.15077\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1421 - accuracy: 0.7969 - val_loss: 0.1510 - val_accuracy: 0.7801\n",
      "Epoch 55/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.7984\n",
      "Epoch 55: val_loss improved from 0.15077 to 0.15061, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.7986 - val_loss: 0.1506 - val_accuracy: 0.7771\n",
      "Epoch 56/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1417 - accuracy: 0.7973\n",
      "Epoch 56: val_loss did not improve from 0.15061\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.7974 - val_loss: 0.1506 - val_accuracy: 0.7791\n",
      "Epoch 57/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.7989\n",
      "Epoch 57: val_loss improved from 0.15061 to 0.15036, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.8002 - val_loss: 0.1504 - val_accuracy: 0.7812\n",
      "Epoch 58/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1424 - accuracy: 0.7970\n",
      "Epoch 58: val_loss did not improve from 0.15036\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.7984 - val_loss: 0.1504 - val_accuracy: 0.7842\n",
      "Epoch 59/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1416 - accuracy: 0.7954\n",
      "Epoch 59: val_loss improved from 0.15036 to 0.15012, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.7966 - val_loss: 0.1501 - val_accuracy: 0.7801\n",
      "Epoch 60/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.7963\n",
      "Epoch 60: val_loss did not improve from 0.15012\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.7981 - val_loss: 0.1504 - val_accuracy: 0.7761\n",
      "Epoch 61/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.7971\n",
      "Epoch 61: val_loss did not improve from 0.15012\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1409 - accuracy: 0.7971 - val_loss: 0.1507 - val_accuracy: 0.7832\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8026\n",
      "Epoch 62: val_loss did not improve from 0.15012\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8004 - val_loss: 0.1501 - val_accuracy: 0.7862\n",
      "Epoch 63/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8007\n",
      "Epoch 63: val_loss improved from 0.15012 to 0.14985, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.8007 - val_loss: 0.1498 - val_accuracy: 0.7791\n",
      "Epoch 64/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8031\n",
      "Epoch 64: val_loss improved from 0.14985 to 0.14981, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8024 - val_loss: 0.1498 - val_accuracy: 0.7801\n",
      "Epoch 65/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.7973\n",
      "Epoch 65: val_loss improved from 0.14981 to 0.14976, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.7979 - val_loss: 0.1498 - val_accuracy: 0.7781\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.7999\n",
      "Epoch 66: val_loss improved from 0.14976 to 0.14962, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8019 - val_loss: 0.1496 - val_accuracy: 0.7791\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8007\n",
      "Epoch 67: val_loss improved from 0.14962 to 0.14938, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.8002 - val_loss: 0.1494 - val_accuracy: 0.7842\n",
      "Epoch 68/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.7984\n",
      "Epoch 68: val_loss improved from 0.14938 to 0.14935, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.7986 - val_loss: 0.1494 - val_accuracy: 0.7791\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8007\n",
      "Epoch 69: val_loss improved from 0.14935 to 0.14931, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1402 - accuracy: 0.8002 - val_loss: 0.1493 - val_accuracy: 0.7822\n",
      "Epoch 70/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1401 - accuracy: 0.8036\n",
      "Epoch 70: val_loss did not improve from 0.14931\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8037 - val_loss: 0.1499 - val_accuracy: 0.7852\n",
      "Epoch 71/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.7952\n",
      "Epoch 71: val_loss improved from 0.14931 to 0.14895, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.7971 - val_loss: 0.1490 - val_accuracy: 0.7822\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8007\n",
      "Epoch 72: val_loss did not improve from 0.14895\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8002 - val_loss: 0.1490 - val_accuracy: 0.7801\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.7999\n",
      "Epoch 73: val_loss did not improve from 0.14895\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.8007 - val_loss: 0.1490 - val_accuracy: 0.7822\n",
      "Epoch 74/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1397 - accuracy: 0.8004\n",
      "Epoch 74: val_loss did not improve from 0.14895\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.8019 - val_loss: 0.1492 - val_accuracy: 0.7801\n",
      "Epoch 75/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.8031\n",
      "Epoch 75: val_loss improved from 0.14895 to 0.14892, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.8014 - val_loss: 0.1489 - val_accuracy: 0.7791\n",
      "Epoch 76/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8026\n",
      "Epoch 76: val_loss improved from 0.14892 to 0.14889, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8027 - val_loss: 0.1489 - val_accuracy: 0.7812\n",
      "Epoch 77/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1397 - accuracy: 0.8009\n",
      "Epoch 77: val_loss improved from 0.14889 to 0.14873, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.8009 - val_loss: 0.1487 - val_accuracy: 0.7812\n",
      "Epoch 78/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8015\n",
      "Epoch 78: val_loss improved from 0.14873 to 0.14868, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8017 - val_loss: 0.1487 - val_accuracy: 0.7801\n",
      "Epoch 79/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8015\n",
      "Epoch 79: val_loss improved from 0.14868 to 0.14860, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8024 - val_loss: 0.1486 - val_accuracy: 0.7771\n",
      "Epoch 80/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8041\n",
      "Epoch 80: val_loss did not improve from 0.14860\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8029 - val_loss: 0.1497 - val_accuracy: 0.7852\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8039\n",
      "Epoch 81: val_loss improved from 0.14860 to 0.14860, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8047 - val_loss: 0.1486 - val_accuracy: 0.7781\n",
      "Epoch 82/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1389 - accuracy: 0.8025\n",
      "Epoch 82: val_loss did not improve from 0.14860\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8027 - val_loss: 0.1486 - val_accuracy: 0.7812\n",
      "Epoch 83/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8044\n",
      "Epoch 83: val_loss did not improve from 0.14860\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8029 - val_loss: 0.1488 - val_accuracy: 0.7812\n",
      "Epoch 84/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1392 - accuracy: 0.8012\n",
      "Epoch 84: val_loss improved from 0.14860 to 0.14812, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8029 - val_loss: 0.1481 - val_accuracy: 0.7801\n",
      "Epoch 85/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.8036\n",
      "Epoch 85: val_loss improved from 0.14812 to 0.14797, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8047 - val_loss: 0.1480 - val_accuracy: 0.7812\n",
      "Epoch 86/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1382 - accuracy: 0.8006\n",
      "Epoch 86: val_loss improved from 0.14797 to 0.14797, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8004 - val_loss: 0.1480 - val_accuracy: 0.7822\n",
      "Epoch 87/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8070\n",
      "Epoch 87: val_loss improved from 0.14797 to 0.14760, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.8062 - val_loss: 0.1476 - val_accuracy: 0.7822\n",
      "Epoch 88/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1382 - accuracy: 0.8046\n",
      "Epoch 88: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.8027 - val_loss: 0.1479 - val_accuracy: 0.7812\n",
      "Epoch 89/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.8028\n",
      "Epoch 89: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8034 - val_loss: 0.1479 - val_accuracy: 0.7791\n",
      "Epoch 90/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1386 - accuracy: 0.8044\n",
      "Epoch 90: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8052 - val_loss: 0.1480 - val_accuracy: 0.7801\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.8015\n",
      "Epoch 91: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8024 - val_loss: 0.1489 - val_accuracy: 0.7842\n",
      "Epoch 92/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8033\n",
      "Epoch 92: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8045 - val_loss: 0.1490 - val_accuracy: 0.7842\n",
      "Epoch 93/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8020\n",
      "Epoch 93: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8017 - val_loss: 0.1481 - val_accuracy: 0.7771\n",
      "Epoch 94/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1382 - accuracy: 0.8025\n",
      "Epoch 94: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8040 - val_loss: 0.1478 - val_accuracy: 0.7791\n",
      "Epoch 95/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.8025\n",
      "Epoch 95: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8052 - val_loss: 0.1485 - val_accuracy: 0.7862\n",
      "Epoch 96/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8020\n",
      "Epoch 96: val_loss did not improve from 0.14760\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8034 - val_loss: 0.1478 - val_accuracy: 0.7791\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8099\n",
      "Epoch 97: val_loss improved from 0.14760 to 0.14754, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8070 - val_loss: 0.1475 - val_accuracy: 0.7832\n",
      "Epoch 98/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.8039\n",
      "Epoch 98: val_loss did not improve from 0.14754\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8042 - val_loss: 0.1476 - val_accuracy: 0.7862\n",
      "Epoch 99/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.8044\n",
      "Epoch 99: val_loss did not improve from 0.14754\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8050 - val_loss: 0.1476 - val_accuracy: 0.7812\n",
      "Epoch 100/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1389 - accuracy: 0.8049\n",
      "Epoch 100: val_loss did not improve from 0.14754\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8062 - val_loss: 0.1478 - val_accuracy: 0.7832\n",
      "Epoch 101/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.8080\n",
      "Epoch 101: val_loss improved from 0.14754 to 0.14720, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8070 - val_loss: 0.1472 - val_accuracy: 0.7771\n",
      "Epoch 102/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1375 - accuracy: 0.8041\n",
      "Epoch 102: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8032 - val_loss: 0.1474 - val_accuracy: 0.7791\n",
      "Epoch 103/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1376 - accuracy: 0.8059\n",
      "Epoch 103: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8057 - val_loss: 0.1474 - val_accuracy: 0.7791\n",
      "Epoch 104/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.8065\n",
      "Epoch 104: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8060 - val_loss: 0.1477 - val_accuracy: 0.7812\n",
      "Epoch 105/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.8070\n",
      "Epoch 105: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8072 - val_loss: 0.1474 - val_accuracy: 0.7781\n",
      "Epoch 106/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.8070\n",
      "Epoch 106: val_loss did not improve from 0.14720\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8057 - val_loss: 0.1473 - val_accuracy: 0.7791\n",
      "Epoch 107/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1375 - accuracy: 0.8070\n",
      "Epoch 107: val_loss improved from 0.14720 to 0.14717, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8083 - val_loss: 0.1472 - val_accuracy: 0.7761\n",
      "Epoch 108/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.8041\n",
      "Epoch 108: val_loss improved from 0.14717 to 0.14708, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8040 - val_loss: 0.1471 - val_accuracy: 0.7761\n",
      "Epoch 109/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.8080\n",
      "Epoch 109: val_loss did not improve from 0.14708\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8065 - val_loss: 0.1473 - val_accuracy: 0.7812\n",
      "Epoch 110/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.8070\n",
      "Epoch 110: val_loss improved from 0.14708 to 0.14704, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8057 - val_loss: 0.1470 - val_accuracy: 0.7862\n",
      "Epoch 111/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.8033\n",
      "Epoch 111: val_loss did not improve from 0.14704\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8040 - val_loss: 0.1471 - val_accuracy: 0.7791\n",
      "Epoch 112/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8067\n",
      "Epoch 112: val_loss improved from 0.14704 to 0.14697, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8078 - val_loss: 0.1470 - val_accuracy: 0.7862\n",
      "Epoch 113/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8054\n",
      "Epoch 113: val_loss improved from 0.14697 to 0.14687, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8060 - val_loss: 0.1469 - val_accuracy: 0.7862\n",
      "Epoch 114/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1368 - accuracy: 0.8067\n",
      "Epoch 114: val_loss did not improve from 0.14687\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8072 - val_loss: 0.1479 - val_accuracy: 0.7852\n",
      "Epoch 115/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1360 - accuracy: 0.8099\n",
      "Epoch 115: val_loss did not improve from 0.14687\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8067 - val_loss: 0.1477 - val_accuracy: 0.7822\n",
      "Epoch 116/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.8062\n",
      "Epoch 116: val_loss did not improve from 0.14687\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8062 - val_loss: 0.1469 - val_accuracy: 0.7781\n",
      "Epoch 117/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.8086\n",
      "Epoch 117: val_loss did not improve from 0.14687\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8093 - val_loss: 0.1475 - val_accuracy: 0.7822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.8049\n",
      "Epoch 118: val_loss improved from 0.14687 to 0.14657, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8040 - val_loss: 0.1466 - val_accuracy: 0.7822\n",
      "Epoch 119/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.8035\n",
      "Epoch 119: val_loss did not improve from 0.14657\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8042 - val_loss: 0.1469 - val_accuracy: 0.7872\n",
      "Epoch 120/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.8083\n",
      "Epoch 120: val_loss did not improve from 0.14657\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8070 - val_loss: 0.1467 - val_accuracy: 0.7801\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8054\n",
      "Epoch 121: val_loss improved from 0.14657 to 0.14634, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8070 - val_loss: 0.1463 - val_accuracy: 0.7812\n",
      "Epoch 122/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.8075\n",
      "Epoch 122: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8062 - val_loss: 0.1470 - val_accuracy: 0.7882\n",
      "Epoch 123/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.8099\n",
      "Epoch 123: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8052 - val_loss: 0.1465 - val_accuracy: 0.7812\n",
      "Epoch 124/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.8081\n",
      "Epoch 124: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8078 - val_loss: 0.1465 - val_accuracy: 0.7801\n",
      "Epoch 125/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.8039\n",
      "Epoch 125: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8037 - val_loss: 0.1471 - val_accuracy: 0.7872\n",
      "Epoch 126/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8073\n",
      "Epoch 126: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8080 - val_loss: 0.1465 - val_accuracy: 0.7791\n",
      "Epoch 127/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.8109\n",
      "Epoch 127: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8100 - val_loss: 0.1466 - val_accuracy: 0.7842\n",
      "Epoch 128/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1367 - accuracy: 0.8065\n",
      "Epoch 128: val_loss did not improve from 0.14634\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8072 - val_loss: 0.1469 - val_accuracy: 0.7862\n",
      "Epoch 129/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8073\n",
      "Epoch 129: val_loss improved from 0.14634 to 0.14619, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1366 - accuracy: 0.8083 - val_loss: 0.1462 - val_accuracy: 0.7812\n",
      "Epoch 130/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.8067\n",
      "Epoch 130: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.8075 - val_loss: 0.1465 - val_accuracy: 0.7822\n",
      "Epoch 131/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.8062\n",
      "Epoch 131: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8067 - val_loss: 0.1464 - val_accuracy: 0.7812\n",
      "Epoch 132/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.8094\n",
      "Epoch 132: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8088 - val_loss: 0.1463 - val_accuracy: 0.7832\n",
      "Epoch 133/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8065\n",
      "Epoch 133: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8062 - val_loss: 0.1468 - val_accuracy: 0.7832\n",
      "Epoch 134/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8036\n",
      "Epoch 134: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8047 - val_loss: 0.1463 - val_accuracy: 0.7791\n",
      "Epoch 135/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.8104\n",
      "Epoch 135: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8085 - val_loss: 0.1463 - val_accuracy: 0.7791\n",
      "Epoch 136/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.8078\n",
      "Epoch 136: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8067 - val_loss: 0.1465 - val_accuracy: 0.7842\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.8096\n",
      "Epoch 137: val_loss did not improve from 0.14619\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8083 - val_loss: 0.1465 - val_accuracy: 0.7852\n",
      "Epoch 138/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.8075\n",
      "Epoch 138: val_loss improved from 0.14619 to 0.14618, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8062 - val_loss: 0.1462 - val_accuracy: 0.7791\n",
      "Epoch 139/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.8088\n",
      "Epoch 139: val_loss improved from 0.14618 to 0.14584, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8080 - val_loss: 0.1458 - val_accuracy: 0.7812\n",
      "Epoch 140/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.8096\n",
      "Epoch 140: val_loss did not improve from 0.14584\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8085 - val_loss: 0.1463 - val_accuracy: 0.7801\n",
      "Epoch 141/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.8065\n",
      "Epoch 141: val_loss improved from 0.14584 to 0.14575, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8072 - val_loss: 0.1458 - val_accuracy: 0.7842\n",
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8067\n",
      "Epoch 142: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8072 - val_loss: 0.1465 - val_accuracy: 0.7801\n",
      "Epoch 143/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.8062\n",
      "Epoch 143: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8075 - val_loss: 0.1461 - val_accuracy: 0.7812\n",
      "Epoch 144/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1351 - accuracy: 0.8088\n",
      "Epoch 144: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8080 - val_loss: 0.1462 - val_accuracy: 0.7791\n",
      "Epoch 145/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.8041\n",
      "Epoch 145: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8047 - val_loss: 0.1483 - val_accuracy: 0.7801\n",
      "Epoch 146/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.8062\n",
      "Epoch 146: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8060 - val_loss: 0.1461 - val_accuracy: 0.7822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.8067\n",
      "Epoch 147: val_loss did not improve from 0.14575\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8070 - val_loss: 0.1458 - val_accuracy: 0.7812\n",
      "Epoch 148/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.8093\n",
      "Epoch 148: val_loss improved from 0.14575 to 0.14574, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8075 - val_loss: 0.1457 - val_accuracy: 0.7832\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.8088\n",
      "Epoch 149: val_loss improved from 0.14574 to 0.14571, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8083 - val_loss: 0.1457 - val_accuracy: 0.7822\n",
      "Epoch 150/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.8107\n",
      "Epoch 150: val_loss did not improve from 0.14571\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1355 - accuracy: 0.8095 - val_loss: 0.1459 - val_accuracy: 0.7822\n",
      "> 80.142\n",
      "Epoch 1/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.4223 - accuracy: 0.7582\n",
      "Epoch 1: val_loss improved from inf to 0.23446, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 2s 5ms/step - loss: 0.4141 - accuracy: 0.7586 - val_loss: 0.2345 - val_accuracy: 0.7568\n",
      "Epoch 2/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.2037 - accuracy: 0.7594\n",
      "Epoch 2: val_loss improved from 0.23446 to 0.19158, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.2038 - accuracy: 0.7586 - val_loss: 0.1916 - val_accuracy: 0.7568\n",
      "Epoch 3/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.7612\n",
      "Epoch 3: val_loss improved from 0.19158 to 0.18441, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1849 - accuracy: 0.7586 - val_loss: 0.1844 - val_accuracy: 0.7568\n",
      "Epoch 4/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1811 - accuracy: 0.7594\n",
      "Epoch 4: val_loss improved from 0.18441 to 0.18269, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1815 - accuracy: 0.7586 - val_loss: 0.1827 - val_accuracy: 0.7568\n",
      "Epoch 5/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.7584\n",
      "Epoch 5: val_loss improved from 0.18269 to 0.18162, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1798 - accuracy: 0.7586 - val_loss: 0.1816 - val_accuracy: 0.7568\n",
      "Epoch 6/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.7578\n",
      "Epoch 6: val_loss improved from 0.18162 to 0.18098, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1785 - accuracy: 0.7586 - val_loss: 0.1810 - val_accuracy: 0.7568\n",
      "Epoch 7/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.7589\n",
      "Epoch 7: val_loss improved from 0.18098 to 0.18009, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1774 - accuracy: 0.7586 - val_loss: 0.1801 - val_accuracy: 0.7568\n",
      "Epoch 8/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1763 - accuracy: 0.7584\n",
      "Epoch 8: val_loss improved from 0.18009 to 0.17929, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1763 - accuracy: 0.7586 - val_loss: 0.1793 - val_accuracy: 0.7568\n",
      "Epoch 9/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.7602\n",
      "Epoch 9: val_loss improved from 0.17929 to 0.17794, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1750 - accuracy: 0.7586 - val_loss: 0.1779 - val_accuracy: 0.7568\n",
      "Epoch 10/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.7586\n",
      "Epoch 10: val_loss improved from 0.17794 to 0.17673, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1737 - accuracy: 0.7586 - val_loss: 0.1767 - val_accuracy: 0.7568\n",
      "Epoch 11/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1715 - accuracy: 0.7589\n",
      "Epoch 11: val_loss improved from 0.17673 to 0.17575, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1720 - accuracy: 0.7586 - val_loss: 0.1757 - val_accuracy: 0.7568\n",
      "Epoch 12/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.7576\n",
      "Epoch 12: val_loss improved from 0.17575 to 0.17260, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1699 - accuracy: 0.7586 - val_loss: 0.1726 - val_accuracy: 0.7568\n",
      "Epoch 13/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.7589\n",
      "Epoch 13: val_loss improved from 0.17260 to 0.17020, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1678 - accuracy: 0.7586 - val_loss: 0.1702 - val_accuracy: 0.7568\n",
      "Epoch 14/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.7592\n",
      "Epoch 14: val_loss improved from 0.17020 to 0.16836, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1655 - accuracy: 0.7586 - val_loss: 0.1684 - val_accuracy: 0.7568\n",
      "Epoch 15/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.7592\n",
      "Epoch 15: val_loss improved from 0.16836 to 0.16590, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1633 - accuracy: 0.7586 - val_loss: 0.1659 - val_accuracy: 0.7568\n",
      "Epoch 16/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1601 - accuracy: 0.7624\n",
      "Epoch 16: val_loss improved from 0.16590 to 0.16426, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1613 - accuracy: 0.7601 - val_loss: 0.1643 - val_accuracy: 0.7609\n",
      "Epoch 17/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.7660\n",
      "Epoch 17: val_loss improved from 0.16426 to 0.16291, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1593 - accuracy: 0.7649 - val_loss: 0.1629 - val_accuracy: 0.7639\n",
      "Epoch 18/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.7709\n",
      "Epoch 18: val_loss improved from 0.16291 to 0.16131, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1577 - accuracy: 0.7703 - val_loss: 0.1613 - val_accuracy: 0.7660\n",
      "Epoch 19/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1553 - accuracy: 0.7715\n",
      "Epoch 19: val_loss improved from 0.16131 to 0.16012, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1563 - accuracy: 0.7698 - val_loss: 0.1601 - val_accuracy: 0.7649\n",
      "Epoch 20/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.7665\n",
      "Epoch 20: val_loss improved from 0.16012 to 0.15908, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1552 - accuracy: 0.7670 - val_loss: 0.1591 - val_accuracy: 0.7639\n",
      "Epoch 21/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1548 - accuracy: 0.7652\n",
      "Epoch 21: val_loss improved from 0.15908 to 0.15834, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1543 - accuracy: 0.7675 - val_loss: 0.1583 - val_accuracy: 0.7629\n",
      "Epoch 22/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1530 - accuracy: 0.7673\n",
      "Epoch 22: val_loss improved from 0.15834 to 0.15784, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1534 - accuracy: 0.7670 - val_loss: 0.1578 - val_accuracy: 0.7609\n",
      "Epoch 23/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.7678\n",
      "Epoch 23: val_loss improved from 0.15784 to 0.15705, saving model to bestparams2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1526 - accuracy: 0.7667 - val_loss: 0.1571 - val_accuracy: 0.7609\n",
      "Epoch 24/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1524 - accuracy: 0.7655\n",
      "Epoch 24: val_loss improved from 0.15705 to 0.15696, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.7662 - val_loss: 0.1570 - val_accuracy: 0.7649\n",
      "Epoch 25/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.7691\n",
      "Epoch 25: val_loss improved from 0.15696 to 0.15646, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1514 - accuracy: 0.7698 - val_loss: 0.1565 - val_accuracy: 0.7660\n",
      "Epoch 26/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.7699\n",
      "Epoch 26: val_loss improved from 0.15646 to 0.15609, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1508 - accuracy: 0.7705 - val_loss: 0.1561 - val_accuracy: 0.7649\n",
      "Epoch 27/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.7667\n",
      "Epoch 27: val_loss did not improve from 0.15609\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1502 - accuracy: 0.7693 - val_loss: 0.1572 - val_accuracy: 0.7649\n",
      "Epoch 28/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.7694\n",
      "Epoch 28: val_loss improved from 0.15609 to 0.15582, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1499 - accuracy: 0.7700 - val_loss: 0.1558 - val_accuracy: 0.7680\n",
      "Epoch 29/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.7748\n",
      "Epoch 29: val_loss improved from 0.15582 to 0.15512, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1495 - accuracy: 0.7743 - val_loss: 0.1551 - val_accuracy: 0.7680\n",
      "Epoch 30/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.7735\n",
      "Epoch 30: val_loss improved from 0.15512 to 0.15490, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1490 - accuracy: 0.7723 - val_loss: 0.1549 - val_accuracy: 0.7670\n",
      "Epoch 31/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.7759\n",
      "Epoch 31: val_loss improved from 0.15490 to 0.15464, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1484 - accuracy: 0.7743 - val_loss: 0.1546 - val_accuracy: 0.7670\n",
      "Epoch 32/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.7762\n",
      "Epoch 32: val_loss did not improve from 0.15464\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.7768 - val_loss: 0.1546 - val_accuracy: 0.7680\n",
      "Epoch 33/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1481 - accuracy: 0.7801\n",
      "Epoch 33: val_loss improved from 0.15464 to 0.15400, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1478 - accuracy: 0.7809 - val_loss: 0.1540 - val_accuracy: 0.7710\n",
      "Epoch 34/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.7837\n",
      "Epoch 34: val_loss did not improve from 0.15400\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1477 - accuracy: 0.7814 - val_loss: 0.1546 - val_accuracy: 0.7680\n",
      "Epoch 35/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1462 - accuracy: 0.7855\n",
      "Epoch 35: val_loss improved from 0.15400 to 0.15382, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1470 - accuracy: 0.7834 - val_loss: 0.1538 - val_accuracy: 0.7720\n",
      "Epoch 36/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.7843\n",
      "Epoch 36: val_loss did not improve from 0.15382\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1465 - accuracy: 0.7844 - val_loss: 0.1547 - val_accuracy: 0.7710\n",
      "Epoch 37/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.7856\n",
      "Epoch 37: val_loss improved from 0.15382 to 0.15318, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.7844 - val_loss: 0.1532 - val_accuracy: 0.7720\n",
      "Epoch 38/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.7837\n",
      "Epoch 38: val_loss improved from 0.15318 to 0.15299, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1461 - accuracy: 0.7834 - val_loss: 0.1530 - val_accuracy: 0.7720\n",
      "Epoch 39/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1458 - accuracy: 0.7870\n",
      "Epoch 39: val_loss improved from 0.15299 to 0.15285, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1457 - accuracy: 0.7872 - val_loss: 0.1528 - val_accuracy: 0.7710\n",
      "Epoch 40/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.7869\n",
      "Epoch 40: val_loss did not improve from 0.15285\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1455 - accuracy: 0.7867 - val_loss: 0.1530 - val_accuracy: 0.7812\n",
      "Epoch 41/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.7877\n",
      "Epoch 41: val_loss improved from 0.15285 to 0.15256, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1452 - accuracy: 0.7872 - val_loss: 0.1526 - val_accuracy: 0.7852\n",
      "Epoch 42/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.7906\n",
      "Epoch 42: val_loss improved from 0.15256 to 0.15241, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1449 - accuracy: 0.7903 - val_loss: 0.1524 - val_accuracy: 0.7761\n",
      "Epoch 43/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.7908\n",
      "Epoch 43: val_loss improved from 0.15241 to 0.15224, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.7913 - val_loss: 0.1522 - val_accuracy: 0.7761\n",
      "Epoch 44/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.7887\n",
      "Epoch 44: val_loss improved from 0.15224 to 0.15171, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.7888 - val_loss: 0.1517 - val_accuracy: 0.7801\n",
      "Epoch 45/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.7905\n",
      "Epoch 45: val_loss did not improve from 0.15171\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.7913 - val_loss: 0.1518 - val_accuracy: 0.7822\n",
      "Epoch 46/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1448 - accuracy: 0.7914\n",
      "Epoch 46: val_loss improved from 0.15171 to 0.15167, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1439 - accuracy: 0.7928 - val_loss: 0.1517 - val_accuracy: 0.7832\n",
      "Epoch 47/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.7905\n",
      "Epoch 47: val_loss did not improve from 0.15167\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1438 - accuracy: 0.7898 - val_loss: 0.1517 - val_accuracy: 0.7812\n",
      "Epoch 48/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.7916\n",
      "Epoch 48: val_loss improved from 0.15167 to 0.15151, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1436 - accuracy: 0.7926 - val_loss: 0.1515 - val_accuracy: 0.7822\n",
      "Epoch 49/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1429 - accuracy: 0.7986\n",
      "Epoch 49: val_loss improved from 0.15151 to 0.15142, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.7974 - val_loss: 0.1514 - val_accuracy: 0.7781\n",
      "Epoch 50/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1438 - accuracy: 0.7956\n",
      "Epoch 50: val_loss improved from 0.15142 to 0.15112, saving model to bestparams2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.7974 - val_loss: 0.1511 - val_accuracy: 0.7842\n",
      "Epoch 51/150\n",
      "234/247 [===========================>..] - ETA: 0s - loss: 0.1430 - accuracy: 0.7954\n",
      "Epoch 51: val_loss did not improve from 0.15112\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1428 - accuracy: 0.7961 - val_loss: 0.1512 - val_accuracy: 0.7812\n",
      "Epoch 52/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.7971\n",
      "Epoch 52: val_loss did not improve from 0.15112\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.7946 - val_loss: 0.1512 - val_accuracy: 0.7822\n",
      "Epoch 53/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1420 - accuracy: 0.7965\n",
      "Epoch 53: val_loss improved from 0.15112 to 0.15088, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.7951 - val_loss: 0.1509 - val_accuracy: 0.7842\n",
      "Epoch 54/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.7968\n",
      "Epoch 54: val_loss improved from 0.15088 to 0.15085, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1427 - accuracy: 0.7941 - val_loss: 0.1508 - val_accuracy: 0.7842\n",
      "Epoch 55/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.7952\n",
      "Epoch 55: val_loss did not improve from 0.15085\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.7948 - val_loss: 0.1514 - val_accuracy: 0.7822\n",
      "Epoch 56/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.7973\n",
      "Epoch 56: val_loss improved from 0.15085 to 0.15025, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1423 - accuracy: 0.7961 - val_loss: 0.1502 - val_accuracy: 0.7832\n",
      "Epoch 57/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.7960\n",
      "Epoch 57: val_loss did not improve from 0.15025\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1421 - accuracy: 0.7961 - val_loss: 0.1504 - val_accuracy: 0.7801\n",
      "Epoch 58/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1415 - accuracy: 0.8007\n",
      "Epoch 58: val_loss did not improve from 0.15025\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1417 - accuracy: 0.7991 - val_loss: 0.1505 - val_accuracy: 0.7781\n",
      "Epoch 59/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1426 - accuracy: 0.7978\n",
      "Epoch 59: val_loss improved from 0.15025 to 0.15019, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.7991 - val_loss: 0.1502 - val_accuracy: 0.7812\n",
      "Epoch 60/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.7986\n",
      "Epoch 60: val_loss did not improve from 0.15019\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.7981 - val_loss: 0.1507 - val_accuracy: 0.7812\n",
      "Epoch 61/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.7981\n",
      "Epoch 61: val_loss improved from 0.15019 to 0.14996, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.7961 - val_loss: 0.1500 - val_accuracy: 0.7781\n",
      "Epoch 62/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1413 - accuracy: 0.7996\n",
      "Epoch 62: val_loss did not improve from 0.14996\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.8004 - val_loss: 0.1500 - val_accuracy: 0.7852\n",
      "Epoch 63/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.7996\n",
      "Epoch 63: val_loss improved from 0.14996 to 0.14956, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.8002 - val_loss: 0.1496 - val_accuracy: 0.7822\n",
      "Epoch 64/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1413 - accuracy: 0.7996\n",
      "Epoch 64: val_loss did not improve from 0.14956\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.7984 - val_loss: 0.1497 - val_accuracy: 0.7812\n",
      "Epoch 65/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1402 - accuracy: 0.7999\n",
      "Epoch 65: val_loss did not improve from 0.14956\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1407 - accuracy: 0.7996 - val_loss: 0.1498 - val_accuracy: 0.7822\n",
      "Epoch 66/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.7973\n",
      "Epoch 66: val_loss did not improve from 0.14956\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1409 - accuracy: 0.7979 - val_loss: 0.1496 - val_accuracy: 0.7812\n",
      "Epoch 67/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.7989\n",
      "Epoch 67: val_loss improved from 0.14956 to 0.14938, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.7986 - val_loss: 0.1494 - val_accuracy: 0.7812\n",
      "Epoch 68/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.7994\n",
      "Epoch 68: val_loss improved from 0.14938 to 0.14934, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1405 - accuracy: 0.7996 - val_loss: 0.1493 - val_accuracy: 0.7801\n",
      "Epoch 69/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8013\n",
      "Epoch 69: val_loss did not improve from 0.14934\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8014 - val_loss: 0.1495 - val_accuracy: 0.7781\n",
      "Epoch 70/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.8018\n",
      "Epoch 70: val_loss did not improve from 0.14934\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1401 - accuracy: 0.8017 - val_loss: 0.1500 - val_accuracy: 0.7771\n",
      "Epoch 71/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1396 - accuracy: 0.8009\n",
      "Epoch 71: val_loss did not improve from 0.14934\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.8007 - val_loss: 0.1494 - val_accuracy: 0.7822\n",
      "Epoch 72/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8015\n",
      "Epoch 72: val_loss improved from 0.14934 to 0.14928, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.8007 - val_loss: 0.1493 - val_accuracy: 0.7791\n",
      "Epoch 73/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.7989\n",
      "Epoch 73: val_loss improved from 0.14928 to 0.14917, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.8004 - val_loss: 0.1492 - val_accuracy: 0.7812\n",
      "Epoch 74/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1390 - accuracy: 0.8028\n",
      "Epoch 74: val_loss did not improve from 0.14917\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.8004 - val_loss: 0.1499 - val_accuracy: 0.7832\n",
      "Epoch 75/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8005\n",
      "Epoch 75: val_loss improved from 0.14917 to 0.14887, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1400 - accuracy: 0.8004 - val_loss: 0.1489 - val_accuracy: 0.7801\n",
      "Epoch 76/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.8018\n",
      "Epoch 76: val_loss did not improve from 0.14887\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8004 - val_loss: 0.1491 - val_accuracy: 0.7761\n",
      "Epoch 77/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.8038\n",
      "Epoch 77: val_loss improved from 0.14887 to 0.14884, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.8042 - val_loss: 0.1488 - val_accuracy: 0.7791\n",
      "Epoch 78/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8020\n",
      "Epoch 78: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1398 - accuracy: 0.8019 - val_loss: 0.1489 - val_accuracy: 0.7822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1397 - accuracy: 0.7986\n",
      "Epoch 79: val_loss did not improve from 0.14884\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8002 - val_loss: 0.1489 - val_accuracy: 0.7822\n",
      "Epoch 80/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8036\n",
      "Epoch 80: val_loss improved from 0.14884 to 0.14833, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8017 - val_loss: 0.1483 - val_accuracy: 0.7791\n",
      "Epoch 81/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8023\n",
      "Epoch 81: val_loss did not improve from 0.14833\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8024 - val_loss: 0.1485 - val_accuracy: 0.7791\n",
      "Epoch 82/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8020\n",
      "Epoch 82: val_loss did not improve from 0.14833\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1392 - accuracy: 0.8027 - val_loss: 0.1491 - val_accuracy: 0.7832\n",
      "Epoch 83/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.8026\n",
      "Epoch 83: val_loss did not improve from 0.14833\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8019 - val_loss: 0.1496 - val_accuracy: 0.7781\n",
      "Epoch 84/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.8015\n",
      "Epoch 84: val_loss did not improve from 0.14833\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.8027 - val_loss: 0.1486 - val_accuracy: 0.7822\n",
      "Epoch 85/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8020\n",
      "Epoch 85: val_loss improved from 0.14833 to 0.14831, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.8022 - val_loss: 0.1483 - val_accuracy: 0.7791\n",
      "Epoch 86/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.8012\n",
      "Epoch 86: val_loss did not improve from 0.14831\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8014 - val_loss: 0.1484 - val_accuracy: 0.7812\n",
      "Epoch 87/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.8041\n",
      "Epoch 87: val_loss did not improve from 0.14831\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8032 - val_loss: 0.1485 - val_accuracy: 0.7832\n",
      "Epoch 88/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.8020\n",
      "Epoch 88: val_loss improved from 0.14831 to 0.14830, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1388 - accuracy: 0.8029 - val_loss: 0.1483 - val_accuracy: 0.7791\n",
      "Epoch 89/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.8039\n",
      "Epoch 89: val_loss improved from 0.14830 to 0.14817, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.8017 - val_loss: 0.1482 - val_accuracy: 0.7801\n",
      "Epoch 90/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.8067\n",
      "Epoch 90: val_loss improved from 0.14817 to 0.14763, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.8060 - val_loss: 0.1476 - val_accuracy: 0.7822\n",
      "Epoch 91/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8020\n",
      "Epoch 91: val_loss did not improve from 0.14763\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1389 - accuracy: 0.8012 - val_loss: 0.1486 - val_accuracy: 0.7872\n",
      "Epoch 92/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.8036\n",
      "Epoch 92: val_loss improved from 0.14763 to 0.14752, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8050 - val_loss: 0.1475 - val_accuracy: 0.7801\n",
      "Epoch 93/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8057\n",
      "Epoch 93: val_loss did not improve from 0.14752\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8047 - val_loss: 0.1484 - val_accuracy: 0.7801\n",
      "Epoch 94/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.8013\n",
      "Epoch 94: val_loss did not improve from 0.14752\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8014 - val_loss: 0.1477 - val_accuracy: 0.7791\n",
      "Epoch 95/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.8049\n",
      "Epoch 95: val_loss did not improve from 0.14752\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8057 - val_loss: 0.1478 - val_accuracy: 0.7791\n",
      "Epoch 96/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8049\n",
      "Epoch 96: val_loss improved from 0.14752 to 0.14746, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.8067 - val_loss: 0.1475 - val_accuracy: 0.7832\n",
      "Epoch 97/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8049\n",
      "Epoch 97: val_loss did not improve from 0.14746\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.8040 - val_loss: 0.1475 - val_accuracy: 0.7842\n",
      "Epoch 98/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8036\n",
      "Epoch 98: val_loss improved from 0.14746 to 0.14737, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8047 - val_loss: 0.1474 - val_accuracy: 0.7852\n",
      "Epoch 99/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8026\n",
      "Epoch 99: val_loss did not improve from 0.14737\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8027 - val_loss: 0.1476 - val_accuracy: 0.7771\n",
      "Epoch 100/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.8028\n",
      "Epoch 100: val_loss did not improve from 0.14737\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.8037 - val_loss: 0.1476 - val_accuracy: 0.7791\n",
      "Epoch 101/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.8051\n",
      "Epoch 101: val_loss did not improve from 0.14737\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1380 - accuracy: 0.8067 - val_loss: 0.1474 - val_accuracy: 0.7791\n",
      "Epoch 102/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8044\n",
      "Epoch 102: val_loss did not improve from 0.14737\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.8042 - val_loss: 0.1477 - val_accuracy: 0.7791\n",
      "Epoch 103/150\n",
      "237/247 [===========================>..] - ETA: 0s - loss: 0.1379 - accuracy: 0.8041\n",
      "Epoch 103: val_loss did not improve from 0.14737\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8057 - val_loss: 0.1476 - val_accuracy: 0.7822\n",
      "Epoch 104/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8060\n",
      "Epoch 104: val_loss improved from 0.14737 to 0.14737, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8052 - val_loss: 0.1474 - val_accuracy: 0.7812\n",
      "Epoch 105/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.8049\n",
      "Epoch 105: val_loss improved from 0.14737 to 0.14732, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8057 - val_loss: 0.1473 - val_accuracy: 0.7852\n",
      "Epoch 106/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1377 - accuracy: 0.8057\n",
      "Epoch 106: val_loss improved from 0.14732 to 0.14712, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1377 - accuracy: 0.8062 - val_loss: 0.1471 - val_accuracy: 0.7822\n",
      "Epoch 107/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.8099\n",
      "Epoch 107: val_loss did not improve from 0.14712\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8100 - val_loss: 0.1473 - val_accuracy: 0.7781\n",
      "Epoch 108/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.8041\n",
      "Epoch 108: val_loss did not improve from 0.14712\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.8052 - val_loss: 0.1477 - val_accuracy: 0.7832\n",
      "Epoch 109/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.8062\n",
      "Epoch 109: val_loss did not improve from 0.14712\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8067 - val_loss: 0.1478 - val_accuracy: 0.7832\n",
      "Epoch 110/150\n",
      "236/247 [===========================>..] - ETA: 0s - loss: 0.1377 - accuracy: 0.8061\n",
      "Epoch 110: val_loss improved from 0.14712 to 0.14688, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.8070 - val_loss: 0.1469 - val_accuracy: 0.7812\n",
      "Epoch 111/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8041\n",
      "Epoch 111: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8045 - val_loss: 0.1469 - val_accuracy: 0.7801\n",
      "Epoch 112/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.8067\n",
      "Epoch 112: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8050 - val_loss: 0.1475 - val_accuracy: 0.7832\n",
      "Epoch 113/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8083\n",
      "Epoch 113: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1373 - accuracy: 0.8078 - val_loss: 0.1471 - val_accuracy: 0.7791\n",
      "Epoch 114/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.8052\n",
      "Epoch 114: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.8052 - val_loss: 0.1471 - val_accuracy: 0.7832\n",
      "Epoch 115/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8062\n",
      "Epoch 115: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1374 - accuracy: 0.8070 - val_loss: 0.1492 - val_accuracy: 0.7822\n",
      "Epoch 116/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.8073\n",
      "Epoch 116: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8067 - val_loss: 0.1472 - val_accuracy: 0.7801\n",
      "Epoch 117/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.8083\n",
      "Epoch 117: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8062 - val_loss: 0.1471 - val_accuracy: 0.7832\n",
      "Epoch 118/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1375 - accuracy: 0.8049\n",
      "Epoch 118: val_loss did not improve from 0.14688\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8067 - val_loss: 0.1471 - val_accuracy: 0.7822\n",
      "Epoch 119/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.8026\n",
      "Epoch 119: val_loss improved from 0.14688 to 0.14675, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8019 - val_loss: 0.1468 - val_accuracy: 0.7822\n",
      "Epoch 120/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.8067\n",
      "Epoch 120: val_loss did not improve from 0.14675\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8052 - val_loss: 0.1483 - val_accuracy: 0.7852\n",
      "Epoch 121/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8036\n",
      "Epoch 121: val_loss improved from 0.14675 to 0.14672, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.8037 - val_loss: 0.1467 - val_accuracy: 0.7842\n",
      "Epoch 122/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8078\n",
      "Epoch 122: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8070 - val_loss: 0.1467 - val_accuracy: 0.7842\n",
      "Epoch 123/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8044\n",
      "Epoch 123: val_loss did not improve from 0.14672\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8042 - val_loss: 0.1470 - val_accuracy: 0.7791\n",
      "Epoch 124/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.8049\n",
      "Epoch 124: val_loss improved from 0.14672 to 0.14652, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8067 - val_loss: 0.1465 - val_accuracy: 0.7801\n",
      "Epoch 125/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8062\n",
      "Epoch 125: val_loss did not improve from 0.14652\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1372 - accuracy: 0.8072 - val_loss: 0.1470 - val_accuracy: 0.7822\n",
      "Epoch 126/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1357 - accuracy: 0.8101\n",
      "Epoch 126: val_loss improved from 0.14652 to 0.14609, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1369 - accuracy: 0.8078 - val_loss: 0.1461 - val_accuracy: 0.7832\n",
      "Epoch 127/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8060\n",
      "Epoch 127: val_loss improved from 0.14609 to 0.14596, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8072 - val_loss: 0.1460 - val_accuracy: 0.7842\n",
      "Epoch 128/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8031\n",
      "Epoch 128: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.8032 - val_loss: 0.1466 - val_accuracy: 0.7822\n",
      "Epoch 129/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8067\n",
      "Epoch 129: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1367 - accuracy: 0.8070 - val_loss: 0.1467 - val_accuracy: 0.7822\n",
      "Epoch 130/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8086\n",
      "Epoch 130: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.8090 - val_loss: 0.1463 - val_accuracy: 0.7832\n",
      "Epoch 131/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1368 - accuracy: 0.8046\n",
      "Epoch 131: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8050 - val_loss: 0.1474 - val_accuracy: 0.7852\n",
      "Epoch 132/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8099\n",
      "Epoch 132: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1364 - accuracy: 0.8098 - val_loss: 0.1460 - val_accuracy: 0.7812\n",
      "Epoch 133/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.8112\n",
      "Epoch 133: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8078 - val_loss: 0.1467 - val_accuracy: 0.7832\n",
      "Epoch 134/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.8088\n",
      "Epoch 134: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8095 - val_loss: 0.1465 - val_accuracy: 0.7771\n",
      "Epoch 135/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8099\n",
      "Epoch 135: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.8095 - val_loss: 0.1462 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.8065\n",
      "Epoch 136: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8062 - val_loss: 0.1463 - val_accuracy: 0.7812\n",
      "Epoch 137/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.8088\n",
      "Epoch 137: val_loss did not improve from 0.14596\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8098 - val_loss: 0.1463 - val_accuracy: 0.7781\n",
      "Epoch 138/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.8054\n",
      "Epoch 138: val_loss improved from 0.14596 to 0.14583, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8052 - val_loss: 0.1458 - val_accuracy: 0.7791\n",
      "Epoch 139/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.8083\n",
      "Epoch 139: val_loss did not improve from 0.14583\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8095 - val_loss: 0.1459 - val_accuracy: 0.7842\n",
      "Epoch 140/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.8080\n",
      "Epoch 140: val_loss did not improve from 0.14583\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.8067 - val_loss: 0.1460 - val_accuracy: 0.7842\n",
      "Epoch 141/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.8073\n",
      "Epoch 141: val_loss did not improve from 0.14583\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8078 - val_loss: 0.1461 - val_accuracy: 0.7832\n",
      "Epoch 142/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.8086\n",
      "Epoch 142: val_loss did not improve from 0.14583\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1361 - accuracy: 0.8067 - val_loss: 0.1460 - val_accuracy: 0.7822\n",
      "Epoch 143/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.8067\n",
      "Epoch 143: val_loss improved from 0.14583 to 0.14570, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.8080 - val_loss: 0.1457 - val_accuracy: 0.7812\n",
      "Epoch 144/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.8054\n",
      "Epoch 144: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8065 - val_loss: 0.1458 - val_accuracy: 0.7822\n",
      "Epoch 145/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.8086\n",
      "Epoch 145: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.8095 - val_loss: 0.1463 - val_accuracy: 0.7822\n",
      "Epoch 146/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.8073\n",
      "Epoch 146: val_loss did not improve from 0.14570\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8072 - val_loss: 0.1468 - val_accuracy: 0.7812\n",
      "Epoch 147/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.8104\n",
      "Epoch 147: val_loss improved from 0.14570 to 0.14545, saving model to bestparams2.h5\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1360 - accuracy: 0.8095 - val_loss: 0.1455 - val_accuracy: 0.7822\n",
      "Epoch 148/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.8057\n",
      "Epoch 148: val_loss did not improve from 0.14545\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.8055 - val_loss: 0.1461 - val_accuracy: 0.7852\n",
      "Epoch 149/150\n",
      "239/247 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.8083\n",
      "Epoch 149: val_loss did not improve from 0.14545\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.8080 - val_loss: 0.1462 - val_accuracy: 0.7812\n",
      "Epoch 150/150\n",
      "238/247 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.8091\n",
      "Epoch 150: val_loss did not improve from 0.14545\n",
      "247/247 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.8090 - val_loss: 0.1459 - val_accuracy: 0.7842\n",
      "> 78.723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean=80.851 std=1.727, n=5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def define_model():\n",
    "    layer0 = Input(shape=tr.shape[1:], name='input')\n",
    "    layer1 = Conv1D(filters=16, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu',name='Conv1D_1')(layer0)\n",
    "    layer2 = BatchNormalization()(layer1)\n",
    "    layer3 = MaxPool1D(pool_size=1, strides=1)(layer2)\n",
    "\n",
    "    layer4 = Conv1D(filters=12, kernel_size=4, strides=1, kernel_initializer='glorot_normal',activation='relu', name='Conv1D_2')(layer3)\n",
    "    layer5 = MaxPool1D(pool_size=2, strides=2)(layer4)\n",
    "    layer6 = GlobalAveragePooling1D()(layer5)\n",
    "\n",
    "\n",
    "    layer7 = Flatten(name='Flatten_1')(layer6)\n",
    "    layer8 = Dense(12,activation='relu',name='Dense_1')(layer7)\n",
    "    layer9 = Dropout(0.00099,name='dropout_3')(layer8)\n",
    "    layer10 = Dense(1,activation='relu',name='Dense_3')(layer9)\n",
    "    layer11 = Dropout(0.00099,name='dropout_4')(layer10)\n",
    "\n",
    "    ### Contextual regression\n",
    "    layer12 = Dense(166*4,kernel_regularizer=tf.keras.regularizers.l1(0.0001),name='Contextual_Weight')(layer11) ## make first layer\n",
    "    layer13 = Dropout(0.00099,name='dropout_5')(layer12)\n",
    "\n",
    "    #### Dot Product\n",
    "    layer14 = Flatten(name='Flatten_2')(layer0)\n",
    "    layer15 = Multiply()([layer13,layer14]) ## same number of nodes as input feaures\n",
    "\n",
    "#    layer16 = Dense(1, activation='sigmoid', kernel_initializer='ones',use_bias=False,name='Sum')(layer15)\n",
    "    layer17 = Dense(1, activation='sigmoid', name='Output')(layer16)\n",
    "\n",
    "    model = Model(inputs=layer0, outputs=layer17)\n",
    "    #model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(SGD(lr=0.01, momentum=0.9),'mean_squared_error', metrics=['accuracy']) ##, momentum=0.9\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX=x_train2, dataY=y_train2, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    # prepare cross validation\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in kfold.split(dataX):\n",
    "    # define model\n",
    "        model = define_model()\n",
    "        # select rows for train and test\n",
    "        trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "        # fit model\n",
    "        earlystop_cb = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n",
    "        check_cb = ModelCheckpoint('bestparams2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        history = model.fit(x_train2, y_train2, batch_size=16, epochs=150, validation_split=0.20, callbacks=[check_cb]) \n",
    "        # evaluate model\n",
    "        _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "        print('> %.3f' % (acc * 100.0))\n",
    "        # append scores\n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "    return scores, histories\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    for i in range(len(histories)):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    # summarize model performance\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    pyplot.boxplot(scores)\n",
    "    pyplot.show()\n",
    "    \n",
    "    # run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX=x_train2\n",
    "    trainY=y_train2\n",
    "    testX=x_test2\n",
    "    testY=y_test2 \n",
    "    # prepare pixel data\n",
    "#    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # evaluate model\n",
    "    scores, histories = evaluate_model(trainX, trainY)\n",
    "    # learning curves\n",
    "    summarize_diagnostics(histories)\n",
    "    # summarize estimated performance\n",
    "    summarize_performance(scores)\n",
    "    \n",
    "    # entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721f461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
